{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "855f9f50-ef38-4069-932a-fb49af02d28e",
      "metadata": {
        "id": "855f9f50-ef38-4069-932a-fb49af02d28e"
      },
      "source": [
        "1. **Query-Time Table Retrieval**: Dynamically retrieve relevant tables in the text-to-SQL prompt.\n",
        "2. **Query-Time Sample Row retrieval**: Embed/Index each row, and dynamically retrieve example rows for each table in the text-to-SQL prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "997717d9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Hp\\\\Documents\\\\GitHub\\\\rag_text-2-sql\\\\notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ad056f1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Hp\\\\Documents\\\\GitHub\\\\rag_text-2-sql'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"../\")\n",
        "\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6bc8cd21",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import time\n",
        "import re\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import gc\n",
        "import traceback\n",
        "import json\n",
        "import json as pyjson\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from typing import List, Dict\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# setup Arize Phoenix for logging/observability\n",
        "import phoenix as px\n",
        "\n",
        "import chromadb\n",
        "\n",
        "\n",
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    text,\n",
        "    inspect,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        ")\n",
        "\n",
        "from llama_index.core import (\n",
        "    Settings, \n",
        "    SQLDatabase, \n",
        "    VectorStoreIndex, \n",
        "    load_index_from_storage,\n",
        "    set_global_handler,\n",
        ")\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.objects import (\n",
        "    SQLTableNodeMapping,\n",
        "    ObjectIndex,\n",
        "    SQLTableSchema,\n",
        ")\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.retrievers import SQLRetriever\n",
        "from llama_index.core.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.llms import ChatResponse\n",
        "# from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core.storage import StorageContext\n",
        "from llama_index.core.workflow import (\n",
        "    Workflow, \n",
        "    step, \n",
        "    StartEvent, \n",
        "    StopEvent,\n",
        ")\n",
        "from llama_index.core.workflow.events import Event\n",
        "from llama_index.utils.workflow import (\n",
        "    draw_all_possible_flows, \n",
        "    draw_most_recent_execution,\n",
        ")\n",
        "\n",
        "from utils.config import CONFIG\n",
        "from utils.logger import setup_logger\n",
        "\n",
        "\n",
        "# configurations\n",
        "LOG_PATH = Path(CONFIG[\"LOG_PATH\"])\n",
        "\n",
        "CHINOOK_DBEAVER_DB_PATH = Path(CONFIG[\"CHINOOK_DBEAVER_DB_PATH\"])\n",
        "CHINOOK_TABLE_SUMMARIES_DB_PATH = Path(CONFIG[\"CHINOOK_TABLE_SUMMARIES_DB_PATH\"])\n",
        "SQLITE_DB_DIR = Path(CONFIG[\"SQLITE_DB_DIR\"])\n",
        "CHROMA_DB_DIR = Path(CONFIG[\"CHROMA_DB_DIR\"])\n",
        "\n",
        "WORKFLOW_VISUALIZATION_DIR = Path(CONFIG[\"WORKFLOW_VISUALIZATION_DIR\"])\n",
        "\n",
        "QUERY_TEXT = CONFIG[\"QUERY_TEXT\"]\n",
        "QUERY_TEXT_INITIAL = CONFIG[\"QUERY_TEXT_INITIAL\"]\n",
        "\n",
        "TOP_K = CONFIG[\"TOP_K\"]\n",
        "TOP_N = CONFIG[\"TOP_N\"]\n",
        "MAX_RETRIES = CONFIG[\"MAX_RETRIES\"]\n",
        "\n",
        "\n",
        "# setup logging\n",
        "LOG_DIR = os.path.join(os.getcwd(), LOG_PATH)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)  # Create the logs directory if it doesn't exist\n",
        "\n",
        "# comment out line 15 in utils/logger.py -> only for notebooks\n",
        "LOG_FILE = os.path.join(LOG_DIR, \"db_connect_notebook.log\")\n",
        "logger = setup_logger(\"db_connect_notebook_logger\", LOG_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fe7acbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.llm.get_prompt_temp import TABLE_INFO_PROMPT\n",
        "from utils.llm.get_llm_func import get_llm_func\n",
        "\n",
        "\n",
        "class TableInfo(BaseModel):\n",
        "    \"\"\"Information regarding a structured table.\"\"\"\n",
        "\n",
        "    table_name: str = Field(\n",
        "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
        "    )\n",
        "    table_summary: str = Field(\n",
        "        ..., description=\"short, concise summary/caption of the table\"\n",
        "    )\n",
        "\n",
        "\n",
        "def text_completion_program(table_info_prompt_temp: str):\n",
        "    \n",
        "    return LLMTextCompletionProgram.from_defaults(\n",
        "        output_cls=TableInfo,\n",
        "        prompt_template_str=table_info_prompt_temp,\n",
        "        llm=get_llm_func(),\n",
        "    )\n",
        "\n",
        "\n",
        "def extract_first_json_block(text: str):\n",
        "    logger.info(\"Extracting the first valid JSON object from text, ignoring extra trailing text.\")\n",
        "    \n",
        "    match = re.search(r\"\\{.*\\}\", text, re.S)\n",
        "    if not match:\n",
        "        logger.error(f\"No JSON object found in text: {text}\")\n",
        "        raise ValueError(\"No JSON object found in output\")\n",
        "    \n",
        "    try:\n",
        "        logger.info(f\"Extracted JSON: {match.group()}\")\n",
        "        return pyjson.loads(match.group())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to parse JSON: {e}\\nRaw text: {text}\")\n",
        "        raise ValueError(f\"Failed to parse JSON: {e}\\nRaw text: {text}\")\n",
        "\n",
        "\n",
        "def prep_summary_engine(sqlite_db_dir: Path, main_db_dir: Path):\n",
        "    \"\"\"Prepare the SQLite engine for storing table summaries.\"\"\"\n",
        "    os.makedirs(sqlite_db_dir, exist_ok=True)\n",
        "    summary_db_path = os.path.join(sqlite_db_dir, \"table_summaries.db\")\n",
        "\n",
        "    logger.info(f\"Creating SQLite DB Engine for the new summaries database: {summary_db_path}\")\n",
        "    summary_engine = create_engine(f\"sqlite:///{summary_db_path}\")\n",
        "\n",
        "    logger.info(f\"Creating SQLite DB Engine for the existing Chinook database at {main_db_dir}\")\n",
        "    engine = create_engine(f\"sqlite:///{main_db_dir}\")\n",
        "    inspector = inspect(engine)\n",
        "\n",
        "    return summary_engine, summary_db_path, engine, inspector\n",
        "\n",
        "\n",
        "def create_summaries_table(summary_engine):\n",
        "    \"\"\"\n",
        "    Ensure the 'table_summaries' table exists in the SQLite DB.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(\" - Ensuring the table exists (id, table_name, table_summary, created_at)\")\n",
        "        with summary_engine.begin() as conn:\n",
        "            conn.execute(text(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS table_summaries (\n",
        "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                    table_name TEXT NOT NULL UNIQUE,\n",
        "                    table_summary TEXT NOT NULL,\n",
        "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "                )\n",
        "            \"\"\"))\n",
        "        logger.info(\"Table 'table_summaries' ensured/created.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to ensure table_summaries exists: {e}\")\n",
        "\n",
        "\n",
        "def dump_summaries_sqlite(summary_engine, table_infos):\n",
        "    \"\"\"\n",
        "    Save table summaries into SQLite DB.\n",
        "    Skips duplicates based on table_name.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with summary_engine.begin() as conn:\n",
        "            for t in table_infos:\n",
        "                conn.execute(\n",
        "                    text(\"\"\"\n",
        "                        INSERT INTO table_summaries (table_name, table_summary) \n",
        "                        VALUES (:name, :summary)\n",
        "                        ON CONFLICT(table_name) DO NOTHING\n",
        "                    \"\"\"),\n",
        "                    {\"name\": t.table_name, \"summary\": t.table_summary},\n",
        "                )\n",
        "        logger.info(f\"Saved {len(table_infos)} summaries to SQLite\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save summaries to SQLite: {e}\")\n",
        "\n",
        "\n",
        "def dump_summaries_json(sqlite_db_dir: Path, table_infos):\n",
        "    \"\"\"\n",
        "    Save table summaries into JSON file.\n",
        "    Appends/merges with existing file instead of overwriting.\n",
        "    \"\"\"\n",
        "    json_path = os.path.join(sqlite_db_dir, \"table_summaries.json\")\n",
        "\n",
        "    try:\n",
        "        # Load old summaries if file exists\n",
        "        if os.path.exists(json_path):\n",
        "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                old_data = json.load(f)\n",
        "        else:\n",
        "            old_data = []\n",
        "\n",
        "        # Merge with new summaries (skip duplicates by table_name)\n",
        "        old_map = {d[\"table_name\"]: d for d in old_data}\n",
        "        for t in table_infos:\n",
        "            old_map[t.table_name] = t.model_dump()\n",
        "\n",
        "        merged_data = list(old_map.values())\n",
        "\n",
        "        # Write back merged data\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"JSON dump updated at {json_path} with {len(table_infos)} new/updated entries\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to dump JSON summaries: {e}\")\n",
        "\n",
        "    return json_path\n",
        "\n",
        "\n",
        "def generate_table_summary(program, summary_engine, summary_db_path, engine, inspector, sqlite_db_dir: Path, max_retries: int):\n",
        "    table_infos = []\n",
        "    create_summaries_table(summary_engine)\n",
        "\n",
        "    logger.info(\"Generating table summaries...\")\n",
        "    with engine.connect() as conn:\n",
        "        existing_tables = set()\n",
        "        \n",
        "        logger.info(\"Fetching existing summaries from the summaries database...\")\n",
        "        with summary_engine.connect() as summary_conn:\n",
        "            rows = summary_conn.execute(text(\"SELECT table_name FROM table_summaries\")).fetchall()\n",
        "            existing_tables = {row[0] for row in rows}\n",
        "            logger.info(f\"Found {len(existing_tables)} existing summaries in DB\")\n",
        "        \n",
        "        for idx, table in enumerate(inspector.get_table_names()):\n",
        "            if table in existing_tables:\n",
        "                logger.info(f\" - Skipping table '{table}' â€” summary already exists.\")\n",
        "                continue\n",
        "            \n",
        "            logger.info(f\" - Processing new table: {table}\")\n",
        "            df = pd.read_sql(f\"SELECT * FROM {table} LIMIT 10;\", conn)\n",
        "            df_str = df.to_csv(index=False)\n",
        "\n",
        "            table_info = None\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    raw_output = program(\n",
        "                        table_str=df_str,\n",
        "                        exclude_table_name_list=str(list(inspector.get_table_names())),\n",
        "                    )\n",
        "\n",
        "                    logger.info(f\"Normalize LLM output\")\n",
        "                    if isinstance(raw_output, str):\n",
        "                        parsed_dict = extract_first_json_block(raw_output)\n",
        "                    elif isinstance(raw_output, dict):\n",
        "                        parsed_dict = raw_output\n",
        "                    elif isinstance(raw_output, TableInfo):\n",
        "                        parsed_dict = raw_output.model_dump()\n",
        "                    else:\n",
        "                        logger.error(f\"Unexpected return type: {type(raw_output)}\")\n",
        "                        raise TypeError(f\"Unexpected return type: {type(raw_output)}\")\n",
        "\n",
        "                    table_info = TableInfo(\n",
        "                        table_name=table,  # use actual SQLAlchemy inspector name\n",
        "                        table_summary=parsed_dict[\"table_summary\"],\n",
        "                    )\n",
        "\n",
        "                    logger.info(f\"Processed table: {table_info.table_name}\")\n",
        "                    break  # success â†’ next table\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error with attempt {attempt+1} for {table}: {e}\")\n",
        "                    time.sleep(2)\n",
        "\n",
        "            if table_info:\n",
        "                table_infos.append(table_info)\n",
        "                \n",
        "                try:\n",
        "                    logger.info(f\"Saving table summary for {table_info.table_name} immediately to summaries DB\")\n",
        "                    dump_summaries_sqlite(summary_engine, table_infos)\n",
        "                    json_path = dump_summaries_json(sqlite_db_dir, table_infos)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to save table summary for {table_info.table_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    return table_infos, summary_db_path, json_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b3e192",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-22 23:56:22,262 [INFO] Creating SQLite DB Engine for the new summaries database: db\\Chinook\\sqlite\\table_summaries.db\n",
            "2025-08-22 23:56:22,264 [INFO] Creating SQLite DB Engine for the existing Chinook database at C:\\Users\\Hp\\AppData\\Roaming\\DBeaverData\\workspace6\\.metadata\\sample-database-sqlite-1\\Chinook.db\n",
            "2025-08-22 23:56:22,269 [INFO]  - Ensuring the table exists (id, table_name, table_summary, created_at)\n",
            "2025-08-22 23:56:22,279 [INFO] Table 'table_summaries' ensured/created.\n",
            "2025-08-22 23:56:22,280 [INFO] Generating table summaries...\n",
            "2025-08-22 23:56:22,281 [INFO] Fetching existing summaries from the summaries database...\n",
            "2025-08-22 23:56:22,283 [INFO] Found 0 existing summaries in DB\n",
            "2025-08-22 23:56:22,285 [INFO]  - Processing new table: Album\n",
            "2025-08-22 23:57:01,160 [INFO] Normalize LLM output\n",
            "2025-08-22 23:57:01,167 [INFO] Processed table: Album\n",
            "2025-08-22 23:57:01,171 [INFO] Saving table summary for Album immediately to summaries DB\n",
            "2025-08-22 23:57:01,190 [INFO] Saved 1 summaries to SQLite\n",
            "2025-08-22 23:57:01,197 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 1 new/updated entries\n",
            "2025-08-22 23:57:01,199 [INFO]  - Processing new table: Artist\n",
            "2025-08-22 23:57:20,342 [INFO] Normalize LLM output\n",
            "2025-08-22 23:57:20,344 [INFO] Processed table: Artist\n",
            "2025-08-22 23:57:20,346 [INFO] Saving table summary for Artist immediately to summaries DB\n",
            "2025-08-22 23:57:20,359 [INFO] Saved 2 summaries to SQLite\n",
            "2025-08-22 23:57:20,384 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 2 new/updated entries\n",
            "2025-08-22 23:57:20,388 [INFO]  - Processing new table: Customer\n",
            "2025-08-22 23:57:46,956 [INFO] Normalize LLM output\n",
            "2025-08-22 23:57:46,958 [INFO] Processed table: Customer\n",
            "2025-08-22 23:57:46,960 [INFO] Saving table summary for Customer immediately to summaries DB\n",
            "2025-08-22 23:57:46,971 [INFO] Saved 3 summaries to SQLite\n",
            "2025-08-22 23:57:46,993 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 3 new/updated entries\n",
            "2025-08-22 23:57:46,996 [INFO]  - Processing new table: Employee\n",
            "2025-08-22 23:58:17,638 [INFO] Normalize LLM output\n",
            "2025-08-22 23:58:17,639 [INFO] Processed table: Employee\n",
            "2025-08-22 23:58:17,640 [INFO] Saving table summary for Employee immediately to summaries DB\n",
            "2025-08-22 23:58:17,650 [INFO] Saved 4 summaries to SQLite\n",
            "2025-08-22 23:58:17,670 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 4 new/updated entries\n",
            "2025-08-22 23:58:17,671 [INFO]  - Processing new table: Genre\n",
            "2025-08-22 23:58:27,253 [INFO] Normalize LLM output\n",
            "2025-08-22 23:58:27,254 [INFO] Processed table: Genre\n",
            "2025-08-22 23:58:27,255 [INFO] Saving table summary for Genre immediately to summaries DB\n",
            "2025-08-22 23:58:27,266 [INFO] Saved 5 summaries to SQLite\n",
            "2025-08-22 23:58:27,292 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 5 new/updated entries\n",
            "2025-08-22 23:58:27,294 [INFO]  - Processing new table: Invoice\n",
            "2025-08-22 23:58:53,238 [INFO] Normalize LLM output\n",
            "2025-08-22 23:58:53,240 [INFO] Processed table: Invoice\n",
            "2025-08-22 23:58:53,241 [INFO] Saving table summary for Invoice immediately to summaries DB\n",
            "2025-08-22 23:58:53,254 [INFO] Saved 6 summaries to SQLite\n",
            "2025-08-22 23:58:53,287 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 6 new/updated entries\n",
            "2025-08-22 23:58:53,289 [INFO]  - Processing new table: InvoiceLine\n",
            "2025-08-22 23:59:07,655 [INFO] Normalize LLM output\n",
            "2025-08-22 23:59:07,656 [INFO] Processed table: InvoiceLine\n",
            "2025-08-22 23:59:07,657 [INFO] Saving table summary for InvoiceLine immediately to summaries DB\n",
            "2025-08-22 23:59:07,668 [INFO] Saved 7 summaries to SQLite\n",
            "2025-08-22 23:59:07,688 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 7 new/updated entries\n",
            "2025-08-22 23:59:07,690 [INFO]  - Processing new table: MediaType\n",
            "2025-08-22 23:59:18,199 [INFO] Normalize LLM output\n",
            "2025-08-22 23:59:18,201 [INFO] Processed table: MediaType\n",
            "2025-08-22 23:59:18,201 [INFO] Saving table summary for MediaType immediately to summaries DB\n",
            "2025-08-22 23:59:18,211 [INFO] Saved 8 summaries to SQLite\n",
            "2025-08-22 23:59:18,230 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 8 new/updated entries\n",
            "2025-08-22 23:59:18,231 [INFO]  - Processing new table: Playlist\n",
            "2025-08-22 23:59:35,359 [INFO] Normalize LLM output\n",
            "2025-08-22 23:59:35,360 [INFO] Processed table: Playlist\n",
            "2025-08-22 23:59:35,363 [INFO] Saving table summary for Playlist immediately to summaries DB\n",
            "2025-08-22 23:59:35,376 [INFO] Saved 9 summaries to SQLite\n",
            "2025-08-22 23:59:35,400 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 9 new/updated entries\n",
            "2025-08-22 23:59:35,402 [INFO]  - Processing new table: PlaylistTrack\n",
            "2025-08-22 23:59:48,745 [INFO] Normalize LLM output\n",
            "2025-08-22 23:59:48,746 [INFO] Processed table: PlaylistTrack\n",
            "2025-08-22 23:59:48,747 [INFO] Saving table summary for PlaylistTrack immediately to summaries DB\n",
            "2025-08-22 23:59:48,757 [INFO] Saved 10 summaries to SQLite\n",
            "2025-08-22 23:59:48,777 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 10 new/updated entries\n",
            "2025-08-22 23:59:48,779 [INFO]  - Processing new table: Track\n",
            "2025-08-23 00:00:16,367 [ERROR] Error with attempt 1 for Track: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 71 [type=json_invalid, input_value='{\"table_name\": \"track_in...Summary of track data\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-23 00:00:30,368 [INFO] Normalize LLM output\n",
            "2025-08-23 00:00:30,369 [INFO] Processed table: Track\n",
            "2025-08-23 00:00:30,370 [INFO] Saving table summary for Track immediately to summaries DB\n",
            "2025-08-23 00:00:30,379 [INFO] Saved 11 summaries to SQLite\n",
            "2025-08-23 00:00:30,403 [INFO] JSON dump updated at db\\Chinook\\sqlite\\table_summaries.json with 11 new/updated entries\n",
            "2025-08-23 00:00:30,410 [INFO] \n",
            " FINAL TABLE SUMMARIES\n",
            "2025-08-23 00:00:30,411 [INFO] - Album: Summary of album and artist data\n",
            "2025-08-23 00:00:30,413 [INFO] - Artist: Summary of artist information\n",
            "2025-08-23 00:00:30,414 [INFO] - Customer: Summary of customer data\n",
            "2025-08-23 00:00:30,415 [INFO] - Employee: Summary of employee information\n",
            "2025-08-23 00:00:30,416 [INFO] - Genre: Summary of genre data\n",
            "2025-08-23 00:00:30,417 [INFO] - Invoice: Invoice Information\n",
            "2025-08-23 00:00:30,418 [INFO] - InvoiceLine: Summary of invoice data\n",
            "2025-08-23 00:00:30,419 [INFO] - MediaType: Summary of media data\n",
            "2025-08-23 00:00:30,419 [INFO] - Playlist: Summary of playlist data including IDs and names\n",
            "2025-08-23 00:00:30,420 [INFO] - PlaylistTrack: Summary of playlist and track data\n",
            "2025-08-23 00:00:30,421 [INFO] - Track: Summary of track data\n",
            "2025-08-23 00:00:30,422 [INFO] \n",
            "Saved 11 summaries to:\n",
            "2025-08-23 00:00:30,423 [INFO]  - SQLite DB: db\\Chinook\\sqlite\\table_summaries.db\n",
            "2025-08-23 00:00:30,426 [INFO]  - JSON backup: db\\Chinook\\sqlite\\table_summaries.json\n"
          ]
        }
      ],
      "source": [
        "program = text_completion_program(TABLE_INFO_PROMPT)\n",
        "\n",
        "summary_engine, summary_db_path, engine, inspector = prep_summary_engine(SQLITE_DB_DIR, CHINOOK_DBEAVER_DB_PATH)\n",
        "\n",
        "table_infos, summary_db_path, json_path = generate_table_summary(program, summary_engine, summary_db_path, engine, inspector, SQLITE_DB_DIR, MAX_RETRIES)\n",
        "\n",
        "logger.info(\"\\n FINAL TABLE SUMMARIES\")\n",
        "for t in table_infos:\n",
        "    logger.info(f\"- {t.table_name}: {t.table_summary}\")\n",
        "\n",
        "logger.info(f\"\\nSaved {len(table_infos)} summaries to:\")\n",
        "logger.info(f\" - SQLite DB: {summary_db_path}\")\n",
        "logger.info(f\" - JSON backup: {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "53544059-de7d-48dd-8e00-89517964852b",
      "metadata": {
        "id": "53544059-de7d-48dd-8e00-89517964852b",
        "outputId": "df224651-1765-4aa7-e841-d58546c20e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-23 00:01:07,558 [INFO] Table Schemas\n",
            "2025-08-23 00:01:07,568 [INFO] \n",
            "Table: Album\n",
            "2025-08-23 00:01:07,569 [INFO]   AlbumId: INTEGER\n",
            "2025-08-23 00:01:07,569 [INFO]   Title: NVARCHAR(160)\n",
            "2025-08-23 00:01:07,570 [INFO]   ArtistId: INTEGER\n",
            "2025-08-23 00:01:07,572 [INFO] \n",
            "Table: Artist\n",
            "2025-08-23 00:01:07,572 [INFO]   ArtistId: INTEGER\n",
            "2025-08-23 00:01:07,573 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-23 00:01:07,574 [INFO] \n",
            "Table: Customer\n",
            "2025-08-23 00:01:07,575 [INFO]   CustomerId: INTEGER\n",
            "2025-08-23 00:01:07,576 [INFO]   FirstName: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,577 [INFO]   LastName: NVARCHAR(20)\n",
            "2025-08-23 00:01:07,577 [INFO]   Company: NVARCHAR(80)\n",
            "2025-08-23 00:01:07,578 [INFO]   Address: NVARCHAR(70)\n",
            "2025-08-23 00:01:07,579 [INFO]   City: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,580 [INFO]   State: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,581 [INFO]   Country: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,581 [INFO]   PostalCode: NVARCHAR(10)\n",
            "2025-08-23 00:01:07,582 [INFO]   Phone: NVARCHAR(24)\n",
            "2025-08-23 00:01:07,582 [INFO]   Fax: NVARCHAR(24)\n",
            "2025-08-23 00:01:07,583 [INFO]   Email: NVARCHAR(60)\n",
            "2025-08-23 00:01:07,584 [INFO]   SupportRepId: INTEGER\n",
            "2025-08-23 00:01:07,586 [INFO] \n",
            "Table: Employee\n",
            "2025-08-23 00:01:07,586 [INFO]   EmployeeId: INTEGER\n",
            "2025-08-23 00:01:07,587 [INFO]   LastName: NVARCHAR(20)\n",
            "2025-08-23 00:01:07,588 [INFO]   FirstName: NVARCHAR(20)\n",
            "2025-08-23 00:01:07,588 [INFO]   Title: NVARCHAR(30)\n",
            "2025-08-23 00:01:07,589 [INFO]   ReportsTo: INTEGER\n",
            "2025-08-23 00:01:07,590 [INFO]   BirthDate: DATETIME\n",
            "2025-08-23 00:01:07,591 [INFO]   HireDate: DATETIME\n",
            "2025-08-23 00:01:07,593 [INFO]   Address: NVARCHAR(70)\n",
            "2025-08-23 00:01:07,594 [INFO]   City: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,595 [INFO]   State: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,596 [INFO]   Country: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,596 [INFO]   PostalCode: NVARCHAR(10)\n",
            "2025-08-23 00:01:07,598 [INFO]   Phone: NVARCHAR(24)\n",
            "2025-08-23 00:01:07,600 [INFO]   Fax: NVARCHAR(24)\n",
            "2025-08-23 00:01:07,602 [INFO]   Email: NVARCHAR(60)\n",
            "2025-08-23 00:01:07,603 [INFO] \n",
            "Table: Genre\n",
            "2025-08-23 00:01:07,604 [INFO]   GenreId: INTEGER\n",
            "2025-08-23 00:01:07,605 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-23 00:01:07,608 [INFO] \n",
            "Table: Invoice\n",
            "2025-08-23 00:01:07,610 [INFO]   InvoiceId: INTEGER\n",
            "2025-08-23 00:01:07,611 [INFO]   CustomerId: INTEGER\n",
            "2025-08-23 00:01:07,611 [INFO]   InvoiceDate: DATETIME\n",
            "2025-08-23 00:01:07,613 [INFO]   BillingAddress: NVARCHAR(70)\n",
            "2025-08-23 00:01:07,614 [INFO]   BillingCity: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,615 [INFO]   BillingState: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,616 [INFO]   BillingCountry: NVARCHAR(40)\n",
            "2025-08-23 00:01:07,616 [INFO]   BillingPostalCode: NVARCHAR(10)\n",
            "2025-08-23 00:01:07,617 [INFO]   Total: NUMERIC(10, 2)\n",
            "2025-08-23 00:01:07,618 [INFO] \n",
            "Table: InvoiceLine\n",
            "2025-08-23 00:01:07,619 [INFO]   InvoiceLineId: INTEGER\n",
            "2025-08-23 00:01:07,620 [INFO]   InvoiceId: INTEGER\n",
            "2025-08-23 00:01:07,620 [INFO]   TrackId: INTEGER\n",
            "2025-08-23 00:01:07,622 [INFO]   UnitPrice: NUMERIC(10, 2)\n",
            "2025-08-23 00:01:07,623 [INFO]   Quantity: INTEGER\n",
            "2025-08-23 00:01:07,626 [INFO] \n",
            "Table: MediaType\n",
            "2025-08-23 00:01:07,628 [INFO]   MediaTypeId: INTEGER\n",
            "2025-08-23 00:01:07,629 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-23 00:01:07,630 [INFO] \n",
            "Table: Playlist\n",
            "2025-08-23 00:01:07,631 [INFO]   PlaylistId: INTEGER\n",
            "2025-08-23 00:01:07,632 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-23 00:01:07,633 [INFO] \n",
            "Table: PlaylistTrack\n",
            "2025-08-23 00:01:07,634 [INFO]   PlaylistId: INTEGER\n",
            "2025-08-23 00:01:07,635 [INFO]   TrackId: INTEGER\n",
            "2025-08-23 00:01:07,637 [INFO] \n",
            "Table: Track\n",
            "2025-08-23 00:01:07,637 [INFO]   TrackId: INTEGER\n",
            "2025-08-23 00:01:07,638 [INFO]   Name: NVARCHAR(200)\n",
            "2025-08-23 00:01:07,638 [INFO]   AlbumId: INTEGER\n",
            "2025-08-23 00:01:07,639 [INFO]   MediaTypeId: INTEGER\n",
            "2025-08-23 00:01:07,641 [INFO]   GenreId: INTEGER\n",
            "2025-08-23 00:01:07,642 [INFO]   Composer: NVARCHAR(220)\n",
            "2025-08-23 00:01:07,643 [INFO]   Milliseconds: INTEGER\n",
            "2025-08-23 00:01:07,644 [INFO]   Bytes: INTEGER\n",
            "2025-08-23 00:01:07,645 [INFO]   UnitPrice: NUMERIC(10, 2)\n"
          ]
        }
      ],
      "source": [
        "def get_table_schema(table_name: str):\n",
        "    \"\"\"Fetch column names and types for an existing table.\"\"\"\n",
        "    columns = inspector.get_columns(table_name)\n",
        "    schema = {col[\"name\"]: str(col[\"type\"]) for col in columns}\n",
        "    return schema\n",
        "\n",
        "\n",
        "logger.info(\"Table Schemas\")\n",
        "for table_name in inspector.get_table_names():\n",
        "    schema = get_table_schema(table_name)\n",
        "    logger.info(f\"\\nTable: {table_name}\")\n",
        "    for col, dtype in schema.items():\n",
        "        logger.info(f\"  {col}: {dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
      "metadata": {
        "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
        "outputId": "4527a7d1-6b5f-4f89-9a63-e5263f4e4441"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
            "  next(self.gen)\n",
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
            "ðŸ“– For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
            "2025-08-23 00:01:19,947 [INFO] Phoenix launched and global handler set.\n"
          ]
        }
      ],
      "source": [
        "px.launch_app()\n",
        "set_global_handler(\"arize_phoenix\")\n",
        "logger.info(\"Phoenix launched and global handler set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266b9e05",
      "metadata": {},
      "source": [
        "1. Object index, retriever, SQLDatabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd2b7a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.llm.get_prompt_temp import RESPONSE_SYNTHESIS_PROMPT\n",
        "from utils.llm.get_llm_func import get_embedding_func\n",
        "\n",
        "\n",
        "def wrap_sql_engine(engine):\n",
        "    \"\"\"Wrap SQLAlchemy engine into LlamaIndex SQLDatabase + Node Mapping.\"\"\"\n",
        "    logger.info(\"Wrapping engine into LlamaIndex SQLDatabase\")\n",
        "    sql_database = SQLDatabase(engine)\n",
        "\n",
        "    logger.info(\"Creating table node mapping, i.e. mapping from SQL tables -> nodes\")\n",
        "    table_node_mapping = SQLTableNodeMapping(sql_database)\n",
        "\n",
        "    return sql_database, table_node_mapping\n",
        "\n",
        "def load_summaries_from_sqlite(summary_engine):\n",
        "    \"\"\"Load table summaries stored in SQLite DB.\"\"\"\n",
        "    logger.info(\"Loading all existing summaries from SQLite DB\")\n",
        "    with summary_engine.connect() as conn:\n",
        "        rows = conn.execute(\n",
        "            text(\"SELECT table_name, table_summary FROM table_summaries\")\n",
        "        ).fetchall()\n",
        "    return rows\n",
        "\n",
        "def filter_valid_summaries(rows, engine):\n",
        "    \"\"\"\n",
        "    Keep only summaries where the table still exists in the main DB.\n",
        "    Returns a list of SQLTableSchema objects.\n",
        "    \"\"\"\n",
        "    logger.info(\"Filtering out only valid tables from loaded summaries that exist in the db\")\n",
        "    table_schema_objs = []\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        inspector = inspect(conn)\n",
        "        existing_tables = inspector.get_table_names()\n",
        "\n",
        "    for row in rows:\n",
        "        if row.table_name in existing_tables and row.table_summary:\n",
        "            table_schema_objs.append(\n",
        "                SQLTableSchema(table_name=row.table_name, context_str=row.table_summary)\n",
        "            )\n",
        "            logger.info(f\"Adding table: {row.table_name} with summary: {row.table_summary}\")\n",
        "        else:\n",
        "            logger.warning(f\"Skipping missing/unextracted table: {row.table_name}\")\n",
        "\n",
        "    return table_schema_objs\n",
        "\n",
        "def load_summaries_from_json(sqlite_db_dir: Path):\n",
        "    \"\"\"Load table summaries stored in JSON file.\"\"\"\n",
        "    os.makedirs(sqlite_db_dir, exist_ok=True)\n",
        "    summary_db_path = os.path.join(sqlite_db_dir, \"table_summaries.json\")\n",
        "    \n",
        "    if not os.path.exists(summary_db_path):\n",
        "        logger.warning(f\"No summary JSON found at {summary_db_path}\")\n",
        "        return []\n",
        "\n",
        "    logger.info(f\"Loading summaries from JSON at {summary_db_path}\")\n",
        "    with open(summary_db_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summaries = json.load(f)\n",
        "\n",
        "    return summaries  # list[{\"table_name\": str, \"table_summary\": str}]\n",
        "\n",
        "\n",
        "def filter_valid_summaries_from_json(summaries, engine):\n",
        "    \"\"\"\n",
        "    Keep only summaries where the table still exists in the main DB.\n",
        "    Returns a list of SQLTableSchema objects.\n",
        "    \"\"\"\n",
        "    logger.info(\"Filtering JSON summaries to only valid tables\")\n",
        "    table_schema_objs = []\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        inspector = inspect(conn)\n",
        "        existing_tables = inspector.get_table_names()\n",
        "\n",
        "    for row in summaries:\n",
        "        table_name = row.get(\"table_name\")\n",
        "        table_summary = row.get(\"table_summary\")\n",
        "        if table_name in existing_tables and table_summary:\n",
        "            table_schema_objs.append(\n",
        "                SQLTableSchema(table_name=table_name, context_str=table_summary)\n",
        "            )\n",
        "            logger.info(f\"Adding table: {table_name} with summary: {table_summary}\")\n",
        "        else:\n",
        "            logger.warning(f\"Skipping missing/unextracted table: {table_name}\")\n",
        "\n",
        "    return table_schema_objs\n",
        "\n",
        "def build_object_index(table_schema_objs, table_node_mapping):\n",
        "    \"\"\"Build ObjectIndex for retrieval from table summaries.\"\"\"\n",
        "    logger.info(\"Building object index for table retrieval\")\n",
        "    obj_index = ObjectIndex.from_objects(\n",
        "        table_schema_objs,\n",
        "        table_node_mapping,\n",
        "        VectorStoreIndex,\n",
        "        embed_model=get_embedding_func(),\n",
        "    )\n",
        "    return obj_index\n",
        "\n",
        "\n",
        "def create_retrievers(sql_database, obj_index, top_k: int):\n",
        "    \"\"\"Create object retriever and SQL retriever.\"\"\"\n",
        "    logger.info(\"Creating retrievers for query execution\")\n",
        "    obj_retriever = obj_index.as_retriever(similarity_top_k=top_k)\n",
        "    sql_retriever = SQLRetriever(sql_database)\n",
        "    return obj_retriever, sql_retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a69ec83",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-23 00:09:17,026 [INFO] Wrapping engine into LlamaIndex SQLDatabase\n",
            "2025-08-23 00:09:17,058 [INFO] Creating table node mapping, i.e. mapping from SQL tables -> nodes\n",
            "2025-08-23 00:09:17,059 [INFO] Loading all existing summaries from SQLite DB\n",
            "2025-08-23 00:09:17,061 [INFO] Filtering out only valid tables from loaded summaries that exist in the db\n",
            "2025-08-23 00:09:17,063 [INFO] Adding table: Album with summary: Summary of album and artist data\n",
            "2025-08-23 00:09:17,063 [INFO] Adding table: Artist with summary: Summary of artist information\n",
            "2025-08-23 00:09:17,064 [INFO] Adding table: Customer with summary: Summary of customer data\n",
            "2025-08-23 00:09:17,065 [INFO] Adding table: Employee with summary: Summary of employee information\n",
            "2025-08-23 00:09:17,065 [INFO] Adding table: Genre with summary: Summary of genre data\n",
            "2025-08-23 00:09:17,067 [INFO] Adding table: Invoice with summary: Invoice Information\n",
            "2025-08-23 00:09:17,069 [INFO] Adding table: InvoiceLine with summary: Summary of invoice data\n",
            "2025-08-23 00:09:17,070 [INFO] Adding table: MediaType with summary: Summary of media data\n",
            "2025-08-23 00:09:17,071 [INFO] Adding table: Playlist with summary: Summary of playlist data including IDs and names\n",
            "2025-08-23 00:09:17,072 [INFO] Adding table: PlaylistTrack with summary: Summary of playlist and track data\n",
            "2025-08-23 00:09:17,072 [INFO] Adding table: Track with summary: Summary of track data\n",
            "2025-08-23 00:09:17,073 [INFO] Building object index for table retrieval\n",
            "2025-08-23 00:09:24,085 [INFO] Creating retrievers for query execution\n"
          ]
        }
      ],
      "source": [
        "sql_database, table_node_mapping = wrap_sql_engine(engine)\n",
        "\n",
        "rows = load_summaries_from_sqlite(summary_engine)\n",
        "table_schema_objs = filter_valid_summaries(rows, engine)\n",
        "\n",
        "# rows = load_summaries_from_json(SQLITE_DB_DIR)\n",
        "# table_schema_objs = filter_valid_summaries_from_json(rows, engine)\n",
        "\n",
        "obj_index = build_object_index(table_schema_objs, table_node_mapping)\n",
        "\n",
        "obj_retriever, sql_retriever = create_retrievers(sql_database, obj_index, top_k=TOP_K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b25bf248",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-23 00:28:59,488 [INFO] Table Context: Table 'Album' has columns: AlbumId (INTEGER), Title (NVARCHAR(160)), ArtistId (INTEGER),  and foreign keys: ['ArtistId'] -> Artist.['ArtistId']. The table description is: Summary of album and artist data\n",
            "\n",
            "Table 'Artist' has columns: ArtistId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of artist information\n",
            "\n",
            "Table 'Customer' has columns: CustomerId (INTEGER), FirstName (NVARCHAR(40)), LastName (NVARCHAR(20)), Company (NVARCHAR(80)), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)), SupportRepId (INTEGER),  and foreign keys: ['SupportRepId'] -> Employee.['EmployeeId']. The table description is: Summary of customer data\n",
            "\n",
            "Table 'Employee' has columns: EmployeeId (INTEGER), LastName (NVARCHAR(20)), FirstName (NVARCHAR(20)), Title (NVARCHAR(30)), ReportsTo (INTEGER), BirthDate (DATETIME), HireDate (DATETIME), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)),  and foreign keys: ['ReportsTo'] -> Employee.['EmployeeId']. The table description is: Summary of employee information\n",
            "\n",
            "Table 'Genre' has columns: GenreId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of genre data\n",
            "\n",
            "Table 'Invoice' has columns: InvoiceId (INTEGER), CustomerId (INTEGER), InvoiceDate (DATETIME), BillingAddress (NVARCHAR(70)), BillingCity (NVARCHAR(40)), BillingState (NVARCHAR(40)), BillingCountry (NVARCHAR(40)), BillingPostalCode (NVARCHAR(10)), Total (NUMERIC(10, 2)),  and foreign keys: ['CustomerId'] -> Customer.['CustomerId']. The table description is: Invoice Information\n",
            "\n",
            "Table 'InvoiceLine' has columns: InvoiceLineId (INTEGER), InvoiceId (INTEGER), TrackId (INTEGER), UnitPrice (NUMERIC(10, 2)), Quantity (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['InvoiceId'] -> Invoice.['InvoiceId']. The table description is: Summary of invoice data\n",
            "\n",
            "Table 'MediaType' has columns: MediaTypeId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of media data\n",
            "\n",
            "Table 'Playlist' has columns: PlaylistId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of playlist data including IDs and names\n",
            "\n",
            "Table 'PlaylistTrack' has columns: PlaylistId (INTEGER), TrackId (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['PlaylistId'] -> Playlist.['PlaylistId']. The table description is: Summary of playlist and track data\n",
            "\n",
            "Table 'Track' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2)),  and foreign keys: ['MediaTypeId'] -> MediaType.['MediaTypeId'], ['GenreId'] -> Genre.['GenreId'], ['AlbumId'] -> Album.['AlbumId']. The table description is: Summary of track data\n"
          ]
        }
      ],
      "source": [
        "# Table Context String\n",
        "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
        "    \"\"\"Get table context string (schema + summary).\"\"\"\n",
        "    context_strs = []\n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        try:\n",
        "            # pull schema directly from DB\n",
        "            table_info = sql_database.get_single_table_info(\n",
        "                table_schema_obj.table_name\n",
        "            )\n",
        "            if table_schema_obj.context_str:\n",
        "                table_opt_context = \" The table description is: \"\n",
        "                table_opt_context += table_schema_obj.context_str\n",
        "                table_info += table_opt_context\n",
        "            context_strs.append(table_info)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Skipping table {table_schema_obj.table_name}: {e}\")\n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "\n",
        "table_parser_component = get_table_context_str(table_schema_objs)\n",
        "logger.info(f\"Table Context: {table_parser_component}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9e8a31c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-23 00:29:58,766 [INFO] \n",
            " Text-to-SQL Prompt: Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
            "\n",
            "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
            "\n",
            "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
            "\n",
            "Question: Question here\n",
            "SQLQuery: SQL Query to run\n",
            "SQLResult: Result of the SQLQuery\n",
            "Answer: Final answer here\n",
            "\n",
            "Only use tables listed below.\n",
            "{schema}\n",
            "\n",
            "Question: {query_str}\n",
            "SQLQuery: \n"
          ]
        }
      ],
      "source": [
        "# SQL Output Parser\n",
        "def parse_response_to_sql(response: ChatResponse) -> str:\n",
        "    \"\"\"Parse response into a clean SQL string.\"\"\"\n",
        "    response = response.message.content\n",
        "\n",
        "    sql_query_start = response.find(\"SQLQuery:\")\n",
        "    if sql_query_start != -1:\n",
        "        response = response[sql_query_start:]\n",
        "        if response.startswith(\"SQLQuery:\"):\n",
        "            response = response[len(\"SQLQuery:\") :]\n",
        "\n",
        "    sql_result_start = response.find(\"SQLResult:\")\n",
        "    if sql_result_start != -1:\n",
        "        response = response[:sql_result_start]\n",
        "\n",
        "    return response.strip().strip(\"```\").strip()\n",
        "\n",
        "\n",
        "sql_parser_component = FunctionTool.from_defaults(fn=parse_response_to_sql)\n",
        "\n",
        "\n",
        "# Prompts\n",
        "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
        "    dialect=engine.dialect.name\n",
        ")\n",
        "logger.info(f\"\\n Text-to-SQL Prompt: {text2sql_prompt.template}\")\n",
        "\n",
        "\n",
        "response_synthesis_prompt = PromptTemplate(RESPONSE_SYNTHESIS_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15203320",
      "metadata": {},
      "source": [
        "### Index Each Table\n",
        "\n",
        "We embed/index the rows of each table, resulting in one index per table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b103cbc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-23 00:58:29,690 [INFO]  [00] Creating persistent Chroma client at: db\\Chinook\\chromadb\n",
            "2025-08-23 00:58:30,866 [INFO] [01] Processing table: Album\n",
            "2025-08-23 00:58:30,930 [INFO] [02] Building new index for empty collection: Album\n",
            "2025-08-23 00:58:30,935 [INFO] [02.2] Converting 347 rows to structured text\n",
            "2025-08-23 00:58:30,939 [INFO] [02.3] Creating vector store for table: Album\n",
            "2025-08-23 00:58:30,941 [INFO] [02.4] Building vector index with 347 nodes\n",
            "2025-08-23 00:58:36,539 [INFO] [02.5] Index created successfully for table: Album\n",
            "2025-08-23 00:58:36,543 [INFO] [04] Successfully indexed table: Album\n",
            "2025-08-23 00:58:36,544 [INFO] [01] Processing table: Artist\n",
            "2025-08-23 00:58:36,592 [INFO] [02] Building new index for empty collection: Artist\n",
            "2025-08-23 00:58:36,596 [INFO] [02.2] Converting 275 rows to structured text\n",
            "2025-08-23 00:58:36,600 [INFO] [02.3] Creating vector store for table: Artist\n",
            "2025-08-23 00:58:36,601 [INFO] [02.4] Building vector index with 275 nodes\n",
            "2025-08-23 00:58:40,239 [INFO] [02.5] Index created successfully for table: Artist\n",
            "2025-08-23 00:58:40,240 [INFO] [04] Successfully indexed table: Artist\n",
            "2025-08-23 00:58:40,241 [INFO] [01] Processing table: Customer\n",
            "2025-08-23 00:58:40,282 [INFO] [02] Building new index for empty collection: Customer\n",
            "2025-08-23 00:58:40,287 [INFO] [02.2] Converting 59 rows to structured text\n",
            "2025-08-23 00:58:40,290 [INFO] [02.3] Creating vector store for table: Customer\n",
            "2025-08-23 00:58:40,291 [INFO] [02.4] Building vector index with 59 nodes\n",
            "2025-08-23 00:58:42,984 [INFO] [02.5] Index created successfully for table: Customer\n",
            "2025-08-23 00:58:42,987 [INFO] [04] Successfully indexed table: Customer\n",
            "2025-08-23 00:58:42,989 [INFO] [01] Processing table: Employee\n",
            "2025-08-23 00:58:43,056 [INFO] [02] Building new index for empty collection: Employee\n",
            "2025-08-23 00:58:43,059 [INFO] [02.2] Converting 8 rows to structured text\n",
            "2025-08-23 00:58:43,060 [INFO] [02.3] Creating vector store for table: Employee\n",
            "2025-08-23 00:58:43,061 [INFO] [02.4] Building vector index with 8 nodes\n",
            "2025-08-23 00:58:43,795 [INFO] [02.5] Index created successfully for table: Employee\n",
            "2025-08-23 00:58:43,807 [INFO] [04] Successfully indexed table: Employee\n",
            "2025-08-23 00:58:43,809 [INFO] [01] Processing table: Genre\n",
            "2025-08-23 00:58:43,872 [INFO] [02] Building new index for empty collection: Genre\n",
            "2025-08-23 00:58:43,875 [INFO] [02.2] Converting 25 rows to structured text\n",
            "2025-08-23 00:58:43,876 [INFO] [02.3] Creating vector store for table: Genre\n",
            "2025-08-23 00:58:43,877 [INFO] [02.4] Building vector index with 25 nodes\n",
            "2025-08-23 00:58:44,558 [INFO] [02.5] Index created successfully for table: Genre\n",
            "2025-08-23 00:58:44,561 [INFO] [04] Successfully indexed table: Genre\n",
            "2025-08-23 00:58:44,564 [INFO] [01] Processing table: Invoice\n",
            "2025-08-23 00:58:44,624 [INFO] [02] Building new index for empty collection: Invoice\n",
            "2025-08-23 00:58:44,639 [INFO] [02.2] Converting 412 rows to structured text\n",
            "2025-08-23 00:58:44,645 [INFO] [02.3] Creating vector store for table: Invoice\n",
            "2025-08-23 00:58:44,648 [INFO] [02.4] Building vector index with 412 nodes\n",
            "2025-08-23 00:58:57,461 [INFO] [02.5] Index created successfully for table: Invoice\n",
            "2025-08-23 00:58:57,463 [INFO] [04] Successfully indexed table: Invoice\n",
            "2025-08-23 00:58:57,465 [INFO] [01] Processing table: InvoiceLine\n",
            "2025-08-23 00:58:57,512 [INFO] [02] Building new index for empty collection: InvoiceLine\n",
            "2025-08-23 00:58:57,529 [INFO] [02.2] Converting 2240 rows to structured text\n",
            "2025-08-23 00:58:57,553 [INFO] [02.3] Creating vector store for table: InvoiceLine\n",
            "2025-08-23 00:58:57,555 [INFO] [02.4] Building vector index with 2240 nodes\n",
            "2025-08-23 00:59:35,965 [INFO] [02.5] Index created successfully for table: InvoiceLine\n",
            "2025-08-23 00:59:35,968 [INFO] [04] Successfully indexed table: InvoiceLine\n",
            "2025-08-23 00:59:35,971 [INFO] [01] Processing table: MediaType\n",
            "2025-08-23 00:59:36,024 [INFO] [02] Building new index for empty collection: MediaType\n",
            "2025-08-23 00:59:36,027 [INFO] [02.2] Converting 5 rows to structured text\n",
            "2025-08-23 00:59:36,029 [INFO] [02.3] Creating vector store for table: MediaType\n",
            "2025-08-23 00:59:36,030 [INFO] [02.4] Building vector index with 5 nodes\n",
            "2025-08-23 00:59:36,256 [INFO] [02.5] Index created successfully for table: MediaType\n",
            "2025-08-23 00:59:36,260 [INFO] [04] Successfully indexed table: MediaType\n",
            "2025-08-23 00:59:36,262 [INFO] [01] Processing table: Playlist\n",
            "2025-08-23 00:59:36,318 [INFO] [02] Building new index for empty collection: Playlist\n",
            "2025-08-23 00:59:36,320 [INFO] [02.2] Converting 18 rows to structured text\n",
            "2025-08-23 00:59:36,321 [INFO] [02.3] Creating vector store for table: Playlist\n",
            "2025-08-23 00:59:36,323 [INFO] [02.4] Building vector index with 18 nodes\n",
            "2025-08-23 00:59:36,679 [INFO] [02.5] Index created successfully for table: Playlist\n",
            "2025-08-23 00:59:36,684 [INFO] [04] Successfully indexed table: Playlist\n",
            "2025-08-23 00:59:36,686 [INFO] [01] Processing table: PlaylistTrack\n",
            "2025-08-23 00:59:36,732 [INFO] [02] Building new index for empty collection: PlaylistTrack\n",
            "2025-08-23 00:59:36,760 [INFO] [02.2] Converting 8715 rows to structured text\n",
            "2025-08-23 00:59:36,849 [INFO] [02.3] Creating vector store for table: PlaylistTrack\n",
            "2025-08-23 00:59:36,853 [INFO] [02.4] Building vector index with 8715 nodes\n",
            "2025-08-23 01:00:46,614 [INFO] [02.5] Index created successfully for table: PlaylistTrack\n",
            "2025-08-23 01:00:46,616 [INFO] [04] Successfully indexed table: PlaylistTrack\n",
            "2025-08-23 01:00:46,618 [INFO] [01] Processing table: Track\n",
            "2025-08-23 01:00:46,673 [INFO] [02] Building new index for empty collection: Track\n",
            "2025-08-23 01:00:46,698 [INFO] [02.2] Converting 3503 rows to structured text\n",
            "2025-08-23 01:00:46,742 [INFO] [02.3] Creating vector store for table: Track\n",
            "2025-08-23 01:00:46,743 [INFO] [02.4] Building vector index with 3503 nodes\n",
            "2025-08-23 01:01:52,927 [INFO] [02.5] Index created successfully for table: Track\n",
            "2025-08-23 01:01:52,929 [INFO] [04] Successfully indexed table: Track\n",
            "2025-08-23 01:01:52,929 [INFO] [05] Successfully indexed 11 tables\n"
          ]
        }
      ],
      "source": [
        "def index_all_tables_with_chroma(sql_database, chroma_db_dir: str) -> Dict[str, VectorStoreIndex]:\n",
        "    \"\"\"Index all tables in the SQL database using ChromaDB as the backend.\n",
        "    Args:\n",
        "        sql_database: SQLDatabase instance\n",
        "        chroma_db_dir: Directory for ChromaDB persistence\n",
        "        \n",
        "    Returns:\n",
        "        Dict mapping table names to VectorStoreIndex instances\n",
        "    \"\"\"\n",
        "    os.makedirs(chroma_db_dir, exist_ok=True)\n",
        "\n",
        "    vector_index_dict = {}\n",
        "    engine = sql_database.engine\n",
        "\n",
        "    logger.info(f\" [00] Creating persistent Chroma client at: {chroma_db_dir}\")\n",
        "    chroma_client = chromadb.PersistentClient(path=chroma_db_dir)\n",
        "\n",
        "    for table_name in sql_database.get_usable_table_names():\n",
        "        logger.info(f\"[01] Processing table: {table_name}\")\n",
        "        \n",
        "        try:\n",
        "            # Create or get collection - ChromaDB handles persistence internally\n",
        "            collection = chroma_client.get_or_create_collection(name=f\"table_{table_name}\")\n",
        "            \n",
        "            # Check if collection already has data\n",
        "            if collection.count() == 0:\n",
        "                logger.info(f\"[02] Building new index for empty collection: {table_name}\")\n",
        "                \n",
        "                # Fetch data from database\n",
        "                with engine.connect() as conn:\n",
        "                    result = conn.execute(text(f'SELECT * FROM \"{table_name}\"'))\n",
        "                    col_names = list(result.keys())\n",
        "                    rows = result.fetchall()\n",
        "                \n",
        "                if not rows:\n",
        "                    logger.warning(f\"[02.1] Table {table_name} is empty, skipping...\")\n",
        "                    continue\n",
        "                \n",
        "                logger.info(f\"[02.2] Converting {len(rows)} rows to structured text\")\n",
        "                row_texts = [\n",
        "                    \" | \".join([f\"{col}={val}\" for col, val in zip(col_names, row)])\n",
        "                    for row in rows\n",
        "                ]\n",
        "                \n",
        "                # Create TextNodes with proper IDs\n",
        "                nodes = [\n",
        "                    TextNode(\n",
        "                        text=row_text, \n",
        "                        id_=f\"{table_name}_row_{idx}\"\n",
        "                    ) \n",
        "                    for idx, row_text in enumerate(row_texts)\n",
        "                ]\n",
        "                \n",
        "                logger.info(f\"[02.3] Creating vector store for table: {table_name}\")\n",
        "                vector_store = ChromaVectorStore(chroma_collection=collection)\n",
        "                \n",
        "                # Create index - this will automatically add nodes to ChromaDB\n",
        "                logger.info(f\"[02.4] Building vector index with {len(nodes)} nodes\")\n",
        "                storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "                index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "                \n",
        "                logger.info(f\"[02.5] Index created successfully for table: {table_name}\")\n",
        "                \n",
        "            else:\n",
        "                logger.info(f\"[03] Reusing existing collection with {collection.count()} items: {table_name}\")\n",
        "                \n",
        "                # Create vector store from existing collection\n",
        "                vector_store = ChromaVectorStore(chroma_collection=collection)\n",
        "                storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "                \n",
        "                # Create index from existing vector store\n",
        "                index = VectorStoreIndex.from_vector_store(\n",
        "                    vector_store=vector_store,\n",
        "                    storage_context=storage_context\n",
        "                )\n",
        "            \n",
        "            vector_index_dict[table_name] = index\n",
        "            logger.info(f\"[04] Successfully indexed table: {table_name}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"[ERROR] Failed to index table {table_name}: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    logger.info(f\"[05] Successfully indexed {len(vector_index_dict)} tables\")\n",
        "    return vector_index_dict\n",
        "\n",
        "\n",
        "# Build vector indexes for all tables using ChromaDB\n",
        "vector_index_dict = index_all_tables_with_chroma(sql_database, CHROMA_DB_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "31906c11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-23 01:44:27,736 [INFO] [01] Getting schema for table: Album\n",
            "2025-08-23 01:44:27,738 [INFO] [02] Retrieving example rows for table: Album\n",
            "2025-08-23 01:44:27,877 [INFO] [03] Retrieved 2 relevant nodes for table: Album\n",
            "2025-08-23 01:44:27,879 [INFO] [01] Getting schema for table: Artist\n",
            "2025-08-23 01:44:27,880 [INFO] [02] Retrieving example rows for table: Artist\n",
            "2025-08-23 01:44:27,939 [INFO] [03] Retrieved 2 relevant nodes for table: Artist\n",
            "2025-08-23 01:44:27,941 [INFO] [01] Getting schema for table: Customer\n",
            "2025-08-23 01:44:27,942 [INFO] [02] Retrieving example rows for table: Customer\n",
            "2025-08-23 01:44:27,995 [INFO] [03] Retrieved 2 relevant nodes for table: Customer\n",
            "2025-08-23 01:44:27,996 [INFO] [01] Getting schema for table: Employee\n",
            "2025-08-23 01:44:27,998 [INFO] [02] Retrieving example rows for table: Employee\n",
            "2025-08-23 01:44:28,099 [INFO] [03] Retrieved 2 relevant nodes for table: Employee\n",
            "2025-08-23 01:44:28,100 [INFO] [01] Getting schema for table: Genre\n",
            "2025-08-23 01:44:28,104 [INFO] [02] Retrieving example rows for table: Genre\n",
            "2025-08-23 01:44:28,249 [ERROR] [ERROR] Failed to get context for table Genre: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk\n",
            "2025-08-23 01:44:28,253 [INFO] [01] Getting schema for table: Invoice\n",
            "2025-08-23 01:44:28,255 [INFO] [02] Retrieving example rows for table: Invoice\n",
            "2025-08-23 01:44:28,342 [INFO] [03] Retrieved 2 relevant nodes for table: Invoice\n",
            "2025-08-23 01:44:28,343 [INFO] [01] Getting schema for table: InvoiceLine\n",
            "2025-08-23 01:44:28,344 [INFO] [02] Retrieving example rows for table: InvoiceLine\n",
            "2025-08-23 01:44:28,399 [INFO] [03] Retrieved 2 relevant nodes for table: InvoiceLine\n",
            "2025-08-23 01:44:28,401 [INFO] [01] Getting schema for table: MediaType\n",
            "2025-08-23 01:44:28,403 [INFO] [02] Retrieving example rows for table: MediaType\n",
            "2025-08-23 01:44:28,477 [INFO] [03] Retrieved 2 relevant nodes for table: MediaType\n",
            "2025-08-23 01:44:28,479 [INFO] [01] Getting schema for table: Playlist\n",
            "2025-08-23 01:44:28,481 [INFO] [02] Retrieving example rows for table: Playlist\n",
            "2025-08-23 01:44:28,642 [INFO] [03] Retrieved 2 relevant nodes for table: Playlist\n",
            "2025-08-23 01:44:28,647 [INFO] [01] Getting schema for table: PlaylistTrack\n",
            "2025-08-23 01:44:28,648 [INFO] [02] Retrieving example rows for table: PlaylistTrack\n",
            "2025-08-23 01:44:28,739 [INFO] [03] Retrieved 2 relevant nodes for table: PlaylistTrack\n",
            "2025-08-23 01:44:28,740 [INFO] [01] Getting schema for table: Track\n",
            "2025-08-23 01:44:28,741 [INFO] [02] Retrieving example rows for table: Track\n",
            "2025-08-23 01:44:28,840 [INFO] [03] Retrieved 2 relevant nodes for table: Track\n",
            "2025-08-23 01:44:28,843 [INFO] Updated table context with rows:\n",
            "Table 'Album' has columns: AlbumId (INTEGER), Title (NVARCHAR(160)), ArtistId (INTEGER),  and foreign keys: ['ArtistId'] -> Artist.['ArtistId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- AlbumId=71 | Title=Elis Regina-Minha HistÃ³ria | ArtistId=41\n",
            "- AlbumId=124 | Title=Cafezinho | ArtistId=97\n",
            "\n",
            "\n",
            "Table 'Artist' has columns: ArtistId (INTEGER), Name (NVARCHAR(120)), .\n",
            "Here are some relevant example rows (column=value):\n",
            "- ArtistId=240 | Name=Gustav Mahler\n",
            "- ArtistId=253 | Name=Calexico\n",
            "\n",
            "\n",
            "Table 'Customer' has columns: CustomerId (INTEGER), FirstName (NVARCHAR(40)), LastName (NVARCHAR(20)), Company (NVARCHAR(80)), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)), SupportRepId (INTEGER),  and foreign keys: ['SupportRepId'] -> Employee.['EmployeeId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- CustomerId=2 | FirstName=Leonie | LastName=KÃ¶hler | Company=None | Address=Theodor-Heuss-StraÃŸe 34 | City=Stuttgart | State=None | Country=Germany | PostalCode=70174 | Phone=+49 0711 2842222 | Fax=None | Email=leonekohler@surfeu.de | SupportRepId=5\n",
            "- CustomerId=6 | FirstName=Helena | LastName=HolÃ½ | Company=None | Address=RilskÃ¡ 3174/6 | City=Prague | State=None | Country=Czech Republic | PostalCode=14300 | Phone=+420 2 4177 0449 | Fax=None | Email=hholy@gmail.com | SupportRepId=5\n",
            "\n",
            "\n",
            "Table 'Employee' has columns: EmployeeId (INTEGER), LastName (NVARCHAR(20)), FirstName (NVARCHAR(20)), Title (NVARCHAR(30)), ReportsTo (INTEGER), BirthDate (DATETIME), HireDate (DATETIME), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)),  and foreign keys: ['ReportsTo'] -> Employee.['EmployeeId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- EmployeeId=4 | LastName=Park | FirstName=Margaret | Title=Sales Support Agent | ReportsTo=2 | BirthDate=1947-09-19 00:00:00 | HireDate=2003-05-03 00:00:00 | Address=683 10 Street SW | City=Calgary | State=AB | Country=Canada | PostalCode=T2P 5G3 | Phone=+1 (403) 263-4423 | Fax=+1 (403) 263-4289 | Email=margaret@chinookcorp.com\n",
            "- EmployeeId=1 | LastName=Adams | FirstName=Andrew | Title=General Manager | ReportsTo=None | BirthDate=1962-02-18 00:00:00 | HireDate=2002-08-14 00:00:00 | Address=11120 Jasper Ave NW | City=Edmonton | State=AB | Country=Canada | PostalCode=T5K 2N1 | Phone=+1 (780) 428-9482 | Fax=+1 (780) 428-3457 | Email=andrew@chinookcorp.com\n",
            "\n",
            "\n",
            "Table 'Genre' has columns: GenreId (INTEGER), Name (NVARCHAR(120)), .\n",
            "\n",
            "Table 'Invoice' has columns: InvoiceId (INTEGER), CustomerId (INTEGER), InvoiceDate (DATETIME), BillingAddress (NVARCHAR(70)), BillingCity (NVARCHAR(40)), BillingState (NVARCHAR(40)), BillingCountry (NVARCHAR(40)), BillingPostalCode (NVARCHAR(10)), Total (NUMERIC(10, 2)),  and foreign keys: ['CustomerId'] -> Customer.['CustomerId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- InvoiceId=83 | CustomerId=42 | InvoiceDate=2009-12-26 00:00:00 | BillingAddress=9, Place Louis Barthou | BillingCity=Bordeaux | BillingState=None | BillingCountry=France | BillingPostalCode=33000 | Total=0.99\n",
            "- InvoiceId=270 | CustomerId=42 | InvoiceDate=2012-03-29 00:00:00 | BillingAddress=9, Place Louis Barthou | BillingCity=Bordeaux | BillingState=None | BillingCountry=France | BillingPostalCode=33000 | Total=8.91\n",
            "\n",
            "\n",
            "Table 'InvoiceLine' has columns: InvoiceLineId (INTEGER), InvoiceId (INTEGER), TrackId (INTEGER), UnitPrice (NUMERIC(10, 2)), Quantity (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['InvoiceId'] -> Invoice.['InvoiceId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- InvoiceLineId=1724 | InvoiceId=319 | TrackId=3482 | UnitPrice=0.99 | Quantity=1\n",
            "- InvoiceLineId=1817 | InvoiceId=334 | TrackId=575 | UnitPrice=0.99 | Quantity=1\n",
            "\n",
            "\n",
            "Table 'MediaType' has columns: MediaTypeId (INTEGER), Name (NVARCHAR(120)), .\n",
            "Here are some relevant example rows (column=value):\n",
            "- MediaTypeId=4 | Name=Purchased AAC audio file\n",
            "- MediaTypeId=5 | Name=AAC audio file\n",
            "\n",
            "\n",
            "Table 'Playlist' has columns: PlaylistId (INTEGER), Name (NVARCHAR(120)), .\n",
            "Here are some relevant example rows (column=value):\n",
            "- PlaylistId=11 | Name=Brazilian Music\n",
            "- PlaylistId=15 | Name=Classical 101 - The Basics\n",
            "\n",
            "\n",
            "Table 'PlaylistTrack' has columns: PlaylistId (INTEGER), TrackId (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['PlaylistId'] -> Playlist.['PlaylistId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- PlaylistId=5 | TrackId=1821\n",
            "- PlaylistId=8 | TrackId=1732\n",
            "\n",
            "\n",
            "Table 'Track' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2)),  and foreign keys: ['MediaTypeId'] -> MediaType.['MediaTypeId'], ['GenreId'] -> Genre.['GenreId'], ['AlbumId'] -> Album.['AlbumId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- TrackId=3503 | Name=Koyaanisqatsi | AlbumId=347 | MediaTypeId=2 | GenreId=10 | Composer=Philip Glass | Milliseconds=206005 | Bytes=3305164 | UnitPrice=0.99\n",
            "- TrackId=1762 | Name=Panis Et Circenses | AlbumId=145 | MediaTypeId=1 | GenreId=7 | Composer=Caetano Veloso e Gilberto Gil | Milliseconds=192339 | Bytes=6318373 | UnitPrice=0.99\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_table_context_and_rows_str(query_str: str, table_schema_objs: List[TableInfo]) -> str:\n",
        "    \"\"\"Get table context string for relevant example rows.\n",
        "    Args:\n",
        "        query_str: Query string for similarity search\n",
        "        table_schema_objs: List of TableInfo objects\n",
        "        \n",
        "    Returns:\n",
        "        Combined context string for all tables\n",
        "    \"\"\"\n",
        "    context_strs = []\n",
        "    \n",
        "    for table_info_obj in table_schema_objs:\n",
        "        table_name = table_info_obj.table_name\n",
        "        \n",
        "        try:\n",
        "            logger.info(f\"[01] Getting schema for table: {table_name}\")\n",
        "            table_info = sql_database.get_single_table_info(table_name)\n",
        "            \n",
        "            # Check if we have a vector index for this table\n",
        "            if table_name not in vector_index_dict:\n",
        "                logger.warning(f\"[02] No vector index found for table: {table_name}\")\n",
        "                context_strs.append(table_info)\n",
        "                continue\n",
        "            \n",
        "            logger.info(f\"[02] Retrieving example rows for table: {table_name}\")\n",
        "            vector_retriever = vector_index_dict[table_name].as_retriever(\n",
        "                similarity_top_k=TOP_N\n",
        "            )\n",
        "            \n",
        "            relevant_nodes = vector_retriever.retrieve(query_str)\n",
        "            logger.info(f\"[03] Retrieved {len(relevant_nodes)} relevant nodes for table: {table_name}\")\n",
        "            \n",
        "            if relevant_nodes:\n",
        "                table_row_context = \"\\nHere are some relevant example rows (column=value):\\n\"\n",
        "                for node in relevant_nodes:\n",
        "                    table_row_context += f\"- {node.get_content()}\\n\"\n",
        "                table_info += table_row_context\n",
        "            else:\n",
        "                logger.info(f\"[03.1] No relevant rows found for query in table: {table_name}\")\n",
        "            \n",
        "            context_strs.append(table_info)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"[ERROR] Failed to get context for table {table_name}: {str(e)}\")\n",
        "            # Still add basic table info even if retrieval fails\n",
        "            try:\n",
        "                table_info = sql_database.get_single_table_info(table_name)\n",
        "                context_strs.append(table_info)\n",
        "            except Exception as schema_error:\n",
        "                logger.error(f\"[ERROR] Failed to get schema for table {table_name}: {str(schema_error)}\")\n",
        "    \n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "table_parser_component = get_table_context_and_rows_str(QUERY_TEXT, table_schema_objs)\n",
        "logger.info(f\"Updated table context with rows:\\n{table_parser_component}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4e02b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_table_context_str(table_schema_objs: List[SQLTableSchema]) -> Dict[str, str]:\n",
        "    \"\"\"Get table context (schema + summary) for multiple tables.\n",
        "    \n",
        "    Returns a dict {table_name: schema_context}\n",
        "    \"\"\"\n",
        "    table_context = {}\n",
        "    \n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        try:            \n",
        "            # pull schema directly from DB\n",
        "            table_info = sql_database.get_single_table_info(\n",
        "                table_schema_obj.table_name\n",
        "            )\n",
        "            if table_schema_obj.context_str:\n",
        "                table_info += f\" The table description is: {table_schema_obj.context_str}\"\n",
        "            \n",
        "            table_context[table_schema_obj.table_name] = table_info\n",
        "        \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Skipping table {table_schema_obj.table_name}: {e}\")\n",
        "    \n",
        "    return table_context\n",
        "\n",
        "def get_table_context_and_rows_str(query_str: str, table_schema_objs: List[TableInfo], top_n: int) -> str:\n",
        "    \"\"\"Get table context string (schema + relevant example rows).\"\"\"\n",
        "    \n",
        "    base_contexts = get_table_context_str(table_schema_objs)\n",
        "    context_strs = []\n",
        "    \n",
        "    for table_name, schema_context in base_contexts.items():\n",
        "        try:\n",
        "            # Check if we have a vector index for this table\n",
        "            if table_name not in vector_index_dict:\n",
        "                logger.warning(f\"No vector index found for table: {table_name}\")\n",
        "                context_strs.append(schema_context)\n",
        "                continue\n",
        "            \n",
        "            logger.info(f\"[01] Retrieving example rows for table: {table_name}\")\n",
        "            vector_retriever = vector_index_dict[table_name].as_retriever(\n",
        "                similarity_top_k=top_n\n",
        "            )\n",
        "            \n",
        "            relevant_nodes = vector_retriever.retrieve(query_str)\n",
        "            logger.info(f\"[02] Retrieved {len(relevant_nodes)} relevant nodes for table: {table_name}\")\n",
        "            \n",
        "            if relevant_nodes:\n",
        "                row_context = \"\\nHere are some relevant example rows (column=value):\\n\"\n",
        "                row_context += \"\\n\".join([f\"- {node.get_content()}\" for node in relevant_nodes])\n",
        "                schema_context += \"\\n\" + row_context\n",
        "            else:\n",
        "                logger.info(f\"[02.1] No relevant rows found for query in table: {table_name}\")\n",
        "            \n",
        "            context_strs.append(schema_context)\n",
        "        \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to enrich context for table {table_name}: {str(e)}\")\n",
        "            context_strs.append(schema_context)  # fallback to just schema\n",
        "    \n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "table_parser_component = get_table_context_and_rows_str(QUERY_TEXT, table_schema_objs)\n",
        "logger.info(f\"Updated table context with rows:\\n{table_parser_component}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab71cf00",
      "metadata": {},
      "source": [
        "### Define Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "22323164",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.workflow.custom_events import (\n",
        "    TableRetrievedEvent,\n",
        "    SchemaProcessedEvent,\n",
        "    SQLPromptReadyEvent,\n",
        "    SQLGeneratedEvent,\n",
        "    SQLParsedEvent,\n",
        "    SQLResultsEvent,\n",
        "    ResponsePromptReadyEvent,\n",
        ")\n",
        "from utils.workflow.custom_fallbacks import (\n",
        "    extract_sql_from_response,\n",
        "    analyze_sql_error,\n",
        "    create_t2s_prompt,\n",
        ")\n",
        "\n",
        "\n",
        "class Text2SQLWorkflowRowRetrieval(Workflow):\n",
        "    @step\n",
        "    async def input_step(self, ev: StartEvent) -> TableRetrievedEvent:\n",
        "        logger.info(f\"[Step 01] Process initial query and retrieve relevant tables\")\n",
        "        query = ev.query\n",
        "\n",
        "        logger.info(f\" - Use object retriever built from your table summaries\")\n",
        "        tables = obj_retriever.retrieve(query)  # candidate schemas\n",
        "        logger.info(f\" - Retrieved {len(tables)} candidate tables for query: {query}\")\n",
        "        \n",
        "        return TableRetrievedEvent(\n",
        "            tables=tables, \n",
        "            query_str=query\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def table_output_parser_step(self, ev: TableRetrievedEvent) -> SchemaProcessedEvent:\n",
        "        logger.info(f\"[Step 02] Parsing schemas and retrieving relevant rows for query: {ev.query_str}\")\n",
        "\n",
        "        logger.info(f\" - Enriching context function with vector row retrieval for tables: {ev.tables}\")\n",
        "        schema_str = get_table_context_and_rows_str(ev.query_str, ev.tables)\n",
        "        \n",
        "        return SchemaProcessedEvent(\n",
        "            table_schema=schema_str, \n",
        "            query_str=ev.query_str\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_prompt_step(self, ev: SchemaProcessedEvent | SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        logger.info(f\"[Step 03] Creating SQL prompt for query: {ev.query_str}\")\n",
        "        if isinstance(ev, SchemaProcessedEvent):\n",
        "            table_schema = ev.table_schema\n",
        "            query_str = ev.query_str\n",
        "            retry_count = 0\n",
        "            error_message = \"\"\n",
        "        else:\n",
        "            table_schema = getattr(ev, 'table_schema', '')\n",
        "            query_str = ev.query_str\n",
        "            retry_count = getattr(ev, 'retry_count', 0) + 1\n",
        "            error_message = getattr(ev, 'error_message', '')\n",
        "\n",
        "        prompt = create_t2s_prompt(table_schema, query_str, retry_count, error_message)\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=prompt,\n",
        "            query_str=query_str,\n",
        "            table_schema=table_schema,\n",
        "            retry_count=retry_count,\n",
        "            error_message=error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_llm_step(self, ev: SQLPromptReadyEvent) -> SQLGeneratedEvent:\n",
        "        logger.info(f\"[Step 04] Running LLM to generate SQL for query: {ev.query_str}\")\n",
        "        sql_response = await Settings.llm.acomplete(ev.t2s_prompt)\n",
        "        \n",
        "        return SQLGeneratedEvent(\n",
        "            sql_query=str(sql_response).strip(),\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_output_parser_step(self, ev: SQLGeneratedEvent) -> SQLParsedEvent:\n",
        "        logger.info(f\"[Step 05] Parsing LLM response to extract clean SQL for query: {ev.query_str}\")\n",
        "        try:\n",
        "            clean_sql = parse_response_to_sql(ev.sql_query)  # primary parser\n",
        "        except Exception:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query, logger)  # fallback\n",
        "        \n",
        "        if not clean_sql:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query, logger)\n",
        "\n",
        "        logger.info(f\"Attempt #{ev.retry_count + 1}\")\n",
        "        logger.info(f\"LLM Response: {ev.sql_query}\")\n",
        "        logger.info(f\"Cleaned SQL: {clean_sql}\")\n",
        "\n",
        "        return SQLParsedEvent(\n",
        "            sql_query=clean_sql,\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_retriever_step(self, ev: SQLParsedEvent) -> SQLResultsEvent:\n",
        "        logger.info(f\"[Step 06] Executing SQL for query: {ev.query_str}\")\n",
        "        try:\n",
        "            results = sql_retriever.retrieve(ev.sql_query)\n",
        "            logger.info(f\"[SUCCESS] Executed on Attempt #{ev.retry_count + 1}\")\n",
        "\n",
        "            return SQLResultsEvent(\n",
        "                context_str=str(results),\n",
        "                sql_query=ev.sql_query,\n",
        "                query_str=ev.query_str,\n",
        "                success=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            logger.error(f\"Execution failed (Attempt #{ev.retry_count + 1}): {error_msg}\")\n",
        "\n",
        "            if ev.retry_count < MAX_RETRIES:\n",
        "                retry_event = SQLResultsEvent(\n",
        "                    context_str=\"\",\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False,\n",
        "                    retry_count=ev.retry_count + 1,\n",
        "                )\n",
        "                retry_event.retry_count = ev.retry_count + 1\n",
        "                retry_event.error_message = analyze_sql_error(error_msg, ev.sql_query, ev.table_schema, logger)\n",
        "                retry_event.table_schema = ev.table_schema\n",
        "                \n",
        "                return retry_event\n",
        "            else:\n",
        "                return SQLResultsEvent(\n",
        "                    context_str=(f\"Failed after {MAX_RETRIES+1} attempts. Final error: {error_msg}\"),\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False,\n",
        "                    retry_count=ev.retry_count + 1,\n",
        "                )\n",
        "\n",
        "    @step\n",
        "    async def retry_handler_step(self, ev: SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        logger.info(f\"[Step 07] Handling retry for query: {ev.query_str}\")\n",
        "        if ev.success:\n",
        "            return None\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=\"\",  # regenerated later\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=getattr(ev, 'table_schema', ''),\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=getattr(ev, 'error_message', 'Unknown error')\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_prompt_step(self, ev: SQLResultsEvent) -> ResponsePromptReadyEvent:\n",
        "        logger.info(f\"[Step 08] Preparing synthesis prompt for query: {ev.query_str}\")\n",
        "        if not ev.success:\n",
        "            return None\n",
        "        prompt = response_synthesis_prompt.format(\n",
        "            query_str=ev.query_str,\n",
        "            context_str=ev.context_str,\n",
        "            sql_query=ev.sql_query\n",
        "        )\n",
        "        \n",
        "        return ResponsePromptReadyEvent(\n",
        "            query_str=ev.query_str,\n",
        "            rs_prompt=prompt\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_llm_step(self, ev: ResponsePromptReadyEvent) -> StopEvent:\n",
        "        logger.info(f\"[Step 09] Generating final answer for query: {ev.query_str}\")\n",
        "        answer = await Settings.llm.acomplete(ev.rs_prompt)\n",
        "        \n",
        "        return StopEvent(result=str(answer))\n",
        "\n",
        "\n",
        "# Runner\n",
        "async def run_text2sql_workflow_row(query: str):\n",
        "    workflow = Text2SQLWorkflowRowRetrieval(timeout=480)\n",
        "    result = await workflow.run(query=query)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e3daf393",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-23 01:46:17,928 [INFO] [Step 01] Process initial query and retrieve relevant tables\n",
            "2025-08-23 01:46:17,929 [INFO]  - Use object retriever built from your table summaries\n",
            "2025-08-23 01:46:17,991 [INFO]  - Retrieved 5 candidate tables for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:46:18,000 [INFO] [Step 02] Parsing schemas and retrieving relevant rows for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:46:18,002 [INFO]  - Enriching context function with vector row retrieval for tables: [SQLTableSchema(table_name='Invoice', context_str='Invoice Information'), SQLTableSchema(table_name='Customer', context_str='Summary of customer data'), SQLTableSchema(table_name='Employee', context_str='Summary of employee information'), SQLTableSchema(table_name='Artist', context_str='Summary of artist information'), SQLTableSchema(table_name='InvoiceLine', context_str='Summary of invoice data')]\n",
            "2025-08-23 01:46:18,002 [INFO] [01] Getting schema for table: Invoice\n",
            "2025-08-23 01:46:18,003 [INFO] [02] Retrieving example rows for table: Invoice\n",
            "2025-08-23 01:46:18,039 [INFO] [03] Retrieved 2 relevant nodes for table: Invoice\n",
            "2025-08-23 01:46:18,039 [INFO] [01] Getting schema for table: Customer\n",
            "2025-08-23 01:46:18,040 [INFO] [02] Retrieving example rows for table: Customer\n",
            "2025-08-23 01:46:18,088 [INFO] [03] Retrieved 2 relevant nodes for table: Customer\n",
            "2025-08-23 01:46:18,089 [INFO] [01] Getting schema for table: Employee\n",
            "2025-08-23 01:46:18,090 [INFO] [02] Retrieving example rows for table: Employee\n",
            "2025-08-23 01:46:18,160 [INFO] [03] Retrieved 2 relevant nodes for table: Employee\n",
            "2025-08-23 01:46:18,163 [INFO] [01] Getting schema for table: Artist\n",
            "2025-08-23 01:46:18,168 [INFO] [02] Retrieving example rows for table: Artist\n",
            "2025-08-23 01:46:18,229 [INFO] [03] Retrieved 2 relevant nodes for table: Artist\n",
            "2025-08-23 01:46:18,230 [INFO] [01] Getting schema for table: InvoiceLine\n",
            "2025-08-23 01:46:18,233 [INFO] [02] Retrieving example rows for table: InvoiceLine\n",
            "2025-08-23 01:46:18,287 [INFO] [03] Retrieved 2 relevant nodes for table: InvoiceLine\n",
            "2025-08-23 01:46:18,297 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:46:18,309 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:47:17,766 [INFO] [Step 05] Parsing LLM response to extract clean SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:47:17,808 [INFO] Extracting SQL from LLM response that might contain reasoning or formatting.\n",
            "2025-08-23 01:47:17,825 [INFO] Removing <think> blocks from response\n",
            "2025-08-23 01:47:17,846 [INFO] Removing non-SQL content at the beginning of response\n",
            "2025-08-23 01:47:17,851 [INFO]  [Method 1] Looking for SQLQuery: pattern\n",
            "2025-08-23 01:47:17,853 [INFO]  [Method 2] Looking for SQL in code blocks\n",
            "2025-08-23 01:47:17,871 [INFO]  [Method 3] Looking for standalone SQL statements\n",
            "2025-08-23 01:47:17,874 [INFO]  - Split by lines and look for SQL statements\n",
            "2025-08-23 01:47:17,875 [INFO]  - Checking line for SQL keywords: SELECT BillingCity FROM Customer WHERE CustomerId = 2;\n",
            "2025-08-23 01:47:17,881 [INFO] Attempt #1\n",
            "2025-08-23 01:47:17,886 [INFO] LLM Response: <think>\n",
            "Okay, let's see. The user is asking for the billing city of Leonie KÃ¶hler. First, I need to figure out which table contains the information about the customer's billing city. The customer table has columns like BillingCity, so that's probably where the answer is.\n",
            "\n",
            "Looking at the provided tables, the customer table has BillingCity as a column. The user provided an example row where BillingCity is Bordeaux. So, to find Leonie's billing city, I need to select the BillingCity from the Customer table where the CustomerId is 2 (from the example given in the table).\n",
            "\n",
            "Wait, the example rows show CustomerId=2 for Leonie. So the SQL query should select BillingCity from Customer where CustomerId = 2. That should give the correct result. Let me check if there are any other tables involved, but the question is specifically about the billing city, so only the Customer table is needed. No need to join with other tables. So the SQL query is straightforward.\n",
            "</think>\n",
            "\n",
            "SELECT BillingCity FROM Customer WHERE CustomerId = 2;\n",
            "2025-08-23 01:47:17,888 [INFO] Cleaned SQL: SELECT BillingCity FROM Customer WHERE CustomerId = 2\n",
            "2025-08-23 01:47:17,908 [INFO] [Step 06] Executing SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:47:18,236 [ERROR] Execution failed (Attempt #1): Statement 'SELECT BillingCity FROM Customer WHERE CustomerId = 2' is invalid SQL.\n",
            "Error: no such column: BillingCity\n",
            "2025-08-23 01:47:18,240 [INFO] Analyzing SQL error and provide suggestions for fixing the query.\n",
            "2025-08-23 01:47:18,241 [INFO] Detected 'no such column' error\n",
            "2025-08-23 01:47:18,241 [INFO] Extracting the problematic column name\n",
            "2025-08-23 01:47:18,244 [INFO] Looking for similar (correct) column names in the schema\n",
            "2025-08-23 01:47:18,274 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:47:18,288 [INFO] [Step 08] Preparing synthesis prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:47:18,303 [INFO] [Step 07] Handling retry for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:47:18,330 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:47:18,387 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:48:32,301 [INFO] [Step 05] Parsing LLM response to extract clean SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:48:32,311 [INFO] Extracting SQL from LLM response that might contain reasoning or formatting.\n",
            "2025-08-23 01:48:32,314 [INFO] Removing <think> blocks from response\n",
            "2025-08-23 01:48:32,315 [INFO] Removing non-SQL content at the beginning of response\n",
            "2025-08-23 01:48:32,317 [INFO]  [Method 1] Looking for SQLQuery: pattern\n",
            "2025-08-23 01:48:32,319 [INFO]  [Method 2] Looking for SQL in code blocks\n",
            "2025-08-23 01:48:32,325 [INFO]  [Method 3] Looking for standalone SQL statements\n",
            "2025-08-23 01:48:32,328 [INFO]  - Split by lines and look for SQL statements\n",
            "2025-08-23 01:48:32,332 [INFO]  - Checking line for SQL keywords: Certainly! To help you create a website, I need more specific details. Could you please share the following information?\n",
            "2025-08-23 01:48:32,337 [INFO]  - Checking line for SQL keywords: 1. **What is the purpose of the website?** (e.g., blog, product page, contact form, etc.)\n",
            "2025-08-23 01:48:32,339 [INFO]  - Checking line for SQL keywords: 2. **Do you want to create it from scratch, or is there existing content you want to integrate?**\n",
            "2025-08-23 01:48:32,344 [INFO]  - Checking line for SQL keywords: 3. **Do you have any specific design preferences or technical requirements?** (e.g., HTML, CSS, JavaScript, etc.)\n",
            "2025-08-23 01:48:32,352 [INFO]  - Checking line for SQL keywords: Once you provide this, I can guide you through the process! ðŸ› ï¸\n",
            "2025-08-23 01:48:32,361 [INFO]  [Method 4] Looking for multi-line SQL statements\n",
            "2025-08-23 01:48:32,399 [INFO] [Fallback] Could not extract SQL from response, return empty string to avoid errors\n",
            "2025-08-23 01:48:32,401 [WARNING] Could not extract SQL from response: Certainly! To help you create a website, I need more specific details. Could you please share the fo...\n",
            "2025-08-23 01:48:32,403 [INFO] Attempt #2\n",
            "2025-08-23 01:48:32,405 [INFO] LLM Response: <think>\n",
            "Okay, the user mentioned they want to know how to create a website, but they haven't provided specific details. I need to ask for more information so I can help them effectively. Let me think... They might be a beginner or an experienced user looking for guidance. They might need help with design, functionality, or something else. I should ask for their specific needs to tailor the response. Maybe they want to know the steps, so I should list them clearly. Also, make sure to keep the tone friendly and helpful.\n",
            "</think>\n",
            "\n",
            "Certainly! To help you create a website, I need more specific details. Could you please share the following information?  \n",
            "\n",
            "1. **What is the purpose of the website?** (e.g., blog, product page, contact form, etc.)  \n",
            "2. **Do you want to create it from scratch, or is there existing content you want to integrate?**  \n",
            "3. **Do you have any specific design preferences or technical requirements?** (e.g., HTML, CSS, JavaScript, etc.)  \n",
            "\n",
            "Once you provide this, I can guide you through the process! ðŸ› ï¸\n",
            "2025-08-23 01:48:32,416 [INFO] Cleaned SQL: SELECT none\n",
            "2025-08-23 01:48:32,463 [INFO] [Step 06] Executing SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:48:32,565 [ERROR] Execution failed (Attempt #2): Statement 'SELECT none' is invalid SQL.\n",
            "Error: no such column: none\n",
            "2025-08-23 01:48:32,570 [INFO] Analyzing SQL error and provide suggestions for fixing the query.\n",
            "2025-08-23 01:48:32,577 [INFO] Detected 'no such column' error\n",
            "2025-08-23 01:48:32,580 [INFO] Extracting the problematic column name\n",
            "2025-08-23 01:48:32,586 [INFO] Looking for similar (correct) column names in the schema\n",
            "2025-08-23 01:48:32,630 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:48:32,653 [INFO] [Step 08] Preparing synthesis prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:48:32,683 [INFO] [Step 07] Handling retry for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:48:32,718 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:48:32,741 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:00,030 [INFO] [Step 05] Parsing LLM response to extract clean SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:00,038 [INFO] Extracting SQL from LLM response that might contain reasoning or formatting.\n",
            "2025-08-23 01:50:00,039 [INFO] Removing <think> blocks from response\n",
            "2025-08-23 01:50:00,044 [INFO] Removing non-SQL content at the beginning of response\n",
            "2025-08-23 01:50:00,051 [INFO]  [Method 1] Looking for SQLQuery: pattern\n",
            "2025-08-23 01:50:00,053 [INFO]  [Method 2] Looking for SQL in code blocks\n",
            "2025-08-23 01:50:00,055 [INFO]  [Method 3] Looking for standalone SQL statements\n",
            "2025-08-23 01:50:00,058 [INFO]  - Split by lines and look for SQL statements\n",
            "2025-08-23 01:50:00,060 [INFO]  - Checking line for SQL keywords: SELECT BillingCity FROM Invoice WHERE CustomerId = 2;\n",
            "2025-08-23 01:50:00,062 [INFO] Attempt #3\n",
            "2025-08-23 01:50:00,064 [INFO] LLM Response: <think>\n",
            "Okay, let's see. The user is asking for the billing city of Leonie KÃ¶hler. From the provided schema, I need to find the BillingCity column in the Invoice table. The previous error mentioned that the column 'billingcity' doesn't exist, but looking at the Invoice table's columns, there's a BillingCity field named 'BillingCity' with type NVARCHAR(40). So the correct SQL query should select BillingCity from the Invoice table where CustomerId is 2 (since Leonie's CustomerId is 2 in the example rows). \n",
            "\n",
            "Wait, the example rows show InvoiceId=83 and 270 with CustomerId=42. But the user is asking about Leonie, whose CustomerId is 2. So the SQL query should join the Invoice table with the Customer table on CustomerId, then select BillingCity. But the user's question is about the billing city, so the correct query would be to get the BillingCity from the Invoice table where CustomerId is 2. \n",
            "\n",
            "So the corrected SQL query would be SELECT BillingCity FROM Invoice WHERE CustomerId = 2; that's the only part needed. No other columns are required here. Let me double-check the schema again to make sure there are no other columns involved. The Invoice table has BillingCity, so yes, that's correct.\n",
            "</think>\n",
            "\n",
            "SELECT BillingCity FROM Invoice WHERE CustomerId = 2;\n",
            "2025-08-23 01:50:00,066 [INFO] Cleaned SQL: SELECT BillingCity FROM Invoice WHERE CustomerId = 2\n",
            "2025-08-23 01:50:00,101 [INFO] [Step 06] Executing SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:00,216 [INFO] [SUCCESS] Executed on Attempt #3\n",
            "2025-08-23 01:50:00,237 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:00,258 [INFO] [Step 08] Preparing synthesis prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:00,278 [INFO] [Step 07] Handling retry for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:00,296 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:00,310 [INFO] [Step 09] Generating final answer for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:17,465 [INFO] [Step 05] Parsing LLM response to extract clean SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:17,473 [INFO] Extracting SQL from LLM response that might contain reasoning or formatting.\n",
            "2025-08-23 01:50:17,482 [INFO] Removing <think> blocks from response\n",
            "2025-08-23 01:50:17,485 [INFO] Removing non-SQL content at the beginning of response\n",
            "2025-08-23 01:50:17,487 [INFO]  [Method 1] Looking for SQLQuery: pattern\n",
            "2025-08-23 01:50:17,490 [INFO]  [Method 2] Looking for SQL in code blocks\n",
            "2025-08-23 01:50:17,492 [INFO]  [Method 3] Looking for standalone SQL statements\n",
            "2025-08-23 01:50:17,493 [INFO]  - Split by lines and look for SQL statements\n",
            "2025-08-23 01:50:17,495 [INFO]  - Checking line for SQL keywords: I'm sorry, but I don't have a query to respond to. Could you please provide more details or clarify your request?\n",
            "2025-08-23 01:50:17,498 [INFO]  [Method 4] Looking for multi-line SQL statements\n",
            "2025-08-23 01:50:17,500 [INFO] [Fallback] Could not extract SQL from response, return empty string to avoid errors\n",
            "2025-08-23 01:50:17,502 [WARNING] Could not extract SQL from response: I'm sorry, but I don't have a query to respond to. Could you please provide more details or clarify ...\n",
            "2025-08-23 01:50:17,503 [INFO] Attempt #3\n",
            "2025-08-23 01:50:17,505 [INFO] LLM Response: <think>\n",
            "Okay, the user is asking me to write a response to their query. Let me check the history to see what they wanted. They mentioned \"I need help with something\" and provided a query. But the query is empty. Maybe they just left it blank. I should respond politely and ask them to provide more details so I can assist better. Let me make sure to keep it friendly and open-ended.\n",
            "</think>\n",
            "\n",
            "I'm sorry, but I don't have a query to respond to. Could you please provide more details or clarify your request?\n",
            "2025-08-23 01:50:17,507 [INFO] Cleaned SQL: SELECT none\n",
            "2025-08-23 01:50:17,542 [INFO] [Step 06] Executing SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:17,621 [ERROR] Execution failed (Attempt #3): Statement 'SELECT none' is invalid SQL.\n",
            "Error: no such column: none\n",
            "2025-08-23 01:50:17,625 [INFO] Analyzing SQL error and provide suggestions for fixing the query.\n",
            "2025-08-23 01:50:17,628 [INFO] Detected 'no such column' error\n",
            "2025-08-23 01:50:17,629 [INFO] Extracting the problematic column name\n",
            "2025-08-23 01:50:17,635 [INFO] Looking for similar (correct) column names in the schema\n",
            "2025-08-23 01:50:17,665 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:17,688 [INFO] [Step 08] Preparing synthesis prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:17,716 [INFO] [Step 07] Handling retry for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:17,739 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:17,754 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:31,527 [INFO] [Step 05] Parsing LLM response to extract clean SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:31,533 [INFO] Extracting SQL from LLM response that might contain reasoning or formatting.\n",
            "2025-08-23 01:50:31,535 [INFO] Removing <think> blocks from response\n",
            "2025-08-23 01:50:31,537 [INFO] Removing non-SQL content at the beginning of response\n",
            "2025-08-23 01:50:31,538 [INFO]  [Method 1] Looking for SQLQuery: pattern\n",
            "2025-08-23 01:50:31,540 [INFO]  [Method 2] Looking for SQL in code blocks\n",
            "2025-08-23 01:50:31,548 [INFO]  [Method 3] Looking for standalone SQL statements\n",
            "2025-08-23 01:50:31,550 [INFO]  - Split by lines and look for SQL statements\n",
            "2025-08-23 01:50:31,551 [INFO]  - Checking line for SQL keywords: SELECT BillingCity FROM Invoice WHERE CustomerId = 42;\n",
            "2025-08-23 01:50:31,552 [INFO] Attempt #4\n",
            "2025-08-23 01:50:31,556 [INFO] LLM Response: <think>\n",
            "Okay, let's see. The user is asking for the billing city of Leonie KÃ¶hler. From the provided schema, I need to find the BillingCity column in the Invoice table. The example rows show that for InvoiceId 83, BillingCity is Bordeaux, and for InvoiceId 270, it's also Bordeaux. So the correct SQL query should select BillingCity from the Invoice table where CustomerId is 42. But wait, the previous error was about a column not existing, but in the schema, the BillingCity is listed. So the corrected query should be SELECT BillingCity FROM Invoice WHERE CustomerId = 42; that's the only part needed. I need to make sure that all columns are correctly referenced and that there are no typos. Let me double-check the column names. Yes, Invoice has BillingCity, and the CustomerId is a foreign key. So the final answer is that query.\n",
            "</think>\n",
            "\n",
            "SELECT BillingCity FROM Invoice WHERE CustomerId = 42;\n",
            "2025-08-23 01:50:31,558 [INFO] Cleaned SQL: SELECT BillingCity FROM Invoice WHERE CustomerId = 42\n",
            "2025-08-23 01:50:31,587 [INFO] [Step 06] Executing SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:31,640 [INFO] [SUCCESS] Executed on Attempt #4\n",
            "2025-08-23 01:50:31,661 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:31,681 [INFO] [Step 08] Preparing synthesis prompt for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:31,694 [INFO] [Step 07] Handling retry for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:31,708 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie KÃ¶hler?\n",
            "2025-08-23 01:50:31,720 [INFO] [Step 09] Generating final answer for query: What is the billing city of Leonie KÃ¶hler?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some tasks did not clean up within timeout\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, let's see. The user is asking about the billing city of Leonie KÃ¶hler, and they provided an SQL query that selects BillingCity from Invoice where CustomerId is 2. The SQL response shows that there are multiple entries with Stuttgart as the billing city. \n",
            "\n",
            "First, I need to parse the SQL response. The metadata part lists the result as [('Stuttgart',), ...], which means there are multiple entries, each with Stuttgart. The score is None, which might indicate that the query didn't find any results, but the user is asking about Leonie, so maybe there's a mistake in the query or the data. However, the key point here is that the billing city is consistently Stuttgart.\n",
            "\n",
            "So, the answer should directly state that Leonie KÃ¶hler's billing city is Stuttgart. Even though there are multiple entries, the city is the same across all. Therefore, the response is straightforward.\n",
            "</think>\n",
            "\n",
            "The billing city of Leonie KÃ¶hler is **Stuttgart**.\n"
          ]
        }
      ],
      "source": [
        "result = await run_text2sql_workflow_row(QUERY_TEXT)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2fdc7f5",
      "metadata": {},
      "source": [
        "Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0059c645",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def visualize_text2sql_workflow_row(sample_query: str, execution_name: str, output_dir: str = WORKFLOW_VISUALIZATION_DIR):\n",
        "    \"\"\"\n",
        "    Function to visualize the Text2SQL workflow in your version:\n",
        "    - Draws all possible flows\n",
        "    - Runs your row-retrieval Text2SQL workflow\n",
        "    - Draws execution path of the actual run\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    logger.info(\"[01] Drawing all possible flows...\")\n",
        "    all_flows_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_flow.html\")\n",
        "    draw_all_possible_flows(\n",
        "        Text2SQLWorkflowRowRetrieval,\n",
        "        filename=all_flows_path\n",
        "    )\n",
        "    logger.info(f\"[SUCCESS] All possible flows saved to: {all_flows_path}\")\n",
        "\n",
        "    logger.info(\"[02] Running workflow and drawing execution path...\")\n",
        "    try:\n",
        "        logger.info(\" - wrapper function instead of manual instantiation\")\n",
        "        result = await run_text2sql_workflow_row(sample_query)\n",
        "\n",
        "        logger.info(\" - Recreating workflow object for execution path drawing\")\n",
        "        workflow = Text2SQLWorkflowRowRetrieval(timeout=240)\n",
        "\n",
        "        execution_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_execution.html\")\n",
        "        draw_most_recent_execution(\n",
        "            workflow,\n",
        "            filename=execution_path\n",
        "        )\n",
        "        logger.info(f\"[SUCCESS] Recent execution path saved to: {execution_path}\")\n",
        "        logger.info(f\"Workflow result: {result.result}\")  \n",
        "        logger.debug(\"this `.result` holds final answer\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during workflow execution: {e}\")\n",
        "        logger.info(\"Note: Ensure retrievers + LLM configs are initialized correctly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04bffff",
      "metadata": {},
      "outputs": [],
      "source": [
        "await visualize_text2sql_workflow_row(QUERY_TEXT, QUERY_TEXT_INITIAL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-text-2-sql",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
