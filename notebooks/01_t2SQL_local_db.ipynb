{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "855f9f50-ef38-4069-932a-fb49af02d28e",
      "metadata": {
        "id": "855f9f50-ef38-4069-932a-fb49af02d28e"
      },
      "source": [
        "1. **Query-Time Table Retrieval**: Dynamically retrieve relevant tables in the text-to-SQL prompt.\n",
        "2. **Query-Time Sample Row retrieval**: Embed/Index each row, and dynamically retrieve example rows for each table in the text-to-SQL prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "997717d9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Hp\\\\Documents\\\\GitHub\\\\rag_text-2-sql\\\\notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ad056f1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Hp\\\\Documents\\\\GitHub\\\\rag_text-2-sql'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"../\")\n",
        "\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6bc8cd21",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from utils.helpers.other_imports import (\n",
        "    io,\n",
        "    time,\n",
        "    re,\n",
        "    requests,\n",
        "    zipfile,\n",
        "    shutil,\n",
        "    gc,\n",
        "    traceback,\n",
        "    json,\n",
        "    pyjson,\n",
        "    pd,\n",
        "    Path,\n",
        "    List,\n",
        "    Dict,\n",
        "    BaseModel,\n",
        "    Field,\n",
        "    px,\n",
        "    chromadb,\n",
        ")\n",
        "\n",
        "from utils.helpers.sql_alchemy_imports import (\n",
        "    create_engine,\n",
        "    text,\n",
        "    inspect,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        ")\n",
        "\n",
        "from utils.helpers.llama_index_imports import (\n",
        "    Settings, \n",
        "    SQLDatabase, \n",
        "    VectorStoreIndex, \n",
        "    ChromaVectorStore,\n",
        "    load_index_from_storage,\n",
        "    set_global_handler,\n",
        "    LLMTextCompletionProgram,\n",
        "    SQLTableNodeMapping,\n",
        "    ObjectIndex,\n",
        "    SQLTableSchema,\n",
        "    SQLRetriever,\n",
        "    DEFAULT_TEXT_TO_SQL_PROMPT,\n",
        "    PromptTemplate,\n",
        "    FunctionTool,\n",
        "    ChatResponse,\n",
        "    TextNode,\n",
        "    StorageContext,\n",
        "    Workflow,\n",
        "    step,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    draw_all_possible_flows,\n",
        "    draw_most_recent_execution,\n",
        ")\n",
        "\n",
        "from utils.config import CONFIG\n",
        "from utils.logger import setup_logger\n",
        "\n",
        "\n",
        "# configurations\n",
        "LOG_PATH = Path(CONFIG[\"LOG_PATH\"])\n",
        "\n",
        "CHINOOK_DBEAVER_DB_PATH = Path(CONFIG[\"CHINOOK_DBEAVER_DB_PATH\"])\n",
        "CHINOOK_TABLE_INDEX_DIR = Path(CONFIG[\"CHINOOK_TABLE_INDEX_DIR\"])\n",
        "SQLITE_DB_DIR = Path(CONFIG[\"SQLITE_DB_DIR\"])\n",
        "CHROMA_DB_DIR = Path(CONFIG[\"CHROMA_DB_DIR\"])\n",
        "\n",
        "WORKFLOW_VISUALIZATION_DIR = Path(CONFIG[\"WORKFLOW_VISUALIZATION_DIR\"])\n",
        "\n",
        "QUERY_1 = CONFIG[\"QUERY_1\"]\n",
        "QUERY_1_INITIAL = CONFIG[\"QUERY_1_INITIAL\"]\n",
        "\n",
        "TOP_K = CONFIG[\"TOP_K\"]\n",
        "TOP_N = CONFIG[\"TOP_N\"]\n",
        "MAX_RETRIES = CONFIG[\"MAX_RETRIES\"]\n",
        "\n",
        "\n",
        "# setup logging\n",
        "LOG_DIR = os.path.join(os.getcwd(), LOG_PATH)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)  # Create the logs directory if it doesn't exist\n",
        "\n",
        "# comment out line 15 in utils/logger.py -> only for notebooks\n",
        "LOG_FILE = os.path.join(LOG_DIR, \"db_connect_notebook.log\")\n",
        "logger = setup_logger(\"db_connect_notebook_logger\", LOG_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fe7acbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:18:26,295 [INFO] Creating SQLite DB Engine for the new summaries database: db\\Chinook\\sqlite\\table_summaries.db\n",
            "2025-08-20 21:18:26,298 [INFO]  - Ensuring the table exists (id, table_name, table_summary, created_at)\n",
            "2025-08-20 21:18:26,311 [INFO] Creating SQLite DB Engine for the existing Chinook database at C:\\Users\\Hp\\AppData\\Roaming\\DBeaverData\\workspace6\\.metadata\\sample-database-sqlite-1\\Chinook.db\n",
            "2025-08-20 21:18:26,315 [INFO] Generating table summaries...\n",
            "2025-08-20 21:18:26,316 [INFO] Fetching existing summaries from the summaries database...\n",
            "2025-08-20 21:18:26,318 [INFO] Found 0 existing summaries in DB\n",
            "2025-08-20 21:18:26,323 [INFO]  - Processing new table: Album\n",
            "2025-08-20 21:18:55,890 [INFO] Normalize LLM output\n",
            "2025-08-20 21:18:55,891 [INFO] Processed table: Album\n",
            "2025-08-20 21:18:55,892 [INFO] Saving table summary for Album immediately to summaries DB\n",
            "2025-08-20 21:18:55,902 [INFO]  - Processing new table: Artist\n",
            "2025-08-20 21:19:12,888 [INFO] Normalize LLM output\n",
            "2025-08-20 21:19:12,889 [INFO] Processed table: Artist\n",
            "2025-08-20 21:19:12,890 [INFO] Saving table summary for Artist immediately to summaries DB\n",
            "2025-08-20 21:19:12,898 [INFO]  - Processing new table: Customer\n",
            "2025-08-20 21:19:35,069 [ERROR] Error with attempt 1 for Customer: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 77 [type=json_invalid, input_value='{\"table_name\": \"customer...mary of customer data\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 21:19:52,148 [ERROR] Error with attempt 2 for Customer: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 84 [type=json_invalid, input_value='{\"table_name\": \"customer... customer information\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 21:20:09,073 [INFO] Normalize LLM output\n",
            "2025-08-20 21:20:09,074 [INFO] Processed table: Customer\n",
            "2025-08-20 21:20:09,075 [INFO] Saving table summary for Customer immediately to summaries DB\n",
            "2025-08-20 21:20:09,084 [INFO]  - Processing new table: Employee\n",
            "2025-08-20 21:20:36,851 [ERROR] Error with attempt 1 for Employee: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 79 [type=json_invalid, input_value='{\"table_name\": \"employee...yee Information Table\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 21:20:58,791 [INFO] Normalize LLM output\n",
            "2025-08-20 21:20:58,792 [INFO] Processed table: Employee\n",
            "2025-08-20 21:20:58,793 [INFO] Saving table summary for Employee immediately to summaries DB\n",
            "2025-08-20 21:20:58,801 [INFO]  - Processing new table: Genre\n",
            "2025-08-20 21:21:08,656 [INFO] Normalize LLM output\n",
            "2025-08-20 21:21:08,658 [INFO] Processed table: Genre\n",
            "2025-08-20 21:21:08,659 [INFO] Saving table summary for Genre immediately to summaries DB\n",
            "2025-08-20 21:21:08,666 [INFO]  - Processing new table: Invoice\n",
            "2025-08-20 21:21:31,079 [ERROR] Error with attempt 1 for Invoice: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 74 [type=json_invalid, input_value='{\"table_name\": \"invoice_... \"Invoice Information\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 21:21:43,521 [INFO] Normalize LLM output\n",
            "2025-08-20 21:21:43,523 [INFO] Processed table: Invoice\n",
            "2025-08-20 21:21:43,524 [INFO] Saving table summary for Invoice immediately to summaries DB\n",
            "2025-08-20 21:21:43,533 [INFO]  - Processing new table: InvoiceLine\n",
            "2025-08-20 21:21:59,489 [INFO] Normalize LLM output\n",
            "2025-08-20 21:21:59,490 [INFO] Processed table: InvoiceLine\n",
            "2025-08-20 21:21:59,492 [INFO] Saving table summary for InvoiceLine immediately to summaries DB\n",
            "2025-08-20 21:21:59,503 [INFO]  - Processing new table: MediaType\n",
            "2025-08-20 21:22:12,444 [INFO] Normalize LLM output\n",
            "2025-08-20 21:22:12,446 [INFO] Processed table: MediaType\n",
            "2025-08-20 21:22:12,448 [INFO] Saving table summary for MediaType immediately to summaries DB\n",
            "2025-08-20 21:22:12,460 [INFO]  - Processing new table: Playlist\n",
            "2025-08-20 21:22:24,057 [ERROR] Error with attempt 1 for Playlist: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 77 [type=json_invalid, input_value='{\"table_name\": \"playlist...mary of playlist data\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 21:22:35,661 [INFO] Normalize LLM output\n",
            "2025-08-20 21:22:35,662 [INFO] Processed table: Playlist\n",
            "2025-08-20 21:22:35,663 [INFO] Saving table summary for Playlist immediately to summaries DB\n",
            "2025-08-20 21:22:35,671 [INFO]  - Processing new table: PlaylistTrack\n",
            "2025-08-20 21:22:47,706 [INFO] Normalize LLM output\n",
            "2025-08-20 21:22:47,707 [INFO] Processed table: PlaylistTrack\n",
            "2025-08-20 21:22:47,708 [INFO] Saving table summary for PlaylistTrack immediately to summaries DB\n",
            "2025-08-20 21:22:47,717 [INFO]  - Processing new table: Track\n",
            "2025-08-20 21:23:09,782 [INFO] Normalize LLM output\n",
            "2025-08-20 21:23:09,783 [INFO] Processed table: Track\n",
            "2025-08-20 21:23:09,784 [INFO] Saving table summary for Track immediately to summaries DB\n",
            "2025-08-20 21:23:09,794 [INFO] \n",
            " FINAL TABLE SUMMARIES\n",
            "2025-08-20 21:23:09,795 [INFO] - Album: Summary of structured album data\n",
            "2025-08-20 21:23:09,796 [INFO] - Artist: Summary of artist information\n",
            "2025-08-20 21:23:09,797 [INFO] - Customer: Summary of customer data\n",
            "2025-08-20 21:23:09,798 [INFO] - Employee: Summary of employee information\n",
            "2025-08-20 21:23:09,799 [INFO] - Genre: Summary of genre data\n",
            "2025-08-20 21:23:09,800 [INFO] - Invoice: Summary of invoice data\n",
            "2025-08-20 21:23:09,800 [INFO] - InvoiceLine: Summary of invoice data entries\n",
            "2025-08-20 21:23:09,801 [INFO] - MediaType: Summary of media data\n",
            "2025-08-20 21:23:09,802 [INFO] - Playlist: Summary of playlist data\n",
            "2025-08-20 21:23:09,802 [INFO] - PlaylistTrack: Information about playlists and tracks structured by PlaylistId\n",
            "2025-08-20 21:23:09,803 [INFO] - Track: Summary of track data\n",
            "2025-08-20 21:23:09,804 [INFO] \n",
            "Saved 11 summaries to:\n",
            "2025-08-20 21:23:09,805 [INFO]  - SQLite DB: db\\Chinook\\sqlite\\table_summaries.db\n",
            "2025-08-20 21:23:09,806 [INFO]  - JSON backup: db\\Chinook\\sqlite\\table_summaries.json\n"
          ]
        }
      ],
      "source": [
        "from utils.llm.get_prompt_temp import TABLE_INFO_PROMPT\n",
        "from utils.llm.get_llm_func import get_llm_func\n",
        "\n",
        "\n",
        "class TableInfo(BaseModel):\n",
        "    \"\"\"Information regarding a structured table.\"\"\"\n",
        "\n",
        "    table_name: str = Field(\n",
        "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
        "    )\n",
        "    table_summary: str = Field(\n",
        "        ..., description=\"short, concise summary/caption of the table\"\n",
        "    )\n",
        "\n",
        "\n",
        "program = LLMTextCompletionProgram.from_defaults(\n",
        "    output_cls=TableInfo,\n",
        "    prompt_template_str=TABLE_INFO_PROMPT,\n",
        "    llm=get_llm_func(),\n",
        ")\n",
        "\n",
        "\n",
        "def extract_first_json_block(text: str):\n",
        "    logger.info(\"Extracting the first valid JSON object from text, ignoring extra trailing text.\")\n",
        "    \n",
        "    match = re.search(r\"\\{.*\\}\", text, re.S)\n",
        "    if not match:\n",
        "        logger.error(f\"No JSON object found in text: {text}\")\n",
        "        raise ValueError(\"No JSON object found in output\")\n",
        "    \n",
        "    try:\n",
        "        logger.info(f\"Extracted JSON: {match.group()}\")\n",
        "        return pyjson.loads(match.group())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to parse JSON: {e}\\nRaw text: {text}\")\n",
        "        raise ValueError(f\"Failed to parse JSON: {e}\\nRaw text: {text}\")\n",
        "\n",
        "\n",
        "os.makedirs(SQLITE_DB_DIR, exist_ok=True)\n",
        "SUMMARY_DB_PATH = os.path.join(SQLITE_DB_DIR, \"table_summaries.db\")\n",
        "\n",
        "logger.info(f\"Creating SQLite DB Engine for the new summaries database: {SUMMARY_DB_PATH}\")\n",
        "summary_engine = create_engine(f\"sqlite:///{SUMMARY_DB_PATH}\")\n",
        "\n",
        "logger.info(f\" - Ensuring the table exists (id, table_name, table_summary, created_at)\")\n",
        "with summary_engine.begin() as conn:\n",
        "    conn.execute(text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS table_summaries (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        table_name TEXT NOT NULL,\n",
        "        table_summary TEXT NOT NULL,\n",
        "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "    )\n",
        "    \"\"\"))\n",
        "\n",
        "table_infos = []\n",
        "\n",
        "logger.info(f\"Creating SQLite DB Engine for the existing Chinook database at {CHINOOK_DBEAVER_DB_PATH}\")\n",
        "engine = create_engine(f\"sqlite:///{CHINOOK_DBEAVER_DB_PATH}\")\n",
        "inspector = inspect(engine)\n",
        "\n",
        "logger.info(\"Generating table summaries...\")\n",
        "with engine.connect() as conn:\n",
        "    existing_tables = set()\n",
        "    \n",
        "    logger.info(\"Fetching existing summaries from the summaries database...\")\n",
        "    with summary_engine.connect() as summary_conn:\n",
        "        rows = summary_conn.execute(text(\"SELECT table_name FROM table_summaries\")).fetchall()\n",
        "        existing_tables = {row[0] for row in rows}\n",
        "        logger.info(f\"Found {len(existing_tables)} existing summaries in DB\")\n",
        "    \n",
        "    for idx, table in enumerate(inspector.get_table_names()):\n",
        "        if table in existing_tables:\n",
        "            logger.info(f\" - Skipping table '{table}' — summary already exists.\")\n",
        "            continue\n",
        "        \n",
        "        logger.info(f\" - Processing new table: {table}\")\n",
        "        df = pd.read_sql(f\"SELECT * FROM {table} LIMIT 10;\", conn)\n",
        "        df_str = df.to_csv(index=False)\n",
        "\n",
        "        table_info = None\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                raw_output = program(\n",
        "                    table_str=df_str,\n",
        "                    exclude_table_name_list=str(list(inspector.get_table_names())),\n",
        "                )\n",
        "\n",
        "                logger.info(f\"Normalize LLM output\")\n",
        "                if isinstance(raw_output, str):\n",
        "                    parsed_dict = extract_first_json_block(raw_output)\n",
        "                elif isinstance(raw_output, dict):\n",
        "                    parsed_dict = raw_output\n",
        "                elif isinstance(raw_output, TableInfo):\n",
        "                    parsed_dict = raw_output.model_dump()\n",
        "                else:\n",
        "                    logger.error(f\"Unexpected return type: {type(raw_output)}\")\n",
        "                    raise TypeError(f\"Unexpected return type: {type(raw_output)}\")\n",
        "\n",
        "                table_info = TableInfo(\n",
        "                    table_name=table,  # use actual SQLAlchemy inspector name\n",
        "                    table_summary=parsed_dict[\"table_summary\"],\n",
        "                )\n",
        "\n",
        "                logger.info(f\"Processed table: {table_info.table_name}\")\n",
        "                break  # success → next table\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error with attempt {attempt+1} for {table}: {e}\")\n",
        "                time.sleep(2)\n",
        "\n",
        "        if table_info:\n",
        "            table_infos.append(table_info)\n",
        "            \n",
        "            try:\n",
        "                logger.info(f\"Saving table summary for {table_info.table_name} immediately to summaries DB\")\n",
        "                with summary_engine.begin() as conn2:\n",
        "                    conn2.execute(\n",
        "                        text(\"INSERT INTO table_summaries (table_name, table_summary) VALUES (:name, :summary)\"),\n",
        "                        {\"name\": table_info.table_name, \"summary\": table_info.table_summary},\n",
        "                    )\n",
        "                \n",
        "                logger.debug(\"JSON dump for testing purposes only\")\n",
        "                json_path = os.path.join(SQLITE_DB_DIR, \"table_summaries.json\")\n",
        "                with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump([t.model_dump() for t in table_infos], f, indent=2, ensure_ascii=False)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to save table summary for {table_info.table_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "logger.info(\"\\n FINAL TABLE SUMMARIES\")\n",
        "for t in table_infos:\n",
        "    logger.info(f\"- {t.table_name}: {t.table_summary}\")\n",
        "\n",
        "logger.info(f\"\\nSaved {len(table_infos)} summaries to:\")\n",
        "logger.info(f\" - SQLite DB: {SUMMARY_DB_PATH}\")\n",
        "logger.info(f\" - JSON backup: {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "53544059-de7d-48dd-8e00-89517964852b",
      "metadata": {
        "id": "53544059-de7d-48dd-8e00-89517964852b",
        "outputId": "df224651-1765-4aa7-e841-d58546c20e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:23:09,832 [INFO] Table Schemas\n",
            "2025-08-20 21:23:09,839 [INFO] \n",
            "Table: Album\n",
            "2025-08-20 21:23:09,840 [INFO]   AlbumId: INTEGER\n",
            "2025-08-20 21:23:09,841 [INFO]   Title: NVARCHAR(160)\n",
            "2025-08-20 21:23:09,843 [INFO]   ArtistId: INTEGER\n",
            "2025-08-20 21:23:09,845 [INFO] \n",
            "Table: Artist\n",
            "2025-08-20 21:23:09,847 [INFO]   ArtistId: INTEGER\n",
            "2025-08-20 21:23:09,850 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 21:23:09,853 [INFO] \n",
            "Table: Customer\n",
            "2025-08-20 21:23:09,854 [INFO]   CustomerId: INTEGER\n",
            "2025-08-20 21:23:09,855 [INFO]   FirstName: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,856 [INFO]   LastName: NVARCHAR(20)\n",
            "2025-08-20 21:23:09,857 [INFO]   Company: NVARCHAR(80)\n",
            "2025-08-20 21:23:09,858 [INFO]   Address: NVARCHAR(70)\n",
            "2025-08-20 21:23:09,859 [INFO]   City: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,860 [INFO]   State: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,861 [INFO]   Country: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,864 [INFO]   PostalCode: NVARCHAR(10)\n",
            "2025-08-20 21:23:09,865 [INFO]   Phone: NVARCHAR(24)\n",
            "2025-08-20 21:23:09,866 [INFO]   Fax: NVARCHAR(24)\n",
            "2025-08-20 21:23:09,867 [INFO]   Email: NVARCHAR(60)\n",
            "2025-08-20 21:23:09,868 [INFO]   SupportRepId: INTEGER\n",
            "2025-08-20 21:23:09,870 [INFO] \n",
            "Table: Employee\n",
            "2025-08-20 21:23:09,871 [INFO]   EmployeeId: INTEGER\n",
            "2025-08-20 21:23:09,872 [INFO]   LastName: NVARCHAR(20)\n",
            "2025-08-20 21:23:09,873 [INFO]   FirstName: NVARCHAR(20)\n",
            "2025-08-20 21:23:09,874 [INFO]   Title: NVARCHAR(30)\n",
            "2025-08-20 21:23:09,875 [INFO]   ReportsTo: INTEGER\n",
            "2025-08-20 21:23:09,876 [INFO]   BirthDate: DATETIME\n",
            "2025-08-20 21:23:09,876 [INFO]   HireDate: DATETIME\n",
            "2025-08-20 21:23:09,877 [INFO]   Address: NVARCHAR(70)\n",
            "2025-08-20 21:23:09,878 [INFO]   City: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,878 [INFO]   State: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,879 [INFO]   Country: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,880 [INFO]   PostalCode: NVARCHAR(10)\n",
            "2025-08-20 21:23:09,881 [INFO]   Phone: NVARCHAR(24)\n",
            "2025-08-20 21:23:09,881 [INFO]   Fax: NVARCHAR(24)\n",
            "2025-08-20 21:23:09,884 [INFO]   Email: NVARCHAR(60)\n",
            "2025-08-20 21:23:09,887 [INFO] \n",
            "Table: Genre\n",
            "2025-08-20 21:23:09,888 [INFO]   GenreId: INTEGER\n",
            "2025-08-20 21:23:09,889 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 21:23:09,891 [INFO] \n",
            "Table: Invoice\n",
            "2025-08-20 21:23:09,892 [INFO]   InvoiceId: INTEGER\n",
            "2025-08-20 21:23:09,893 [INFO]   CustomerId: INTEGER\n",
            "2025-08-20 21:23:09,894 [INFO]   InvoiceDate: DATETIME\n",
            "2025-08-20 21:23:09,896 [INFO]   BillingAddress: NVARCHAR(70)\n",
            "2025-08-20 21:23:09,898 [INFO]   BillingCity: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,898 [INFO]   BillingState: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,899 [INFO]   BillingCountry: NVARCHAR(40)\n",
            "2025-08-20 21:23:09,900 [INFO]   BillingPostalCode: NVARCHAR(10)\n",
            "2025-08-20 21:23:09,901 [INFO]   Total: NUMERIC(10, 2)\n",
            "2025-08-20 21:23:09,903 [INFO] \n",
            "Table: InvoiceLine\n",
            "2025-08-20 21:23:09,904 [INFO]   InvoiceLineId: INTEGER\n",
            "2025-08-20 21:23:09,905 [INFO]   InvoiceId: INTEGER\n",
            "2025-08-20 21:23:09,905 [INFO]   TrackId: INTEGER\n",
            "2025-08-20 21:23:09,906 [INFO]   UnitPrice: NUMERIC(10, 2)\n",
            "2025-08-20 21:23:09,907 [INFO]   Quantity: INTEGER\n",
            "2025-08-20 21:23:09,909 [INFO] \n",
            "Table: MediaType\n",
            "2025-08-20 21:23:09,910 [INFO]   MediaTypeId: INTEGER\n",
            "2025-08-20 21:23:09,911 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 21:23:09,912 [INFO] \n",
            "Table: Playlist\n",
            "2025-08-20 21:23:09,913 [INFO]   PlaylistId: INTEGER\n",
            "2025-08-20 21:23:09,914 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 21:23:09,916 [INFO] \n",
            "Table: PlaylistTrack\n",
            "2025-08-20 21:23:09,918 [INFO]   PlaylistId: INTEGER\n",
            "2025-08-20 21:23:09,921 [INFO]   TrackId: INTEGER\n",
            "2025-08-20 21:23:09,923 [INFO] \n",
            "Table: Track\n",
            "2025-08-20 21:23:09,924 [INFO]   TrackId: INTEGER\n",
            "2025-08-20 21:23:09,925 [INFO]   Name: NVARCHAR(200)\n",
            "2025-08-20 21:23:09,926 [INFO]   AlbumId: INTEGER\n",
            "2025-08-20 21:23:09,927 [INFO]   MediaTypeId: INTEGER\n",
            "2025-08-20 21:23:09,928 [INFO]   GenreId: INTEGER\n",
            "2025-08-20 21:23:09,929 [INFO]   Composer: NVARCHAR(220)\n",
            "2025-08-20 21:23:09,929 [INFO]   Milliseconds: INTEGER\n",
            "2025-08-20 21:23:09,930 [INFO]   Bytes: INTEGER\n",
            "2025-08-20 21:23:09,931 [INFO]   UnitPrice: NUMERIC(10, 2)\n"
          ]
        }
      ],
      "source": [
        "def get_table_schema(table_name: str):\n",
        "    \"\"\"Fetch column names and types for an existing table.\"\"\"\n",
        "    columns = inspector.get_columns(table_name)\n",
        "    schema = {col[\"name\"]: str(col[\"type\"]) for col in columns}\n",
        "    return schema\n",
        "\n",
        "\n",
        "logger.info(\"Table Schemas\")\n",
        "for table_name in inspector.get_table_names():\n",
        "    schema = get_table_schema(table_name)\n",
        "    logger.info(f\"\\nTable: {table_name}\")\n",
        "    for col, dtype in schema.items():\n",
        "        logger.info(f\"  {col}: {dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
      "metadata": {
        "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
        "outputId": "4527a7d1-6b5f-4f89-9a63-e5263f4e4441"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
            "  next(self.gen)\n",
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
            "📖 For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
            "2025-08-20 21:23:12,188 [INFO] Phoenix launched and global handler set.\n"
          ]
        }
      ],
      "source": [
        "px.launch_app()\n",
        "set_global_handler(\"arize_phoenix\")\n",
        "\n",
        "# logger.info(\"🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\")\n",
        "# logger.info(\"📖 For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\")\n",
        "\n",
        "logger.info(\"Phoenix launched and global handler set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266b9e05",
      "metadata": {},
      "source": [
        "1. Object index, retriever, SQLDatabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019",
      "metadata": {
        "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:23:12,206 [INFO] Wrapping engine into LlamaIndex SQLDatabase\n",
            "2025-08-20 21:23:12,293 [INFO] Creating table node mapping, i.e. mapping from SQL tables -> nodes\n",
            "2025-08-20 21:23:12,295 [INFO] Loading all existing summaries from SQLite DB\n",
            "2025-08-20 21:23:12,297 [INFO] Filtering out only valid tables from loaded summaries that exist in the db\n",
            "2025-08-20 21:23:12,301 [INFO] Adding table: Album with summary: Summary of structured album data\n",
            "2025-08-20 21:23:12,303 [INFO] Adding table: Artist with summary: Summary of artist information\n",
            "2025-08-20 21:23:12,304 [INFO] Adding table: Customer with summary: Summary of customer data\n",
            "2025-08-20 21:23:12,305 [INFO] Adding table: Employee with summary: Summary of employee information\n",
            "2025-08-20 21:23:12,309 [INFO] Adding table: Genre with summary: Summary of genre data\n",
            "2025-08-20 21:23:12,313 [INFO] Adding table: Invoice with summary: Summary of invoice data\n",
            "2025-08-20 21:23:12,315 [INFO] Adding table: InvoiceLine with summary: Summary of invoice data entries\n",
            "2025-08-20 21:23:12,316 [INFO] Adding table: MediaType with summary: Summary of media data\n",
            "2025-08-20 21:23:12,317 [INFO] Adding table: Playlist with summary: Summary of playlist data\n",
            "2025-08-20 21:23:12,318 [INFO] Adding table: PlaylistTrack with summary: Information about playlists and tracks structured by PlaylistId\n",
            "2025-08-20 21:23:12,319 [INFO] Adding table: Track with summary: Summary of track data\n",
            "2025-08-20 21:23:12,320 [INFO] Building object index for table retrieval\n",
            "2025-08-20 21:23:17,284 [INFO] Creating SQL retriever for query execution\n"
          ]
        }
      ],
      "source": [
        "from utils.llm.get_llm_func import get_embedding_func\n",
        "from utils.llm.get_prompt_temp import RESPONSE_SYNTHESIS_PROMPT\n",
        "\n",
        "\n",
        "logger.info(\"Wrapping engine into LlamaIndex SQLDatabase\")\n",
        "sql_database = SQLDatabase(engine)\n",
        "\n",
        "logger.info(\"Creating table node mapping, i.e. mapping from SQL tables -> nodes\")\n",
        "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
        "\n",
        "logger.info(\"Loading all existing summaries from SQLite DB\")\n",
        "with summary_engine.connect() as conn:\n",
        "    rows = conn.execute(text(\"SELECT table_name, table_summary FROM table_summaries\")).fetchall()\n",
        "\n",
        "logger.info(\"Filtering out only valid tables from loaded summaries that exist in the db\")\n",
        "table_schema_objs = []\n",
        "\n",
        "with engine.connect() as conn:\n",
        "    inspector = inspect(conn)\n",
        "    existing_tables = inspector.get_table_names()\n",
        "\n",
        "# for t in table_infos:\n",
        "#     if t.table_name in existing_tables and t.table_summary:\n",
        "#         table_schema_objs.append(\n",
        "#             SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
        "#         )\n",
        "#         logger.info(f\"Adding table: {t.table_name} with summary: {t.table_summary}\")\n",
        "#     else:\n",
        "#         logger.warning(f\"Skipping missing/unextracted table: {t.table_name}\")\n",
        "\n",
        "for row in rows:\n",
        "    if row.table_name in existing_tables and row.table_summary:\n",
        "        table_schema_objs.append(\n",
        "            SQLTableSchema(table_name=row.table_name, context_str=row.table_summary)\n",
        "        )\n",
        "        logger.info(f\"Adding table: {row.table_name} with summary: {row.table_summary}\")\n",
        "    else:\n",
        "        logger.warning(f\"Skipping missing/unextracted table: {row.table_name}\")\n",
        "\n",
        "logger.info(\"Building object index for table retrieval\")\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    table_schema_objs,\n",
        "    table_node_mapping,\n",
        "    VectorStoreIndex,\n",
        "    embed_model=get_embedding_func(),\n",
        ")\n",
        "obj_retriever = obj_index.as_retriever(similarity_top_k=TOP_K)\n",
        "\n",
        "logger.info(\"Creating SQL retriever for query execution\")\n",
        "sql_retriever = SQLRetriever(sql_database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b25bf248",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:23:17,323 [INFO] Table Context: Table 'Album' has columns: AlbumId (INTEGER), Title (NVARCHAR(160)), ArtistId (INTEGER),  and foreign keys: ['ArtistId'] -> Artist.['ArtistId']. The table description is: Summary of structured album data\n",
            "\n",
            "Table 'Artist' has columns: ArtistId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of artist information\n",
            "\n",
            "Table 'Customer' has columns: CustomerId (INTEGER), FirstName (NVARCHAR(40)), LastName (NVARCHAR(20)), Company (NVARCHAR(80)), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)), SupportRepId (INTEGER),  and foreign keys: ['SupportRepId'] -> Employee.['EmployeeId']. The table description is: Summary of customer data\n",
            "\n",
            "Table 'Employee' has columns: EmployeeId (INTEGER), LastName (NVARCHAR(20)), FirstName (NVARCHAR(20)), Title (NVARCHAR(30)), ReportsTo (INTEGER), BirthDate (DATETIME), HireDate (DATETIME), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)),  and foreign keys: ['ReportsTo'] -> Employee.['EmployeeId']. The table description is: Summary of employee information\n",
            "\n",
            "Table 'Genre' has columns: GenreId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of genre data\n",
            "\n",
            "Table 'Invoice' has columns: InvoiceId (INTEGER), CustomerId (INTEGER), InvoiceDate (DATETIME), BillingAddress (NVARCHAR(70)), BillingCity (NVARCHAR(40)), BillingState (NVARCHAR(40)), BillingCountry (NVARCHAR(40)), BillingPostalCode (NVARCHAR(10)), Total (NUMERIC(10, 2)),  and foreign keys: ['CustomerId'] -> Customer.['CustomerId']. The table description is: Summary of invoice data\n",
            "\n",
            "Table 'InvoiceLine' has columns: InvoiceLineId (INTEGER), InvoiceId (INTEGER), TrackId (INTEGER), UnitPrice (NUMERIC(10, 2)), Quantity (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['InvoiceId'] -> Invoice.['InvoiceId']. The table description is: Summary of invoice data entries\n",
            "\n",
            "Table 'MediaType' has columns: MediaTypeId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of media data\n",
            "\n",
            "Table 'Playlist' has columns: PlaylistId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of playlist data\n",
            "\n",
            "Table 'PlaylistTrack' has columns: PlaylistId (INTEGER), TrackId (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['PlaylistId'] -> Playlist.['PlaylistId']. The table description is: Information about playlists and tracks structured by PlaylistId\n",
            "\n",
            "Table 'Track' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2)),  and foreign keys: ['MediaTypeId'] -> MediaType.['MediaTypeId'], ['GenreId'] -> Genre.['GenreId'], ['AlbumId'] -> Album.['AlbumId']. The table description is: Summary of track data\n"
          ]
        }
      ],
      "source": [
        "# Table Context String\n",
        "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
        "    \"\"\"Get table context string (schema + summary).\"\"\"\n",
        "    context_strs = []\n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        try:\n",
        "            # pull schema directly from DB\n",
        "            table_info = sql_database.get_single_table_info(\n",
        "                table_schema_obj.table_name\n",
        "            )\n",
        "            if table_schema_obj.context_str:\n",
        "                table_opt_context = \" The table description is: \"\n",
        "                table_opt_context += table_schema_obj.context_str\n",
        "                table_info += table_opt_context\n",
        "            context_strs.append(table_info)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Skipping table {table_schema_obj.table_name}: {e}\")\n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "\n",
        "table_parser_component = get_table_context_str(table_schema_objs)\n",
        "logger.info(f\"Table Context: {table_parser_component}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9e8a31c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:23:17,363 [INFO] \n",
            " Text-to-SQL Prompt: Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
            "\n",
            "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
            "\n",
            "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
            "\n",
            "Question: Question here\n",
            "SQLQuery: SQL Query to run\n",
            "SQLResult: Result of the SQLQuery\n",
            "Answer: Final answer here\n",
            "\n",
            "Only use tables listed below.\n",
            "{schema}\n",
            "\n",
            "Question: {query_str}\n",
            "SQLQuery: \n"
          ]
        }
      ],
      "source": [
        "# SQL Output Parser\n",
        "def parse_response_to_sql(response: ChatResponse) -> str:\n",
        "    \"\"\"Parse response into a clean SQL string.\"\"\"\n",
        "    response = response.message.content\n",
        "\n",
        "    sql_query_start = response.find(\"SQLQuery:\")\n",
        "    if sql_query_start != -1:\n",
        "        response = response[sql_query_start:]\n",
        "        if response.startswith(\"SQLQuery:\"):\n",
        "            response = response[len(\"SQLQuery:\") :]\n",
        "\n",
        "    sql_result_start = response.find(\"SQLResult:\")\n",
        "    if sql_result_start != -1:\n",
        "        response = response[:sql_result_start]\n",
        "\n",
        "    return response.strip().strip(\"```\").strip()\n",
        "\n",
        "\n",
        "sql_parser_component = FunctionTool.from_defaults(fn=parse_response_to_sql)\n",
        "\n",
        "\n",
        "# Prompts\n",
        "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
        "    dialect=engine.dialect.name\n",
        ")\n",
        "logger.info(f\"\\n Text-to-SQL Prompt: {text2sql_prompt.template}\")\n",
        "\n",
        "\n",
        "response_synthesis_prompt = PromptTemplate(RESPONSE_SYNTHESIS_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15203320",
      "metadata": {},
      "source": [
        "### Index Each Table\n",
        "\n",
        "We embed/index the rows of each table, resulting in one index per table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b103cbc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:23:17,394 [INFO]  [00] Creating persistent Chroma client at: db\\Chinook\\chromadb\n",
            "2025-08-20 21:23:18,107 [INFO] [01] Processing table: Album\n",
            "2025-08-20 21:23:18,174 [INFO] [02] Building new index for empty collection: Album\n",
            "2025-08-20 21:23:18,176 [INFO] [02.2] Converting 347 rows to structured text\n",
            "2025-08-20 21:23:18,178 [INFO] [02.3] Creating vector store for table: Album\n",
            "2025-08-20 21:23:18,179 [INFO] [02.4] Building vector index with 347 nodes\n",
            "2025-08-20 21:23:21,187 [INFO] [02.5] Index created successfully for table: Album\n",
            "2025-08-20 21:23:21,188 [INFO] [04] Successfully indexed table: Album\n",
            "2025-08-20 21:23:21,188 [INFO] [01] Processing table: Artist\n",
            "2025-08-20 21:23:21,220 [INFO] [02] Building new index for empty collection: Artist\n",
            "2025-08-20 21:23:21,225 [INFO] [02.2] Converting 275 rows to structured text\n",
            "2025-08-20 21:23:21,226 [INFO] [02.3] Creating vector store for table: Artist\n",
            "2025-08-20 21:23:21,227 [INFO] [02.4] Building vector index with 275 nodes\n",
            "2025-08-20 21:23:23,728 [INFO] [02.5] Index created successfully for table: Artist\n",
            "2025-08-20 21:23:23,730 [INFO] [04] Successfully indexed table: Artist\n",
            "2025-08-20 21:23:23,732 [INFO] [01] Processing table: Customer\n",
            "2025-08-20 21:23:23,759 [INFO] [02] Building new index for empty collection: Customer\n",
            "2025-08-20 21:23:23,763 [INFO] [02.2] Converting 59 rows to structured text\n",
            "2025-08-20 21:23:23,765 [INFO] [02.3] Creating vector store for table: Customer\n",
            "2025-08-20 21:23:23,766 [INFO] [02.4] Building vector index with 59 nodes\n",
            "2025-08-20 21:23:25,559 [INFO] [02.5] Index created successfully for table: Customer\n",
            "2025-08-20 21:23:25,563 [INFO] [04] Successfully indexed table: Customer\n",
            "2025-08-20 21:23:25,565 [INFO] [01] Processing table: Employee\n",
            "2025-08-20 21:23:25,603 [INFO] [02] Building new index for empty collection: Employee\n",
            "2025-08-20 21:23:25,605 [INFO] [02.2] Converting 8 rows to structured text\n",
            "2025-08-20 21:23:25,608 [INFO] [02.3] Creating vector store for table: Employee\n",
            "2025-08-20 21:23:25,609 [INFO] [02.4] Building vector index with 8 nodes\n",
            "2025-08-20 21:23:26,015 [INFO] [02.5] Index created successfully for table: Employee\n",
            "2025-08-20 21:23:26,015 [INFO] [04] Successfully indexed table: Employee\n",
            "2025-08-20 21:23:26,017 [INFO] [01] Processing table: Genre\n",
            "2025-08-20 21:23:26,049 [INFO] [02] Building new index for empty collection: Genre\n",
            "2025-08-20 21:23:26,051 [INFO] [02.2] Converting 25 rows to structured text\n",
            "2025-08-20 21:23:26,052 [INFO] [02.3] Creating vector store for table: Genre\n",
            "2025-08-20 21:23:26,053 [INFO] [02.4] Building vector index with 25 nodes\n",
            "2025-08-20 21:23:26,413 [INFO] [02.5] Index created successfully for table: Genre\n",
            "2025-08-20 21:23:26,415 [INFO] [04] Successfully indexed table: Genre\n",
            "2025-08-20 21:23:26,416 [INFO] [01] Processing table: Invoice\n",
            "2025-08-20 21:23:26,480 [INFO] [02] Building new index for empty collection: Invoice\n",
            "2025-08-20 21:23:26,490 [INFO] [02.2] Converting 412 rows to structured text\n",
            "2025-08-20 21:23:26,495 [INFO] [02.3] Creating vector store for table: Invoice\n",
            "2025-08-20 21:23:26,496 [INFO] [02.4] Building vector index with 412 nodes\n",
            "2025-08-20 21:23:34,340 [INFO] [02.5] Index created successfully for table: Invoice\n",
            "2025-08-20 21:23:34,341 [INFO] [04] Successfully indexed table: Invoice\n",
            "2025-08-20 21:23:34,341 [INFO] [01] Processing table: InvoiceLine\n",
            "2025-08-20 21:23:34,370 [INFO] [02] Building new index for empty collection: InvoiceLine\n",
            "2025-08-20 21:23:34,383 [INFO] [02.2] Converting 2240 rows to structured text\n",
            "2025-08-20 21:23:34,398 [INFO] [02.3] Creating vector store for table: InvoiceLine\n",
            "2025-08-20 21:23:34,399 [INFO] [02.4] Building vector index with 2240 nodes\n",
            "2025-08-20 21:23:57,044 [INFO] [02.5] Index created successfully for table: InvoiceLine\n",
            "2025-08-20 21:23:57,046 [INFO] [04] Successfully indexed table: InvoiceLine\n",
            "2025-08-20 21:23:57,047 [INFO] [01] Processing table: MediaType\n",
            "2025-08-20 21:23:57,093 [INFO] [02] Building new index for empty collection: MediaType\n",
            "2025-08-20 21:23:57,096 [INFO] [02.2] Converting 5 rows to structured text\n",
            "2025-08-20 21:23:57,098 [INFO] [02.3] Creating vector store for table: MediaType\n",
            "2025-08-20 21:23:57,099 [INFO] [02.4] Building vector index with 5 nodes\n",
            "2025-08-20 21:23:57,271 [INFO] [02.5] Index created successfully for table: MediaType\n",
            "2025-08-20 21:23:57,273 [INFO] [04] Successfully indexed table: MediaType\n",
            "2025-08-20 21:23:57,274 [INFO] [01] Processing table: Playlist\n",
            "2025-08-20 21:23:57,306 [INFO] [02] Building new index for empty collection: Playlist\n",
            "2025-08-20 21:23:57,308 [INFO] [02.2] Converting 18 rows to structured text\n",
            "2025-08-20 21:23:57,309 [INFO] [02.3] Creating vector store for table: Playlist\n",
            "2025-08-20 21:23:57,310 [INFO] [02.4] Building vector index with 18 nodes\n",
            "2025-08-20 21:23:57,572 [INFO] [02.5] Index created successfully for table: Playlist\n",
            "2025-08-20 21:23:57,577 [INFO] [04] Successfully indexed table: Playlist\n",
            "2025-08-20 21:23:57,579 [INFO] [01] Processing table: PlaylistTrack\n",
            "2025-08-20 21:23:57,615 [INFO] [02] Building new index for empty collection: PlaylistTrack\n",
            "2025-08-20 21:23:57,634 [INFO] [02.2] Converting 8715 rows to structured text\n",
            "2025-08-20 21:23:57,686 [INFO] [02.3] Creating vector store for table: PlaylistTrack\n",
            "2025-08-20 21:23:57,687 [INFO] [02.4] Building vector index with 8715 nodes\n",
            "2025-08-20 21:24:43,671 [INFO] [02.5] Index created successfully for table: PlaylistTrack\n",
            "2025-08-20 21:24:43,674 [INFO] [04] Successfully indexed table: PlaylistTrack\n",
            "2025-08-20 21:24:43,675 [INFO] [01] Processing table: Track\n",
            "2025-08-20 21:24:43,713 [INFO] [02] Building new index for empty collection: Track\n",
            "2025-08-20 21:24:43,742 [INFO] [02.2] Converting 3503 rows to structured text\n",
            "2025-08-20 21:24:43,768 [INFO] [02.3] Creating vector store for table: Track\n",
            "2025-08-20 21:24:43,769 [INFO] [02.4] Building vector index with 3503 nodes\n",
            "2025-08-20 21:25:42,501 [INFO] [02.5] Index created successfully for table: Track\n",
            "2025-08-20 21:25:42,503 [INFO] [04] Successfully indexed table: Track\n",
            "2025-08-20 21:25:42,503 [INFO] [05] Successfully indexed 11 tables\n"
          ]
        }
      ],
      "source": [
        "def index_all_tables_with_chroma(sql_database, chroma_db_dir: str = CHROMA_DB_DIR) -> Dict[str, VectorStoreIndex]:\n",
        "    \"\"\"Index all tables in the SQL database using ChromaDB as the backend.\n",
        "    Args:\n",
        "        sql_database: SQLDatabase instance\n",
        "        chroma_db_dir: Directory for ChromaDB persistence\n",
        "        \n",
        "    Returns:\n",
        "        Dict mapping table names to VectorStoreIndex instances\n",
        "    \"\"\"\n",
        "    os.makedirs(chroma_db_dir, exist_ok=True)\n",
        "\n",
        "    vector_index_dict = {}\n",
        "    engine = sql_database.engine\n",
        "\n",
        "    logger.info(f\" [00] Creating persistent Chroma client at: {chroma_db_dir}\")\n",
        "    chroma_client = chromadb.PersistentClient(path=chroma_db_dir)\n",
        "\n",
        "    for table_name in sql_database.get_usable_table_names():\n",
        "        logger.info(f\"[01] Processing table: {table_name}\")\n",
        "        \n",
        "        try:\n",
        "            # Create or get collection - ChromaDB handles persistence internally\n",
        "            collection = chroma_client.get_or_create_collection(name=f\"table_{table_name}\")\n",
        "            \n",
        "            # Check if collection already has data\n",
        "            if collection.count() == 0:\n",
        "                logger.info(f\"[02] Building new index for empty collection: {table_name}\")\n",
        "                \n",
        "                # Fetch data from database\n",
        "                with engine.connect() as conn:\n",
        "                    result = conn.execute(text(f'SELECT * FROM \"{table_name}\"'))\n",
        "                    col_names = list(result.keys())\n",
        "                    rows = result.fetchall()\n",
        "                \n",
        "                if not rows:\n",
        "                    logger.warning(f\"[02.1] Table {table_name} is empty, skipping...\")\n",
        "                    continue\n",
        "                \n",
        "                logger.info(f\"[02.2] Converting {len(rows)} rows to structured text\")\n",
        "                row_texts = [\n",
        "                    \" | \".join([f\"{col}={val}\" for col, val in zip(col_names, row)])\n",
        "                    for row in rows\n",
        "                ]\n",
        "                \n",
        "                # Create TextNodes with proper IDs\n",
        "                nodes = [\n",
        "                    TextNode(\n",
        "                        text=row_text, \n",
        "                        id_=f\"{table_name}_row_{idx}\"\n",
        "                    ) \n",
        "                    for idx, row_text in enumerate(row_texts)\n",
        "                ]\n",
        "                \n",
        "                logger.info(f\"[02.3] Creating vector store for table: {table_name}\")\n",
        "                vector_store = ChromaVectorStore(chroma_collection=collection)\n",
        "                \n",
        "                # Create index - this will automatically add nodes to ChromaDB\n",
        "                logger.info(f\"[02.4] Building vector index with {len(nodes)} nodes\")\n",
        "                storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "                index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "                \n",
        "                logger.info(f\"[02.5] Index created successfully for table: {table_name}\")\n",
        "                \n",
        "            else:\n",
        "                logger.info(f\"[03] Reusing existing collection with {collection.count()} items: {table_name}\")\n",
        "                \n",
        "                # Create vector store from existing collection\n",
        "                vector_store = ChromaVectorStore(chroma_collection=collection)\n",
        "                storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "                \n",
        "                # Create index from existing vector store\n",
        "                index = VectorStoreIndex.from_vector_store(\n",
        "                    vector_store=vector_store,\n",
        "                    storage_context=storage_context\n",
        "                )\n",
        "            \n",
        "            vector_index_dict[table_name] = index\n",
        "            logger.info(f\"[04] Successfully indexed table: {table_name}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"[ERROR] Failed to index table {table_name}: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    logger.info(f\"[05] Successfully indexed {len(vector_index_dict)} tables\")\n",
        "    return vector_index_dict\n",
        "\n",
        "# Build vector indexes for all tables using ChromaDB\n",
        "vector_index_dict = index_all_tables_with_chroma(sql_database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "31906c11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:25:42,521 [INFO] [01] Getting schema for table: Album\n",
            "2025-08-20 21:25:42,523 [INFO] [02] Retrieving example rows for table: Album\n",
            "2025-08-20 21:25:42,693 [ERROR] [ERROR] Failed to get context for table Album: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk\n",
            "2025-08-20 21:25:42,695 [INFO] [01] Getting schema for table: Artist\n",
            "2025-08-20 21:25:42,697 [INFO] [02] Retrieving example rows for table: Artist\n",
            "2025-08-20 21:25:42,773 [ERROR] [ERROR] Failed to get context for table Artist: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk\n",
            "2025-08-20 21:25:42,775 [INFO] [01] Getting schema for table: Customer\n",
            "2025-08-20 21:25:42,778 [INFO] [02] Retrieving example rows for table: Customer\n",
            "2025-08-20 21:25:42,907 [INFO] [03] Retrieved 2 relevant nodes for table: Customer\n",
            "2025-08-20 21:25:42,908 [INFO] [01] Getting schema for table: Employee\n",
            "2025-08-20 21:25:42,910 [INFO] [02] Retrieving example rows for table: Employee\n",
            "2025-08-20 21:25:42,986 [ERROR] [ERROR] Failed to get context for table Employee: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk\n",
            "2025-08-20 21:25:42,988 [INFO] [01] Getting schema for table: Genre\n",
            "2025-08-20 21:25:42,991 [INFO] [02] Retrieving example rows for table: Genre\n",
            "2025-08-20 21:25:43,110 [ERROR] [ERROR] Failed to get context for table Genre: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk\n",
            "2025-08-20 21:25:43,112 [INFO] [01] Getting schema for table: Invoice\n",
            "2025-08-20 21:25:43,113 [INFO] [02] Retrieving example rows for table: Invoice\n",
            "2025-08-20 21:25:43,177 [INFO] [03] Retrieved 2 relevant nodes for table: Invoice\n",
            "2025-08-20 21:25:43,179 [INFO] [01] Getting schema for table: InvoiceLine\n",
            "2025-08-20 21:25:43,180 [INFO] [02] Retrieving example rows for table: InvoiceLine\n",
            "2025-08-20 21:25:43,258 [INFO] [03] Retrieved 2 relevant nodes for table: InvoiceLine\n",
            "2025-08-20 21:25:43,259 [INFO] [01] Getting schema for table: MediaType\n",
            "2025-08-20 21:25:43,260 [INFO] [02] Retrieving example rows for table: MediaType\n",
            "2025-08-20 21:25:43,385 [INFO] [03] Retrieved 2 relevant nodes for table: MediaType\n",
            "2025-08-20 21:25:43,387 [INFO] [01] Getting schema for table: Playlist\n",
            "2025-08-20 21:25:43,388 [INFO] [02] Retrieving example rows for table: Playlist\n",
            "2025-08-20 21:25:43,458 [INFO] [03] Retrieved 2 relevant nodes for table: Playlist\n",
            "2025-08-20 21:25:43,459 [INFO] [01] Getting schema for table: PlaylistTrack\n",
            "2025-08-20 21:25:43,460 [INFO] [02] Retrieving example rows for table: PlaylistTrack\n",
            "2025-08-20 21:25:43,535 [INFO] [03] Retrieved 2 relevant nodes for table: PlaylistTrack\n",
            "2025-08-20 21:25:43,537 [INFO] [01] Getting schema for table: Track\n",
            "2025-08-20 21:25:43,538 [INFO] [02] Retrieving example rows for table: Track\n",
            "2025-08-20 21:25:43,664 [INFO] [03] Retrieved 2 relevant nodes for table: Track\n",
            "2025-08-20 21:25:43,668 [INFO] Updated table context with rows:\n",
            "Table 'Album' has columns: AlbumId (INTEGER), Title (NVARCHAR(160)), ArtistId (INTEGER),  and foreign keys: ['ArtistId'] -> Artist.['ArtistId'].\n",
            "\n",
            "Table 'Artist' has columns: ArtistId (INTEGER), Name (NVARCHAR(120)), .\n",
            "\n",
            "Table 'Customer' has columns: CustomerId (INTEGER), FirstName (NVARCHAR(40)), LastName (NVARCHAR(20)), Company (NVARCHAR(80)), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)), SupportRepId (INTEGER),  and foreign keys: ['SupportRepId'] -> Employee.['EmployeeId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- CustomerId=2 | FirstName=Leonie | LastName=Köhler | Company=None | Address=Theodor-Heuss-Straße 34 | City=Stuttgart | State=None | Country=Germany | PostalCode=70174 | Phone=+49 0711 2842222 | Fax=None | Email=leonekohler@surfeu.de | SupportRepId=5\n",
            "- CustomerId=6 | FirstName=Helena | LastName=Holý | Company=None | Address=Rilská 3174/6 | City=Prague | State=None | Country=Czech Republic | PostalCode=14300 | Phone=+420 2 4177 0449 | Fax=None | Email=hholy@gmail.com | SupportRepId=5\n",
            "\n",
            "\n",
            "Table 'Employee' has columns: EmployeeId (INTEGER), LastName (NVARCHAR(20)), FirstName (NVARCHAR(20)), Title (NVARCHAR(30)), ReportsTo (INTEGER), BirthDate (DATETIME), HireDate (DATETIME), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)),  and foreign keys: ['ReportsTo'] -> Employee.['EmployeeId'].\n",
            "\n",
            "Table 'Genre' has columns: GenreId (INTEGER), Name (NVARCHAR(120)), .\n",
            "\n",
            "Table 'Invoice' has columns: InvoiceId (INTEGER), CustomerId (INTEGER), InvoiceDate (DATETIME), BillingAddress (NVARCHAR(70)), BillingCity (NVARCHAR(40)), BillingState (NVARCHAR(40)), BillingCountry (NVARCHAR(40)), BillingPostalCode (NVARCHAR(10)), Total (NUMERIC(10, 2)),  and foreign keys: ['CustomerId'] -> Customer.['CustomerId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- InvoiceId=83 | CustomerId=42 | InvoiceDate=2009-12-26 00:00:00 | BillingAddress=9, Place Louis Barthou | BillingCity=Bordeaux | BillingState=None | BillingCountry=France | BillingPostalCode=33000 | Total=0.99\n",
            "- InvoiceId=270 | CustomerId=42 | InvoiceDate=2012-03-29 00:00:00 | BillingAddress=9, Place Louis Barthou | BillingCity=Bordeaux | BillingState=None | BillingCountry=France | BillingPostalCode=33000 | Total=8.91\n",
            "\n",
            "\n",
            "Table 'InvoiceLine' has columns: InvoiceLineId (INTEGER), InvoiceId (INTEGER), TrackId (INTEGER), UnitPrice (NUMERIC(10, 2)), Quantity (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['InvoiceId'] -> Invoice.['InvoiceId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- InvoiceLineId=1724 | InvoiceId=319 | TrackId=3482 | UnitPrice=0.99 | Quantity=1\n",
            "- InvoiceLineId=1817 | InvoiceId=334 | TrackId=575 | UnitPrice=0.99 | Quantity=1\n",
            "\n",
            "\n",
            "Table 'MediaType' has columns: MediaTypeId (INTEGER), Name (NVARCHAR(120)), .\n",
            "Here are some relevant example rows (column=value):\n",
            "- MediaTypeId=4 | Name=Purchased AAC audio file\n",
            "- MediaTypeId=5 | Name=AAC audio file\n",
            "\n",
            "\n",
            "Table 'Playlist' has columns: PlaylistId (INTEGER), Name (NVARCHAR(120)), .\n",
            "Here are some relevant example rows (column=value):\n",
            "- PlaylistId=11 | Name=Brazilian Music\n",
            "- PlaylistId=15 | Name=Classical 101 - The Basics\n",
            "\n",
            "\n",
            "Table 'PlaylistTrack' has columns: PlaylistId (INTEGER), TrackId (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['PlaylistId'] -> Playlist.['PlaylistId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- PlaylistId=5 | TrackId=1821\n",
            "- PlaylistId=8 | TrackId=1732\n",
            "\n",
            "\n",
            "Table 'Track' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2)),  and foreign keys: ['MediaTypeId'] -> MediaType.['MediaTypeId'], ['GenreId'] -> Genre.['GenreId'], ['AlbumId'] -> Album.['AlbumId'].\n",
            "Here are some relevant example rows (column=value):\n",
            "- TrackId=3503 | Name=Koyaanisqatsi | AlbumId=347 | MediaTypeId=2 | GenreId=10 | Composer=Philip Glass | Milliseconds=206005 | Bytes=3305164 | UnitPrice=0.99\n",
            "- TrackId=1762 | Name=Panis Et Circenses | AlbumId=145 | MediaTypeId=1 | GenreId=7 | Composer=Caetano Veloso e Gilberto Gil | Milliseconds=192339 | Bytes=6318373 | UnitPrice=0.99\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_table_context_and_rows_str(query_str: str, table_schema_objs: List[TableInfo]) -> str:\n",
        "    \"\"\"Get table context string for relevant example rows.\n",
        "    Args:\n",
        "        query_str: Query string for similarity search\n",
        "        table_schema_objs: List of TableInfo objects\n",
        "        \n",
        "    Returns:\n",
        "        Combined context string for all tables\n",
        "    \"\"\"\n",
        "    context_strs = []\n",
        "    \n",
        "    for table_info_obj in table_schema_objs:\n",
        "        table_name = table_info_obj.table_name\n",
        "        \n",
        "        try:\n",
        "            logger.info(f\"[01] Getting schema for table: {table_name}\")\n",
        "            table_info = sql_database.get_single_table_info(table_name)\n",
        "            \n",
        "            # Check if we have a vector index for this table\n",
        "            if table_name not in vector_index_dict:\n",
        "                logger.warning(f\"[02] No vector index found for table: {table_name}\")\n",
        "                context_strs.append(table_info)\n",
        "                continue\n",
        "            \n",
        "            logger.info(f\"[02] Retrieving example rows for table: {table_name}\")\n",
        "            vector_retriever = vector_index_dict[table_name].as_retriever(\n",
        "                similarity_top_k=TOP_N\n",
        "            )\n",
        "            \n",
        "            relevant_nodes = vector_retriever.retrieve(query_str)\n",
        "            logger.info(f\"[03] Retrieved {len(relevant_nodes)} relevant nodes for table: {table_name}\")\n",
        "            \n",
        "            if relevant_nodes:\n",
        "                table_row_context = \"\\nHere are some relevant example rows (column=value):\\n\"\n",
        "                for node in relevant_nodes:\n",
        "                    table_row_context += f\"- {node.get_content()}\\n\"\n",
        "                table_info += table_row_context\n",
        "            else:\n",
        "                logger.info(f\"[03.1] No relevant rows found for query in table: {table_name}\")\n",
        "            \n",
        "            context_strs.append(table_info)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"[ERROR] Failed to get context for table {table_name}: {str(e)}\")\n",
        "            # Still add basic table info even if retrieval fails\n",
        "            try:\n",
        "                table_info = sql_database.get_single_table_info(table_name)\n",
        "                context_strs.append(table_info)\n",
        "            except Exception as schema_error:\n",
        "                logger.error(f\"[ERROR] Failed to get schema for table {table_name}: {str(schema_error)}\")\n",
        "    \n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "table_parser_component = get_table_context_and_rows_str(QUERY_1, table_schema_objs)\n",
        "logger.info(f\"Updated table context with rows:\\n{table_parser_component}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab71cf00",
      "metadata": {},
      "source": [
        "### Define Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "22323164",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.t2SQL_workflow.custom_events import (\n",
        "    TableRetrievedEvent,\n",
        "    SchemaProcessedEvent,\n",
        "    SQLPromptReadyEvent,\n",
        "    SQLGeneratedEvent,\n",
        "    SQLParsedEvent,\n",
        "    SQLResultsEvent,\n",
        "    ResponsePromptReadyEvent,\n",
        ")\n",
        "from utils.t2SQL_workflow.custom_fallbacks import (\n",
        "    extract_sql_from_response,\n",
        "    analyze_sql_error,\n",
        "    create_t2s_prompt,\n",
        ")\n",
        "\n",
        "\n",
        "class Text2SQLWorkflowRowRetrieval(Workflow):\n",
        "    @step\n",
        "    async def input_step(self, ev: StartEvent) -> TableRetrievedEvent:\n",
        "        logger.info(f\"[Step 01] Process initial query and retrieve relevant tables\")\n",
        "        query = ev.query\n",
        "\n",
        "        logger.info(f\" - Use object retriever built from your table summaries\")\n",
        "        tables = obj_retriever.retrieve(query)  # candidate schemas\n",
        "        logger.info(f\" - Retrieved {len(tables)} candidate tables for query: {query}\")\n",
        "        \n",
        "        return TableRetrievedEvent(\n",
        "            tables=tables, \n",
        "            query_str=query\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def table_output_parser_step(self, ev: TableRetrievedEvent) -> SchemaProcessedEvent:\n",
        "        logger.info(f\"[Step 02] Parsing schemas and retrieving relevant rows for query: {ev.query_str}\")\n",
        "\n",
        "        logger.info(f\" - Enriching context function with vector row retrieval for tables: {ev.tables}\")\n",
        "        schema_str = get_table_context_and_rows_str(ev.query_str, ev.tables)\n",
        "        \n",
        "        return SchemaProcessedEvent(\n",
        "            table_schema=schema_str, \n",
        "            query_str=ev.query_str\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_prompt_step(self, ev: SchemaProcessedEvent | SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        logger.info(f\"[Step 03] Creating SQL prompt for query: {ev.query_str}\")\n",
        "        if isinstance(ev, SchemaProcessedEvent):\n",
        "            table_schema = ev.table_schema\n",
        "            query_str = ev.query_str\n",
        "            retry_count = 0\n",
        "            error_message = \"\"\n",
        "        else:\n",
        "            table_schema = getattr(ev, 'table_schema', '')\n",
        "            query_str = ev.query_str\n",
        "            retry_count = getattr(ev, 'retry_count', 0) + 1\n",
        "            error_message = getattr(ev, 'error_message', '')\n",
        "\n",
        "        prompt = create_t2s_prompt(table_schema, query_str, retry_count, error_message)\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=prompt,\n",
        "            query_str=query_str,\n",
        "            table_schema=table_schema,\n",
        "            retry_count=retry_count,\n",
        "            error_message=error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_llm_step(self, ev: SQLPromptReadyEvent) -> SQLGeneratedEvent:\n",
        "        logger.info(f\"[Step 04] Running LLM to generate SQL for query: {ev.query_str}\")\n",
        "        sql_response = await Settings.llm.acomplete(ev.t2s_prompt)\n",
        "        \n",
        "        return SQLGeneratedEvent(\n",
        "            sql_query=str(sql_response).strip(),\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_output_parser_step(self, ev: SQLGeneratedEvent) -> SQLParsedEvent:\n",
        "        logger.info(f\"[Step 05] Parsing LLM response to extract clean SQL for query: {ev.query_str}\")\n",
        "        try:\n",
        "            clean_sql = parse_response_to_sql(ev.sql_query)  # primary parser\n",
        "        except Exception:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query, logger)  # fallback\n",
        "        \n",
        "        if not clean_sql:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query, logger)\n",
        "\n",
        "        logger.info(f\"Attempt #{ev.retry_count + 1}\")\n",
        "        logger.info(f\"LLM Response: {ev.sql_query}\")\n",
        "        logger.info(f\"Cleaned SQL: {clean_sql}\")\n",
        "\n",
        "        return SQLParsedEvent(\n",
        "            sql_query=clean_sql,\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_retriever_step(self, ev: SQLParsedEvent) -> SQLResultsEvent:\n",
        "        logger.info(f\"[Step 06] Executing SQL for query: {ev.query_str}\")\n",
        "        try:\n",
        "            results = sql_retriever.retrieve(ev.sql_query)\n",
        "            logger.info(f\"[SUCCESS] Executed on Attempt #{ev.retry_count + 1}\")\n",
        "\n",
        "            return SQLResultsEvent(\n",
        "                context_str=str(results),\n",
        "                sql_query=ev.sql_query,\n",
        "                query_str=ev.query_str,\n",
        "                success=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            logger.error(f\"Execution failed (Attempt #{ev.retry_count + 1}): {error_msg}\")\n",
        "\n",
        "            if ev.retry_count < MAX_RETRIES:\n",
        "                retry_event = SQLResultsEvent(\n",
        "                    context_str=\"\",\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False,\n",
        "                    retry_count=ev.retry_count + 1,\n",
        "                )\n",
        "                retry_event.retry_count = ev.retry_count + 1\n",
        "                retry_event.error_message = analyze_sql_error(error_msg, ev.sql_query, ev.table_schema, logger)\n",
        "                retry_event.table_schema = ev.table_schema\n",
        "                \n",
        "                return retry_event\n",
        "            else:\n",
        "                return SQLResultsEvent(\n",
        "                    context_str=(f\"Failed after {MAX_RETRIES+1} attempts. Final error: {error_msg}\"),\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False,\n",
        "                    retry_count=ev.retry_count + 1,\n",
        "                )\n",
        "\n",
        "    @step\n",
        "    async def retry_handler_step(self, ev: SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        logger.info(f\"[Step 07] Handling retry for query: {ev.query_str}\")\n",
        "        if ev.success:\n",
        "            return None\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=\"\",  # regenerated later\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=getattr(ev, 'table_schema', ''),\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=getattr(ev, 'error_message', 'Unknown error')\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_prompt_step(self, ev: SQLResultsEvent) -> ResponsePromptReadyEvent:\n",
        "        logger.info(f\"[Step 08] Preparing synthesis prompt for query: {ev.query_str}\")\n",
        "        if not ev.success:\n",
        "            return None\n",
        "        prompt = response_synthesis_prompt.format(\n",
        "            query_str=ev.query_str,\n",
        "            context_str=ev.context_str,\n",
        "            sql_query=ev.sql_query\n",
        "        )\n",
        "        \n",
        "        return ResponsePromptReadyEvent(\n",
        "            query_str=ev.query_str,\n",
        "            rs_prompt=prompt\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_llm_step(self, ev: ResponsePromptReadyEvent) -> StopEvent:\n",
        "        logger.info(f\"[Step 09] Generating final answer for query: {ev.query_str}\")\n",
        "        answer = await Settings.llm.acomplete(ev.rs_prompt)\n",
        "        \n",
        "        return StopEvent(result=str(answer))\n",
        "\n",
        "\n",
        "# Runner\n",
        "async def run_text2sql_workflow_row(query: str):\n",
        "    workflow = Text2SQLWorkflowRowRetrieval(timeout=480)\n",
        "    result = await workflow.run(query=query)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e3daf393",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 21:43:14,194 [INFO] [Step 01] Process initial query and retrieve relevant tables\n",
            "2025-08-20 21:43:14,195 [INFO]  - Use object retriever built from your table summaries\n",
            "2025-08-20 21:43:14,240 [INFO]  - Retrieved 5 candidate tables for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:43:14,246 [INFO] [Step 02] Parsing schemas and retrieving relevant rows for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:43:14,247 [INFO]  - Enriching context function with vector row retrieval for tables: [SQLTableSchema(table_name='Invoice', context_str='Summary of invoice data'), SQLTableSchema(table_name='Customer', context_str='Summary of customer data'), SQLTableSchema(table_name='Employee', context_str='Summary of employee information'), SQLTableSchema(table_name='Artist', context_str='Summary of artist information'), SQLTableSchema(table_name='InvoiceLine', context_str='Summary of invoice data entries')]\n",
            "2025-08-20 21:43:14,248 [INFO] [01] Getting schema for table: Invoice\n",
            "2025-08-20 21:43:14,249 [INFO] [02] Retrieving example rows for table: Invoice\n",
            "2025-08-20 21:43:14,432 [INFO] [03] Retrieved 2 relevant nodes for table: Invoice\n",
            "2025-08-20 21:43:14,434 [INFO] [01] Getting schema for table: Customer\n",
            "2025-08-20 21:43:14,435 [INFO] [02] Retrieving example rows for table: Customer\n",
            "2025-08-20 21:43:14,466 [INFO] [03] Retrieved 2 relevant nodes for table: Customer\n",
            "2025-08-20 21:43:14,468 [INFO] [01] Getting schema for table: Employee\n",
            "2025-08-20 21:43:14,469 [INFO] [02] Retrieving example rows for table: Employee\n",
            "2025-08-20 21:43:14,507 [ERROR] [ERROR] Failed to get context for table Employee: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk\n",
            "2025-08-20 21:43:14,508 [INFO] [01] Getting schema for table: Artist\n",
            "2025-08-20 21:43:14,509 [INFO] [02] Retrieving example rows for table: Artist\n",
            "2025-08-20 21:43:14,555 [ERROR] [ERROR] Failed to get context for table Artist: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk\n",
            "2025-08-20 21:43:14,557 [INFO] [01] Getting schema for table: InvoiceLine\n",
            "2025-08-20 21:43:14,558 [INFO] [02] Retrieving example rows for table: InvoiceLine\n",
            "2025-08-20 21:43:14,633 [INFO] [03] Retrieved 2 relevant nodes for table: InvoiceLine\n",
            "2025-08-20 21:43:14,643 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:43:14,649 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:44:04,287 [INFO] [Step 05] Parsing LLM response to extract clean SQL for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:44:04,292 [INFO] Extracting SQL from LLM response that might contain reasoning or formatting.\n",
            "2025-08-20 21:44:04,292 [INFO] Removing <think> blocks from response\n",
            "2025-08-20 21:44:04,294 [INFO] Removing non-SQL content at the beginning of response\n",
            "2025-08-20 21:44:04,295 [INFO]  [Method 1] Looking for SQLQuery: pattern\n",
            "2025-08-20 21:44:04,297 [INFO]  [Method 2] Looking for SQL in code blocks\n",
            "2025-08-20 21:44:04,299 [INFO]  [Method 3] Looking for standalone SQL statements\n",
            "2025-08-20 21:44:04,300 [INFO]  - Split by lines and look for SQL statements\n",
            "2025-08-20 21:44:04,302 [INFO]  - Checking line for SQL keywords: SELECT Invoice.BillingCity FROM Invoice JOIN Customer ON Invoice.CustomerId = Customer.CustomerId WHERE Customer.FirstName = 'Leonie' AND Customer.LastName = 'Köhler';\n",
            "2025-08-20 21:44:04,305 [INFO] Attempt #1\n",
            "2025-08-20 21:44:04,307 [INFO] LLM Response: <think>\n",
            "Okay, let's see. The user is asking for the billing city of Leonie Köhler. First, I need to figure out which table contains the information about her. Looking at the provided tables, the Customer table has CustomerId, FirstName, LastName, etc. So Leonie Köhler's information should be in the Customer table.\n",
            "\n",
            "The Customer table has a column called FirstName and LastName, which matches Leonie's name. So I need to select the LastName and then find out the BillingCity from the Invoice table? Wait, no. Wait, the BillingCity is part of the Invoice table's BillingAddress. Wait, the Invoice table has BillingAddress, but the question is about the billing city, which is BillingCity. But how do I get that from the Invoice table?\n",
            "\n",
            "Wait, the BillingCity is a column in the Invoice table. But the user is asking for the billing city of Leonie, which is in the Customer table. So maybe I need to join the Customer and Invoice tables on CustomerId? Because the Invoice has a CustomerId, which should link to the Customer table. So the BillingCity would be in the Invoice's BillingAddress. But how to get that from the Invoice table?\n",
            "\n",
            "Wait, the BillingAddress is in the Invoice table. So if I can find the InvoiceId where CustomerId is 2 (from the example), then I can get the BillingAddress, which would be the BillingCity. But the example shows that for CustomerId 2, the BillingAddress is \"Theodor-Heuss-Straße 34\" and BillingCity is \"Stuttgart\". So the BillingCity is in the Invoice's BillingAddress. Therefore, the correct approach is to join the Customer and Invoice tables on CustomerId, then select the BillingCity from the Invoice.\n",
            "\n",
            "So the SQL query would be:\n",
            "\n",
            "SELECT Invoice.BillingCity\n",
            "FROM Invoice\n",
            "JOIN Customer ON Invoice.CustomerId = Customer.CustomerId\n",
            "WHERE Customer.FirstName = 'Leonie' AND Customer.LastName = 'Köhler';\n",
            "\n",
            "But wait, the example shows that for CustomerId 2, the BillingAddress is \"Theodor-Heuss-Straße 34\" and BillingCity is \"Stuttgart\". So the BillingCity is in the Invoice's BillingAddress. Therefore, the query should select the BillingCity from the Invoice table where the Customer's FirstName and LastName match.\n",
            "\n",
            "But the user's question is about the billing city, so the answer would be the BillingCity column from the Invoice table linked to the Customer. So the SQL query would be as above.\n",
            "\n",
            "But the user's example shows that for CustomerId 2, the BillingCity is \"Stuttgart\". So the query should return that. Therefore, the correct SQL query is as above.\n",
            "</think>\n",
            "\n",
            "SELECT Invoice.BillingCity FROM Invoice JOIN Customer ON Invoice.CustomerId = Customer.CustomerId WHERE Customer.FirstName = 'Leonie' AND Customer.LastName = 'Köhler';\n",
            "2025-08-20 21:44:04,309 [INFO] Cleaned SQL: SELECT Invoice.BillingCity FROM Invoice JOIN Customer ON Invoice.CustomerId = Customer.CustomerId WHERE Customer.FirstName = 'Leonie' AND Customer.LastName = 'Köhler'\n",
            "2025-08-20 21:44:04,319 [INFO] [Step 06] Executing SQL for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:44:04,397 [INFO] [SUCCESS] Executed on Attempt #1\n",
            "2025-08-20 21:44:04,412 [INFO] [Step 03] Creating SQL prompt for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:44:04,429 [INFO] [Step 08] Preparing synthesis prompt for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:44:04,445 [INFO] [Step 07] Handling retry for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:44:04,455 [INFO] [Step 04] Running LLM to generate SQL for query: What is the billing city of Leonie Köhler?\n",
            "2025-08-20 21:44:04,480 [INFO] [Step 09] Generating final answer for query: What is the billing city of Leonie Köhler?\n",
            "<think>\n",
            "Okay, let's see. The user is asking about the billing city of Leonie Köhler based on the SQL query provided. The SQL query joins the Invoice table with the Customer table and filters where the Customer's first and last name are 'Leonie' and 'Köhler' respectively. The SQL Response shows that the result has multiple entries, all from 'Stuttgart'.\n",
            "\n",
            "First, I need to parse the SQL query to understand what's being executed. The query selects BillingCity from Invoice where Customer's first and last names match. The result has 7 entries, all in 'Stuttgart'. So the billing city is consistently 'Stuttgart' across all records.\n",
            "\n",
            "Now, the user wants the response synthesized from the query results. The response provided is a list of tuples, each with the BillingCity. So the answer should be that the billing city is 'Stuttgart' for all the records. I should present this clearly, maybe in a sentence or list, ensuring that all the information is accurate and concise.\n",
            "</think>\n",
            "\n",
            "The billing city of Leonie Köhler is **Stuttgart**. All records in the query result indicate that the billing city is consistently 'Stuttgart'.\n"
          ]
        }
      ],
      "source": [
        "result = await run_text2sql_workflow_row(QUERY_1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2fdc7f5",
      "metadata": {},
      "source": [
        "Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11dc8cab",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def visualize_text2sql_workflow_row(sample_query: str, execution_name: str, output_dir: str = WORKFLOW_VISUALIZATION_DIR):\n",
        "    \"\"\"\n",
        "    Function to visualize the Text2SQL workflow in your version:\n",
        "    - Draws all possible flows\n",
        "    - Runs your row-retrieval Text2SQL workflow\n",
        "    - Draws execution path of the actual run\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    logger.info(\"[01] Drawing all possible flows...\")\n",
        "    all_flows_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_flow.html\")\n",
        "    draw_all_possible_flows(\n",
        "        Text2SQLWorkflowRowRetrieval,\n",
        "        filename=all_flows_path\n",
        "    )\n",
        "    logger.info(f\"[SUCCESS] All possible flows saved to: {all_flows_path}\")\n",
        "\n",
        "    logger.info(\"[02] Running workflow and drawing execution path...\")\n",
        "    try:\n",
        "        logger.info(\" - wrapper function instead of manual instantiation\")\n",
        "        result = await run_text2sql_workflow_row(sample_query)\n",
        "\n",
        "        logger.info(\" - Recreating workflow object for execution path drawing\")\n",
        "        workflow = Text2SQLWorkflowRowRetrieval(timeout=240)\n",
        "\n",
        "        execution_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_execution.html\")\n",
        "        draw_most_recent_execution(\n",
        "            workflow,\n",
        "            filename=execution_path\n",
        "        )\n",
        "        logger.info(f\"[SUCCESS] Recent execution path saved to: {execution_path}\")\n",
        "        logger.info(f\"Workflow result: {result.result}\")  \n",
        "        logger.debug(\"this `.result` holds final answer\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during workflow execution: {e}\")\n",
        "        logger.info(\"Note: Ensure retrievers + LLM configs are initialized correctly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963acccf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# await visualize_text2sql_workflow_row(QUERY_1, QUERY_1_INITIAL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-text-2-sql",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
