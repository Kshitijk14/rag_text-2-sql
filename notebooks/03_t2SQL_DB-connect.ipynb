{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "855f9f50-ef38-4069-932a-fb49af02d28e",
      "metadata": {
        "id": "855f9f50-ef38-4069-932a-fb49af02d28e"
      },
      "source": [
        "# Query Pipeline for Advanced Text-to-SQL\n",
        "\n",
        "In this guide we show you how to setup a text-to-SQL pipeline over your data with our [query pipeline](https://docs.llamaindex.ai/en/stable/module_guides/querying/pipeline/root.html) syntax.\n",
        "\n",
        "This gives you flexibility to enhance text-to-SQL with additional techniques. We show these in the below sections:\n",
        "1. **Query-Time Table Retrieval**: Dynamically retrieve relevant tables in the text-to-SQL prompt.\n",
        "2. **Query-Time Sample Row retrieval**: Embed/Index each row, and dynamically retrieve example rows for each table in the text-to-SQL prompt.\n",
        "\n",
        "Our out-of-the box pipelines include our `NLSQLTableQueryEngine` and `SQLTableRetrieverQueryEngine`. (if you want to check out our text-to-SQL guide using these modules, take a look [here](https://docs.llamaindex.ai/en/stable/examples/index_structs/struct_indices/SQLIndexDemo.html)). This guide implements an advanced version of those modules, giving you the utmost flexibility to apply this to your own setting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39cc96c-9fbc-44c3-b57f-017a9aa75473",
      "metadata": {
        "id": "e39cc96c-9fbc-44c3-b57f-017a9aa75473"
      },
      "source": [
        "## Load and Ingest Data\n",
        "\n",
        "\n",
        "### Load Data\n",
        "We use the [WikiTableQuestions dataset](https://ppasupat.github.io/WikiTableQuestions/) (Pasupat and Liang 2015) as our test dataset.\n",
        "\n",
        "We go through all the csv's in one folder, store each in a sqlite database (we will then build an object index over each table schema)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc8cd21",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import io, os, time, re, requests, zipfile, json\n",
        "import json as pyjson\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# put data into sqlite db\n",
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    text,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        ")\n",
        "\n",
        "# setup Arize Phoenix for logging/observability\n",
        "import phoenix as px\n",
        "\n",
        "from llama_index.core import (\n",
        "    Settings, \n",
        "    SQLDatabase, \n",
        "    VectorStoreIndex, \n",
        "    load_index_from_storage,\n",
        "    set_global_handler,\n",
        ")\n",
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.objects import (\n",
        "    SQLTableNodeMapping,\n",
        "    ObjectIndex,\n",
        "    SQLTableSchema,\n",
        ")\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.retrievers import SQLRetriever\n",
        "from llama_index.core.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.llms import ChatResponse\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core.storage import StorageContext\n",
        "from llama_index.core.workflow import (\n",
        "    Workflow, \n",
        "    step, \n",
        "    StartEvent, \n",
        "    StopEvent,\n",
        ")\n",
        "from llama_index.core.workflow.events import Event\n",
        "from llama_index.utils.workflow import (\n",
        "    draw_all_possible_flows, \n",
        "    draw_most_recent_execution,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b2221fdd",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"../data/WikiTableQuestions/csv/200-csv\")\n",
        "CSV_FILES = sorted([f for f in DATA_DIR.glob(\"*.csv\")])\n",
        "\n",
        "TABLEINFO_DIR = \"../data/WikiTableQuestions_TableInfo\"\n",
        "os.makedirs(TABLEINFO_DIR, exist_ok=True)\n",
        "\n",
        "MAX_RETRIES = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5a710f7a-74b4-48c3-98d1-a6d409a7af1a",
      "metadata": {
        "id": "5a710f7a-74b4-48c3-98d1-a6d409a7af1a",
        "outputId": "fb840754-8360-459d-fe4b-bbe92b24475e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\0.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\1.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\10.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\11.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\12.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\14.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\15.csv\n",
            "Error parsing ..\\data\\WikiTableQuestions\\csv\\200-csv\\15.csv: Error tokenizing data. C error: Expected 4 fields in line 16, saw 5\n",
            "\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\17.csv\n",
            "Error parsing ..\\data\\WikiTableQuestions\\csv\\200-csv\\17.csv: Error tokenizing data. C error: Expected 6 fields in line 5, saw 7\n",
            "\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\18.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\20.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\22.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\24.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\25.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\26.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\28.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\29.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\3.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\30.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\31.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\32.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\33.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\34.csv\n",
            "Error parsing ..\\data\\WikiTableQuestions\\csv\\200-csv\\34.csv: Error tokenizing data. C error: Expected 4 fields in line 6, saw 13\n",
            "\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\35.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\36.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\37.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\38.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\4.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\41.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\42.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\44.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\45.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\46.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\47.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\48.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\7.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\8.csv\n",
            "processing file: ..\\data\\WikiTableQuestions\\csv\\200-csv\\9.csv\n"
          ]
        }
      ],
      "source": [
        "dfs = []\n",
        "\n",
        "for csv_file in CSV_FILES:\n",
        "    print(f\"processing file: {csv_file}\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_file)\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing {csv_file}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c271b986-9ed6-4a8a-a7c2-1dad7a642c06",
      "metadata": {
        "id": "c271b986-9ed6-4a8a-a7c2-1dad7a642c06"
      },
      "source": [
        "### Extract Table Name and Summary from each Table\n",
        "\n",
        "Here we use gpt-3.5 to extract a table name (with underscores) and summary from each table with our Pydantic program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4cef9585-87f8-4427-aaca-c0038c289e00",
      "metadata": {
        "id": "4cef9585-87f8-4427-aaca-c0038c289e00",
        "outputId": "03b3f4e4-93cf-4125-b745-a5f190208bbf"
      },
      "outputs": [],
      "source": [
        "class TableInfo(BaseModel):\n",
        "    \"\"\"Information regarding a structured table.\"\"\"\n",
        "\n",
        "    table_name: str = Field(\n",
        "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
        "    )\n",
        "    table_summary: str = Field(\n",
        "        ..., description=\"short, concise summary/caption of the table\"\n",
        "    )\n",
        "\n",
        "PROMPT_STR = \"\"\"\\\n",
        "    Return only a JSON object, with no explanation, no prose, no markdown, and no trailing text.\n",
        "    You are to produce **only** a JSON object matching the following exact schema:\n",
        "\n",
        "    {\n",
        "        \"table_name\": \"<short_name_in_snake_case_without_spaces>\",\n",
        "        \"table_summary\": \"<short concise caption of the table>\"\n",
        "    }\n",
        "\n",
        "    Example:\n",
        "    {\"table_name\": \"movie_info\", \"table_summary\": \"Summary of movie data\"}\n",
        "\n",
        "    Rules:\n",
        "    - The table_name must be unique to the table, describe it clearly, and be in snake_case.\n",
        "    - Do NOT output a generic table name (e.g., \"table\", \"my_table\").\n",
        "    - Do NOT make the table name one of the following: {exclude_table_name_list}.\n",
        "    - Do NOT include any keys other than \"table_name\" and \"table_summary\".\n",
        "    - Do NOT include extra text before/after the JSON.\n",
        "    - Do NOT include any other keys or text before/after the JSON.\n",
        "    - Do NOT wrap in ```json.\n",
        "\n",
        "    Table:\n",
        "    {table_str}\n",
        "\"\"\"\n",
        "\n",
        "Settings.llm = Ollama(\n",
        "    model=\"qwen3:0.6b\", \n",
        "    request_timeout=240,\n",
        "    format=\"json\",\n",
        ")\n",
        "\n",
        "program = LLMTextCompletionProgram.from_defaults(\n",
        "    output_cls=TableInfo,\n",
        "    prompt_template_str=PROMPT_STR,\n",
        "    llm=Settings.llm,\n",
        ")\n",
        "\n",
        "\n",
        "def extract_first_json_block(text: str):\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.S)  # grab first {...} block\n",
        "    if not match:\n",
        "        raise ValueError(\"No JSON object found in output\")\n",
        "    return pyjson.loads(match.group())\n",
        "\n",
        "\n",
        "def _get_tableinfo_with_index(idx: int) -> str:\n",
        "    results_gen = Path(TABLEINFO_DIR).glob(f\"{idx}_*\")\n",
        "    results_list = list(results_gen)\n",
        "    \n",
        "    if len(results_list) == 0:\n",
        "        return None\n",
        "    elif len(results_list) == 1:\n",
        "        path = results_list[0]\n",
        "        json_str = path.read_text(encoding=\"utf-8\")\n",
        "        return TableInfo.model_validate_json(json_str)\n",
        "    else:\n",
        "        raise ValueError(f\"More than one file matching index: {list(results_gen)}\")\n",
        "\n",
        "\n",
        "table_names = set()\n",
        "table_infos = []\n",
        "\n",
        "for idx, df in enumerate(dfs):\n",
        "    table_info = _get_tableinfo_with_index(idx)\n",
        "    if table_info:\n",
        "        table_infos.append(table_info)\n",
        "        continue\n",
        "\n",
        "    df_str = df.head(10).to_csv()\n",
        "\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            raw_output = program(\n",
        "                table_str=df_str,\n",
        "                exclude_table_name_list=str(list(table_names)),\n",
        "            )\n",
        "\n",
        "            if isinstance(raw_output, TableInfo):\n",
        "                table_info = raw_output\n",
        "            elif isinstance(raw_output, dict):\n",
        "                table_info = TableInfo(**raw_output)\n",
        "            elif isinstance(raw_output, str):\n",
        "                parsed_dict = extract_first_json_block(raw_output)\n",
        "                table_info = TableInfo(**parsed_dict)\n",
        "            else:\n",
        "                raise TypeError(f\"Unexpected return type from program(): {type(raw_output)}\")\n",
        "\n",
        "            table_name = table_info.table_name\n",
        "            print(f\"Processed table: {table_name}\")\n",
        "\n",
        "            if table_name in table_names:\n",
        "                print(f\"Table name '{table_name}' already exists, skipping this table.\")\n",
        "                table_info = None  # don’t append duplicate\n",
        "                break  # skip\n",
        "\n",
        "            # save table info\n",
        "            table_names.add(table_name)\n",
        "            out_file = f\"{TABLEINFO_DIR}/{idx}_{table_name}.json\"\n",
        "            json.dump(table_info.model_dump(), open(out_file, \"w\"))\n",
        "            break  # move to next table\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with attempt {attempt+1}: {e}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    if table_info:\n",
        "        table_infos.append(table_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1cc3230-d14c-4491-b79f-45159708badb",
      "metadata": {
        "id": "a1cc3230-d14c-4491-b79f-45159708badb"
      },
      "source": [
        "### Put Data in SQL Database\n",
        "\n",
        "We use `sqlalchemy`, a popular SQL database toolkit, to load all the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53544059-de7d-48dd-8e00-89517964852b",
      "metadata": {
        "id": "53544059-de7d-48dd-8e00-89517964852b",
        "outputId": "df224651-1765-4aa7-e841-d58546c20e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating table: movie_chart_positions\n",
            "Creating table: movie_data\n",
            "Creating table: death_accident_statistics\n",
            "Creating table: award_data_1972\n",
            "Creating table: award_data\n",
            "Creating table: people_info\n",
            "Creating table: broadcasting_info\n",
            "Creating table: person_info\n",
            "Creating table: chart_positions\n",
            "Creating table: kodachrome_film_info\n",
            "Creating table: bbc_radio_costs\n",
            "Creating table: airport_locations\n",
            "Creating table: party_voters\n",
            "Creating table: club_performance\n",
            "Creating table: horse_race_data\n",
            "Creating table: grammy_awards\n",
            "Creating table: boxing_matches\n",
            "Creating table: sports_performance_data\n",
            "Creating table: district_info\n",
            "Creating table: party_data\n",
            "Creating table: award_nominations\n",
            "Creating table: government_ministers\n",
            "Creating table: new_municipality_old_municipality_seat\n",
            "Creating table: team_performance\n",
            "Creating table: encoding_info\n",
            "Creating table: temperature_data\n",
            "Creating table: people_terms\n",
            "Creating table: new_mexico_governorships\n",
            "Creating table: weather_statistics\n",
            "Creating table: drop_event_data\n",
            "Creating table: precipitation_data\n",
            "Creating table: afrikaans_language_usage\n",
            "Creating table:  ohio_districts\n",
            "Creating table: gene_functions\n"
          ]
        }
      ],
      "source": [
        "# Function to create a sanitized column name\n",
        "def sanitize_column_name(col_name):\n",
        "    # Remove special characters and replace spaces with underscores\n",
        "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
        "\n",
        "\n",
        "# Function to create a table from a DataFrame using SQLAlchemy\n",
        "def create_table_from_dataframe(\n",
        "    df: pd.DataFrame, table_name: str, engine, metadata_obj\n",
        "):\n",
        "    # Sanitize column names\n",
        "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
        "    df = df.rename(columns=sanitized_columns)\n",
        "\n",
        "    # Dynamically create columns based on DataFrame columns and data types\n",
        "    columns = [\n",
        "        Column(col, String if dtype == \"object\" else Integer)\n",
        "        for col, dtype in zip(df.columns, df.dtypes)\n",
        "    ]\n",
        "\n",
        "    # Create a table with the defined columns\n",
        "    table = Table(table_name, metadata_obj, *columns)\n",
        "\n",
        "    # Create the table in the database\n",
        "    metadata_obj.create_all(engine)\n",
        "\n",
        "    # Insert data from DataFrame into the table\n",
        "    with engine.connect() as conn:\n",
        "        for _, row in df.iterrows():\n",
        "            insert_stmt = table.insert().values(**row.to_dict())\n",
        "            conn.execute(insert_stmt)\n",
        "        conn.commit()\n",
        "\n",
        "\n",
        "# engine = create_engine(\"sqlite:///:memory:\")\n",
        "engine = create_engine(\"sqlite:///../db/sqlite/db.sqlite3\")\n",
        "metadata_obj = MetaData()\n",
        "for idx, df in enumerate(dfs):\n",
        "    tableinfo = _get_tableinfo_with_index(idx)\n",
        "    if tableinfo is None:\n",
        "        print(f\"[ERROR] No TableInfo for index {idx}\")\n",
        "        continue  # skip this one or handle it differently\n",
        "    print(f\"Creating table: {tableinfo.table_name}\")\n",
        "    create_table_from_dataframe(df, tableinfo.table_name, engine, metadata_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
      "metadata": {
        "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
        "outputId": "4527a7d1-6b5f-4f89-9a63-e5263f4e4441"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
            "  next(self.gen)\n",
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:144: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
            "📖 For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n"
          ]
        }
      ],
      "source": [
        "px.launch_app()\n",
        "set_global_handler(\"arize_phoenix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5906e7-4e20-4b0a-bc42-615f6b86beb7",
      "metadata": {
        "id": "1b5906e7-4e20-4b0a-bc42-615f6b86beb7"
      },
      "source": [
        "### Define Modules\n",
        "\n",
        "Here we define the core modules.\n",
        "1. Object index + retriever to store table schemas\n",
        "2. SQLDatabase object to connect to the above tables + SQLRetriever.\n",
        "3. Text-to-SQL Prompt\n",
        "4. Response synthesis Prompt\n",
        "5. LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019",
      "metadata": {
        "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
            "\n",
            "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
            "\n",
            "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
            "\n",
            "Question: Question here\n",
            "SQLQuery: SQL Query to run\n",
            "SQLResult: Result of the SQLQuery\n",
            "Answer: Final answer here\n",
            "\n",
            "Only use tables listed below.\n",
            "{schema}\n",
            "\n",
            "Question: {query_str}\n",
            "SQLQuery: \n"
          ]
        }
      ],
      "source": [
        "# Object index, retriever, SQLDatabase\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "sql_database = SQLDatabase(engine)\n",
        "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
        "\n",
        "table_schema_objs = [\n",
        "    SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
        "    for t in table_infos\n",
        "]  # add a SQLTableSchema for each table\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    table_schema_objs,\n",
        "    table_node_mapping,\n",
        "    VectorStoreIndex,\n",
        "    embed_model=Settings.embed_model,\n",
        ")\n",
        "obj_retriever = obj_index.as_retriever(similarity_top_k=5)\n",
        "\n",
        "\n",
        "\n",
        "# SQLRetriever + Table Parser\n",
        "sql_retriever = SQLRetriever(sql_database)\n",
        "\n",
        "\n",
        "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
        "    \"\"\"Get table context string.\"\"\"\n",
        "    context_strs = []\n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        table_info = sql_database.get_single_table_info(\n",
        "            table_schema_obj.table_name\n",
        "        )\n",
        "        if table_schema_obj.context_str:\n",
        "            table_opt_context = \" The table description is: \"\n",
        "            table_opt_context += table_schema_obj.context_str\n",
        "            table_info += table_opt_context\n",
        "\n",
        "        context_strs.append(table_info)\n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "\n",
        "table_parser_component = get_table_context_str(table_schema_objs)\n",
        "\n",
        "\n",
        "\n",
        "# Text-to-SQL Prompt + Output Parser\n",
        "def parse_response_to_sql(response: ChatResponse) -> str:\n",
        "    \"\"\"Parse response to SQL.\"\"\"\n",
        "    response = response.message.content\n",
        "    sql_query_start = response.find(\"SQLQuery:\")\n",
        "    \n",
        "    if sql_query_start != -1:\n",
        "        response = response[sql_query_start:]\n",
        "        \n",
        "        if response.startswith(\"SQLQuery:\"):\n",
        "            response = response[len(\"SQLQuery:\") :]\n",
        "    \n",
        "    sql_result_start = response.find(\"SQLResult:\")\n",
        "    \n",
        "    if sql_result_start != -1:\n",
        "        response = response[:sql_result_start]\n",
        "    \n",
        "    return response.strip().strip(\"```\").strip()\n",
        "\n",
        "\n",
        "sql_parser_component = FunctionTool.from_defaults(fn=parse_response_to_sql)\n",
        "\n",
        "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
        "    dialect=engine.dialect.name\n",
        ")\n",
        "print(text2sql_prompt.template)\n",
        "\n",
        "\n",
        "\n",
        "# Response Synthesis Prompt\n",
        "response_synthesis_prompt_str = (\n",
        "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"SQL: {sql_query}\\n\"\n",
        "    \"SQL Response: {context_str}\\n\"\n",
        "    \"Response: \"\n",
        ")\n",
        "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fda34ab",
      "metadata": {},
      "source": [
        "## Advanced Capability 2: Text-to-SQL with Query-Time Row Retrieval (along with Table Retrieval)\n",
        "\n",
        "One problem in the previous example is that if the user asks a query that asks for \"The Notorious BIG\" but the artist is stored as \"The Notorious B.I.G\", then the generated SELECT statement will likely not return any matches.\n",
        "\n",
        "We can alleviate this problem by fetching a small number of example rows per table. A naive option would be to just take the first k rows. Instead, we embed, index, and retrieve k relevant rows given the user query to give the text-to-SQL LLM the most contextually relevant information for SQL generation.\n",
        "\n",
        "We now extend our query pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d99d3f4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Configure Settings\n",
        "callback_manager = CallbackManager()\n",
        "Settings.callback_manager = callback_manager"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15203320",
      "metadata": {},
      "source": [
        "### Index Each Table\n",
        "\n",
        "We embed/index the rows of each table, resulting in one index per table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "732a46e6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indexing rows in table:  ohio_districts\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\ ohio_districts\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\ ohio_districts\\index_store.json.\n",
            "Indexing rows in table: afrikaans_language_usage\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\afrikaans_language_usage\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\afrikaans_language_usage\\index_store.json.\n",
            "Indexing rows in table: airport_locations\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\airport_locations\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\airport_locations\\index_store.json.\n",
            "Indexing rows in table: award_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\award_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\award_data\\index_store.json.\n",
            "Indexing rows in table: award_data_1972\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\award_data_1972\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\award_data_1972\\index_store.json.\n",
            "Indexing rows in table: award_nominations\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\award_nominations\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\award_nominations\\index_store.json.\n",
            "Indexing rows in table: bbc_radio_costs\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\bbc_radio_costs\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\bbc_radio_costs\\index_store.json.\n",
            "Indexing rows in table: boxing_matches\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\boxing_matches\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\boxing_matches\\index_store.json.\n",
            "Indexing rows in table: broadcasting_info\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\broadcasting_info\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\broadcasting_info\\index_store.json.\n",
            "Indexing rows in table: chart_positions\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\chart_positions\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\chart_positions\\index_store.json.\n",
            "Indexing rows in table: club_performance\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\club_performance\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\club_performance\\index_store.json.\n",
            "Indexing rows in table: death_accident_statistics\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\death_accident_statistics\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\death_accident_statistics\\index_store.json.\n",
            "Indexing rows in table: district_info\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\district_info\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\district_info\\index_store.json.\n",
            "Indexing rows in table: drop_event_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\drop_event_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\drop_event_data\\index_store.json.\n",
            "Indexing rows in table: encoding_info\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\encoding_info\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\encoding_info\\index_store.json.\n",
            "Indexing rows in table: gene_functions\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\gene_functions\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\gene_functions\\index_store.json.\n",
            "Indexing rows in table: government_ministers\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\government_ministers\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\government_ministers\\index_store.json.\n",
            "Indexing rows in table: grammy_awards\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\grammy_awards\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\grammy_awards\\index_store.json.\n",
            "Indexing rows in table: horse_race_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\horse_race_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\horse_race_data\\index_store.json.\n",
            "Indexing rows in table: kodachrome_film_info\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\kodachrome_film_info\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\kodachrome_film_info\\index_store.json.\n",
            "Indexing rows in table: movie_chart_positions\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\movie_chart_positions\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\movie_chart_positions\\index_store.json.\n",
            "Indexing rows in table: movie_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\movie_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\movie_data\\index_store.json.\n",
            "Indexing rows in table: new_mexico_governorships\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\new_mexico_governorships\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\new_mexico_governorships\\index_store.json.\n",
            "Indexing rows in table: new_municipality_old_municipality_seat\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\new_municipality_old_municipality_seat\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\new_municipality_old_municipality_seat\\index_store.json.\n",
            "Indexing rows in table: party_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\party_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\party_data\\index_store.json.\n",
            "Indexing rows in table: party_voters\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\party_voters\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\party_voters\\index_store.json.\n",
            "Indexing rows in table: people_info\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\people_info\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\people_info\\index_store.json.\n",
            "Indexing rows in table: people_terms\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\people_terms\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\people_terms\\index_store.json.\n",
            "Indexing rows in table: person_info\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\person_info\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\person_info\\index_store.json.\n",
            "Indexing rows in table: precipitation_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\precipitation_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\precipitation_data\\index_store.json.\n",
            "Indexing rows in table: sports_performance_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\sports_performance_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\sports_performance_data\\index_store.json.\n",
            "Indexing rows in table: team_performance\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\team_performance\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\team_performance\\index_store.json.\n",
            "Indexing rows in table: temperature_data\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\temperature_data\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\temperature_data\\index_store.json.\n",
            "Indexing rows in table: weather_statistics\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\weather_statistics\\docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ..\\data\\table_index_dir\\weather_statistics\\index_store.json.\n"
          ]
        }
      ],
      "source": [
        "def index_all_tables(sql_database, table_index_dir: str = \"../data/table_index_dir\") -> Dict[str, VectorStoreIndex]:\n",
        "    \"\"\"Index all tables in the SQL database.\"\"\"\n",
        "    Path(table_index_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    vector_index_dict = {}\n",
        "    engine = sql_database.engine\n",
        "\n",
        "    for table_name in sql_database.get_usable_table_names():\n",
        "        print(f\"Indexing rows in table: {table_name}\")\n",
        "        table_path = Path(table_index_dir) / table_name\n",
        "\n",
        "        if not table_path.exists():\n",
        "            # Fetch all rows from the table\n",
        "            with engine.connect() as conn:\n",
        "                result = conn.execute(text(f'SELECT * FROM \"{table_name}\"'))\n",
        "                row_tuples = [tuple(row) for row in result.fetchall()]\n",
        "\n",
        "            # Create TextNode objects from rows\n",
        "            nodes = [TextNode(text=str(row)) for row in row_tuples]\n",
        "\n",
        "            # Build the index using current global Settings\n",
        "            index = VectorStoreIndex(nodes)\n",
        "\n",
        "            # Save index\n",
        "            index.set_index_id(\"vector_index\")\n",
        "            index.storage_context.persist(persist_dir=str(table_path))\n",
        "\n",
        "        else:\n",
        "            # Rebuild storage context from saved directory\n",
        "            storage_context = StorageContext.from_defaults(\n",
        "                persist_dir=str(table_path)\n",
        "            )\n",
        "\n",
        "            # Load existing index\n",
        "            index = load_index_from_storage(\n",
        "                storage_context, index_id=\"vector_index\"\n",
        "            )\n",
        "\n",
        "        vector_index_dict[table_name] = index\n",
        "\n",
        "    return vector_index_dict\n",
        "\n",
        "vector_index_dict = index_all_tables(sql_database)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bccd815",
      "metadata": {},
      "source": [
        "### Define Expanded Table Parser Component\n",
        "\n",
        "We expand the capability of our `table_parser_component` to not only return the relevant table schemas, but also return relevant rows per table schema.\n",
        "\n",
        "It now takes in both `table_schema_objs` (output of table retriever), but also the original `query_str` which will then be used for vector retrieval of relevant rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "31906c11",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_table_context_and_rows_str(query_str: str, table_schema_objs: List[SQLTableSchema]):\n",
        "    \"\"\"Get table context string.\"\"\"\n",
        "    context_strs = []\n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        # first append table info + additional context\n",
        "        table_info = sql_database.get_single_table_info(\n",
        "            table_schema_obj.table_name\n",
        "        )\n",
        "        if table_schema_obj.context_str:\n",
        "            table_opt_context = \" The table description is: \"\n",
        "            table_opt_context += table_schema_obj.context_str\n",
        "            table_info += table_opt_context\n",
        "\n",
        "        # also lookup vector index to return relevant table rows\n",
        "        vector_retriever = vector_index_dict[\n",
        "            table_schema_obj.table_name\n",
        "        ].as_retriever(similarity_top_k=2)\n",
        "        relevant_nodes = vector_retriever.retrieve(query_str)\n",
        "        if len(relevant_nodes) > 0:\n",
        "            table_row_context = \"\\nHere are some relevant example rows (values in the same order as columns above)\\n\"\n",
        "            for node in relevant_nodes:\n",
        "                table_row_context += str(node.get_content()) + \"\\n\"\n",
        "            table_info += table_row_context\n",
        "\n",
        "        context_strs.append(table_info)\n",
        "    return \"\\n\\n\".join(context_strs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bfd87a74",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_str = \"What was the year that The Notorious B.I.G was signed to Bad Boy?\"\n",
        "table_parser_component = get_table_context_and_rows_str(query_str, table_schema_objs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab71cf00",
      "metadata": {},
      "source": [
        "### Define Expanded Query Pipeline\n",
        "\n",
        "This looks similar to the query pipeline in section 1, but with an upgraded table_parser_component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc386be",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# custom events\n",
        "class TableRetrievedEvent(Event):\n",
        "    tables: list\n",
        "    query_str: str\n",
        "\n",
        "class SchemaProcessedEvent(Event):\n",
        "    table_schema: str\n",
        "    query_str: str\n",
        "\n",
        "class SQLPromptReadyEvent(Event):\n",
        "    t2s_prompt: str\n",
        "    query_str: str\n",
        "    table_schema: str\n",
        "    retry_count: int = 0\n",
        "    error_message: str = \"\"\n",
        "\n",
        "class SQLGeneratedEvent(Event):\n",
        "    sql_query: str\n",
        "    query_str: str\n",
        "    table_schema: str\n",
        "    retry_count: int = 0\n",
        "    error_message: str = \"\"\n",
        "\n",
        "class SQLParsedEvent(Event):\n",
        "    sql_query: str\n",
        "    query_str: str\n",
        "    table_schema: str\n",
        "    retry_count: int = 0\n",
        "    error_message: str = \"\"\n",
        "\n",
        "class SQLResultsEvent(Event):\n",
        "    context_str: str\n",
        "    sql_query: str\n",
        "    query_str: str\n",
        "    success: bool = True\n",
        "\n",
        "class ResponsePromptReadyEvent(Event):\n",
        "    rs_prompt: str\n",
        "\n",
        "\n",
        "# helpers\n",
        "def _is_valid_sql_start(text: str) -> bool:\n",
        "    \"\"\"Check if text starts with valid SQL\"\"\"\n",
        "    if not text:\n",
        "        return False\n",
        "    \n",
        "    sql_keywords = ['SELECT', 'WITH', 'INSERT', 'UPDATE', 'DELETE']\n",
        "    text_upper = text.upper().strip()\n",
        "    return any(text_upper.startswith(keyword) for keyword in sql_keywords)\n",
        "\n",
        "def _clean_sql_query(sql: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean and standardize SQL query.\n",
        "    \"\"\"\n",
        "    if not sql:\n",
        "        return \"SELECT 1\"\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    sql = ' '.join(sql.split())\n",
        "    \n",
        "    # Fix quote issues - convert double quotes to single quotes for string literals\n",
        "    # This is a simple approach - for more complex cases, you'd need a proper SQL parser\n",
        "    sql = re.sub(r'\"([^\"]*)\"', r\"'\\1'\", sql)\n",
        "    \n",
        "    # Remove multiple semicolons\n",
        "    sql = re.sub(r';+', ';', sql)\n",
        "    \n",
        "    # Remove trailing semicolon and add it back cleanly\n",
        "    sql = sql.rstrip(';').strip()\n",
        "    \n",
        "    # Don't add semicolon for now since it might be causing issues\n",
        "    return sql\n",
        "\n",
        "\n",
        "# custom fallbacks\n",
        "def extract_sql_from_response(llm_response: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract SQL query from LLM response that might contain reasoning or formatting.\n",
        "    \"\"\"\n",
        "    response = llm_response.strip()\n",
        "    \n",
        "    # First, remove <think> blocks entirely\n",
        "    response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
        "    \n",
        "    # Remove any non-SQL content at the beginning\n",
        "    response = re.sub(r'^[^S]*(?=SELECT|WITH|INSERT|UPDATE|DELETE)', '', response, flags=re.IGNORECASE)\n",
        "    \n",
        "    # Method 1: Look for SQLQuery: pattern\n",
        "    sql_query_match = re.search(r'SQLQuery:\\s*([^;]+;?)', response, re.IGNORECASE | re.DOTALL)\n",
        "    if sql_query_match:\n",
        "        sql = sql_query_match.group(1).strip()\n",
        "        return _clean_sql_query(sql)\n",
        "    \n",
        "    # Method 2: Look for SQL in code blocks\n",
        "    code_block_patterns = [\n",
        "        r'```sql\\s*\\n(.*?)\\n```',\n",
        "        r'```\\s*\\n(.*?)\\n```',\n",
        "        r'`([^`]+)`'\n",
        "    ]\n",
        "    \n",
        "    for pattern in code_block_patterns:\n",
        "        match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            sql = match.group(1).strip()\n",
        "            if _is_valid_sql_start(sql):\n",
        "                return _clean_sql_query(sql)\n",
        "    \n",
        "    # Method 3: Look for standalone SQL statements\n",
        "    sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'WITH']\n",
        "    \n",
        "    # Split by lines and look for SQL statements\n",
        "    lines = response.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "            \n",
        "        # Check if line starts with SQL keyword\n",
        "        if any(line.upper().startswith(keyword.upper()) for keyword in sql_keywords):\n",
        "            return _clean_sql_query(line)\n",
        "    \n",
        "    # Method 4: Look for multi-line SQL statements\n",
        "    for keyword in sql_keywords:\n",
        "        pattern = rf'\\b{keyword}\\b.*?(?=\\n\\s*\\n|\\nSQLResult|\\nAnswer|$)'\n",
        "        sql_match = re.search(pattern, response, re.IGNORECASE | re.DOTALL)\n",
        "        if sql_match:\n",
        "            sql = sql_match.group(0).strip()\n",
        "            return _clean_sql_query(sql)\n",
        "    \n",
        "    # Fallback: if nothing found, return empty string to avoid errors\n",
        "    print(f\"Warning: Could not extract SQL from response: {response[:100]}...\")\n",
        "    return \"SELECT [-]\"  # Safe fallback query\n",
        "\n",
        "def analyze_sql_error(error_message: str, sql_query: str, table_schema: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze SQL error and provide suggestions for fixing the query.\n",
        "    \"\"\"\n",
        "    error_lower = error_message.lower()\n",
        "    \n",
        "    if \"no such column\" in error_lower:\n",
        "        # Extract the problematic column name\n",
        "        column_match = re.search(r'no such column:\\s*(\\w+)', error_lower)\n",
        "        if column_match:\n",
        "            bad_column = column_match.group(1)\n",
        "            \n",
        "            # Try to suggest correct column names from schema\n",
        "            schema_lower = table_schema.lower()\n",
        "            possible_columns = re.findall(r'(\\w+):', schema_lower)\n",
        "            \n",
        "            suggestions = []\n",
        "            for col in possible_columns:\n",
        "                if bad_column.lower() in col.lower() or col.lower() in bad_column.lower():\n",
        "                    suggestions.append(col)\n",
        "            \n",
        "            error_msg = f\"Column '{bad_column}' does not exist.\"\n",
        "            if suggestions:\n",
        "                error_msg += f\" Did you mean: {', '.join(suggestions[:3])}?\"\n",
        "            error_msg += f\"\\n\\nAvailable columns from schema:\\n{table_schema}\"\n",
        "            return error_msg\n",
        "    \n",
        "    elif \"no such table\" in error_lower:\n",
        "        table_match = re.search(r'no such table:\\s*([\\w\\s\\[\\]]+)', error_lower)\n",
        "        if table_match:\n",
        "            bad_table = table_match.group(1).strip()\n",
        "            return f\"Table '{bad_table}' does not exist. Available tables from schema:\\n{table_schema}\"\n",
        "    \n",
        "    elif \"syntax error\" in error_lower:\n",
        "        return f\"SQL syntax error. Please check:\\n- Missing quotes around strings\\n- Proper parentheses\\n- Correct SQL keywords\\n\\nFailed query: {sql_query}\"\n",
        "    \n",
        "    return f\"SQL execution error: {error_message}\\n\\nFailed query: {sql_query}\\n\\nSchema: {table_schema}\"\n",
        "\n",
        "def create_enhanced_prompt(table_schema: str, query_str: str, retry_count: int = 0, error_message: str = \"\"):\n",
        "    if retry_count == 0:\n",
        "        # Initial attempt\n",
        "        ENHANCED_PROMPT = f\"\"\"Given the table schema and user question below, generate ONLY a valid SQL query.\n",
        "\n",
        "            Table Schema:\n",
        "            {table_schema}\n",
        "\n",
        "            User Question: {query_str}\n",
        "\n",
        "            IMPORTANT RULES:\n",
        "            1. Return ONLY the SQL query, nothing else\n",
        "            2. Use single quotes for string literals, not double quotes\n",
        "            3. Do not include any explanations, reasoning, or additional text\n",
        "            4. Do not include labels like \"SQLQuery:\", \"Answer:\", etc.\n",
        "            5. Do not wrap in code blocks or markdown formatting\n",
        "            6. Do not include semicolons at the end\n",
        "            7. Do not include any <think> tags or reasoning\n",
        "            8. Only use column names that exist in the provided schema\n",
        "\n",
        "            Example format:\n",
        "            SELECT column_name FROM table_name WHERE condition\n",
        "\n",
        "            Your SQL query:\n",
        "        \"\"\"\n",
        "    else:\n",
        "        # Retry attempt with error information\n",
        "        ENHANCED_PROMPT = f\"\"\"The previous SQL query failed with an error. Please generate a corrected SQL query.\n",
        "\n",
        "            Table Schema:\n",
        "            {table_schema}\n",
        "\n",
        "            User Question: {query_str}\n",
        "\n",
        "            Previous Error: {error_message}\n",
        "\n",
        "            IMPORTANT RULES:\n",
        "            1. Return ONLY the corrected SQL query, nothing else\n",
        "            2. Use single quotes for string literals, not double quotes\n",
        "            3. Carefully check that all column names exist in the provided schema\n",
        "            4. Do not include any explanations, reasoning, or additional text\n",
        "            5. Do not include labels like \"SQLQuery:\", \"Answer:\", etc.\n",
        "            6. Do not wrap in code blocks or markdown formatting\n",
        "            7. Do not include semicolons at the end\n",
        "            8. Only use column names that are explicitly listed in the schema above\n",
        "\n",
        "            Your corrected SQL query:\n",
        "        \"\"\"\n",
        "    \n",
        "    return ENHANCED_PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22323164",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Text2SQLWorkflowRowRetrieval(Workflow):\n",
        "    @step\n",
        "    async def input_step(self, ev: StartEvent) -> TableRetrievedEvent:\n",
        "        \"\"\"Step 1: Process initial query and retrieve relevant tables\"\"\"\n",
        "        query = ev.query\n",
        "        tables = obj_retriever.retrieve(query)  # retrieve candidate schemas\n",
        "        \n",
        "        return TableRetrievedEvent(\n",
        "            tables=tables, \n",
        "            query_str=query\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def table_output_parser_step(self, ev: TableRetrievedEvent) -> SchemaProcessedEvent:\n",
        "        \"\"\"Step 2: Parse schemas + retrieve relevant rows\"\"\"\n",
        "        schema_str = get_table_context_and_rows_str(ev.query_str, ev.tables)\n",
        "        \n",
        "        return SchemaProcessedEvent(\n",
        "            table_schema=schema_str, \n",
        "            query_str=ev.query_str\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_prompt_step(self, ev: SchemaProcessedEvent | SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        \"\"\"Step 3: Create prompt (initial or retry)\"\"\"\n",
        "        if isinstance(ev, SchemaProcessedEvent):\n",
        "            table_schema = ev.table_schema\n",
        "            query_str = ev.query_str\n",
        "            retry_count = 0\n",
        "            error_message = \"\"\n",
        "        else:\n",
        "            table_schema = getattr(ev, 'table_schema', '')\n",
        "            query_str = ev.query_str\n",
        "            retry_count = getattr(ev, 'retry_count', 0) + 1\n",
        "            error_message = getattr(ev, 'error_message', '')\n",
        "\n",
        "        prompt = create_enhanced_prompt(table_schema, query_str, retry_count, error_message)\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=prompt,\n",
        "            query_str=query_str,\n",
        "            table_schema=table_schema,\n",
        "            retry_count=retry_count,\n",
        "            error_message=error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_llm_step(self, ev: SQLPromptReadyEvent) -> SQLGeneratedEvent:\n",
        "        \"\"\"Step 4: Run LLM to generate SQL\"\"\"\n",
        "        sql_response = await Settings.llm.acomplete(ev.t2s_prompt)\n",
        "        \n",
        "        return SQLGeneratedEvent(\n",
        "            sql_query=str(sql_response).strip(),\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_output_parser_step(self, ev: SQLGeneratedEvent) -> SQLParsedEvent:\n",
        "        \"\"\"Step 5: Parse/clean SQL\"\"\"\n",
        "        try:\n",
        "            clean_sql = parse_response_to_sql(ev.sql_query)  # primary parser\n",
        "        except Exception:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query)  # fallback\n",
        "        \n",
        "        if not clean_sql:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query)\n",
        "\n",
        "        print(f\"Attempt #{ev.retry_count + 1}\")\n",
        "        print(f\"LLM Response: {ev.sql_query}\")\n",
        "        print(f\"Cleaned SQL: {clean_sql}\")\n",
        "\n",
        "        return SQLParsedEvent(\n",
        "            sql_query=clean_sql,\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_retriever_step(self, ev: SQLParsedEvent) -> SQLResultsEvent:\n",
        "        \"\"\"Step 6: Execute SQL with retries\"\"\"\n",
        "        try:\n",
        "            results = sql_retriever.retrieve(ev.sql_query)\n",
        "            print(f\"[SUCCESS] Executed on attempt #{ev.retry_count + 1}\")\n",
        "            \n",
        "            return SQLResultsEvent(\n",
        "                context_str=str(results),\n",
        "                sql_query=ev.sql_query,\n",
        "                query_str=ev.query_str,\n",
        "                success=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"[ERROR] Execution failed (Attempt #{ev.retry_count + 1}): {error_msg}\")\n",
        "\n",
        "            if ev.retry_count < MAX_RETRIES:\n",
        "                retry_event = SQLResultsEvent(\n",
        "                    context_str=\"\",\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False\n",
        "                )\n",
        "                retry_event.retry_count = ev.retry_count + 1\n",
        "                retry_event.error_message = analyze_sql_error(error_msg, ev.sql_query, ev.table_schema)\n",
        "                retry_event.table_schema = ev.table_schema\n",
        "                \n",
        "                return retry_event\n",
        "            else:\n",
        "                return SQLResultsEvent(\n",
        "                    context_str=f\"Failed after {MAX_RETRIES+1} attempts. Final error: {error_msg}\",\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False\n",
        "                )\n",
        "\n",
        "    @step\n",
        "    async def retry_handler_step(self, ev: SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        \"\"\"Step 7: Retry failed SQL by regenerating prompt\"\"\"\n",
        "        if ev.success:\n",
        "            return None\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=\"\",  # regenerated later\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=getattr(ev, 'table_schema', ''),\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=getattr(ev, 'error_message', 'Unknown error')\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_prompt_step(self, ev: SQLResultsEvent) -> ResponsePromptReadyEvent:\n",
        "        \"\"\"Step 8: Prepare final synthesis prompt\"\"\"\n",
        "        if not ev.success:\n",
        "            return None\n",
        "        prompt = response_synthesis_prompt.format(\n",
        "            query_str=ev.query_str,\n",
        "            context_str=ev.context_str,\n",
        "            sql_query=ev.sql_query\n",
        "        )\n",
        "        \n",
        "        return ResponsePromptReadyEvent(rs_prompt=prompt)\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_llm_step(self, ev: ResponsePromptReadyEvent) -> StopEvent:\n",
        "        \"\"\"Step 9: Generate final human-readable answer\"\"\"\n",
        "        answer = await Settings.llm.acomplete(ev.rs_prompt)\n",
        "        \n",
        "        return StopEvent(result=str(answer))\n",
        "\n",
        "\n",
        "# Runner\n",
        "async def run_text2sql_workflow_row(query: str):\n",
        "    workflow = Text2SQLWorkflowRowRetrieval(timeout=480)\n",
        "    result = await workflow.run(query=query)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "702f159a",
      "metadata": {},
      "source": [
        "Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0129d573",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def visualize_text2sql_workflow(sample_query: str, execution_name: str):\n",
        "    \"\"\"\n",
        "    Function to visualize the Text2SQL workflow both as all possible flows\n",
        "    and a specific execution example\n",
        "    \"\"\"\n",
        "    output_dir = (\"../outputs/trials_v2/execution\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # 1. Draw ALL possible flows through your workflow\n",
        "    print(\"Drawing all possible flows...\")\n",
        "    all_flows_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_flow.html\")\n",
        "    draw_all_possible_flows(\n",
        "        Text2SQLWorkflowRowRetrieval, \n",
        "        filename=all_flows_path\n",
        "    )\n",
        "    print(f\"[SUCCESS] All possible flows saved to: {all_flows_path}\")\n",
        "\n",
        "    # 2. Run workflow + visualize the execution path\n",
        "    print(\"Running workflow and drawing execution path...\")\n",
        "    try:\n",
        "        workflow = Text2SQLWorkflowRowRetrieval(timeout=240)\n",
        "        result = await workflow.run(query=sample_query)\n",
        "\n",
        "        # Draw the execution path\n",
        "        execution_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_execution.html\")\n",
        "        draw_most_recent_execution(\n",
        "            workflow,\n",
        "            filename=execution_path\n",
        "        )\n",
        "        print(f\"[SUCCESS] Recent execution path saved to: {execution_path}\")\n",
        "        print(f\"Workflow result: {result}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Error during workflow execution: {e}\")\n",
        "        print(\"Note: You may need to set up your retriever and LLM settings first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb9637d",
      "metadata": {},
      "source": [
        "### Run Some Queries\n",
        "\n",
        "We can now ask about relevant entries even if it doesn't exactly match the entry in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ee0e5fcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = (\"What was the year that The Notorious B.I.G was signed to Bad Boy?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e3daf393",
      "metadata": {},
      "outputs": [],
      "source": [
        "# result = await run_text2sql_workflow_row(query)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "963acccf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drawing all possible flows...\n",
            "../outputs/trials_v2/execution\\BIG_text2sql_workflow_flow.html\n",
            "[SUCCESS] All possible flows saved to: ../outputs/trials_v2/execution\\BIG_text2sql_workflow_flow.html\n",
            "Running workflow and drawing execution path...\n",
            "Attempt #1\n",
            "LLM Response: <think>\n",
            "Okay, let's see. The user is asking for the year that The Notorious B.I.G was signed to Bad Boy. So I need to find the year in the people_info table where the artist is The Notorious B.I.G.\n",
            "\n",
            "First, I'll check the table schema. The people_info table has columns Act, Year_signed, and _Albums_released_under_Bad_Boy. The example rows show for The Notorious B.I.G, Year_signed is 1993, and _Albums_released_under_Bad_Boy is '5'. So the question is straightforward: look up the Year_signed column where the Act is 'The Notorious B.I.G'.\n",
            "\n",
            "I should use a SELECT statement with the WHERE clause to filter by the Act. The column names are given, so I can directly select. The SQL query would be SELECT Year_signed FROM people_info WHERE Act = 'The Notorious B.I.G'; but wait, the user's example shows the Year_signed as 1993, so the query would need to return that value. But the question is not asking for the year, but the year when they signed. Wait, the problem says to return only a valid SQL query. So maybe the answer is just that query.\n",
            "\n",
            "But wait, the example rows have the Year_signed as 1993, so the query is simply SELECT Year_signed FROM people_info WHERE Act = 'The Notorious B.I.G';. But the user's question is phrased as \"What was the year...\", which implies that the result is the value, not the query. But the instructions say to generate only the SQL query. So maybe the answer is that query. Let me check the example again. The example shows that the query would return 1993, so the correct SQL is that query.\n",
            "</think>\n",
            "\n",
            "SELECT Year_signed FROM people_info WHERE Act = 'The Notorious B.I.G';\n",
            "Cleaned SQL: SELECT Year_signed FROM people_info WHERE Act = 'The Notorious B.I.G'\n",
            "[SUCCESS] Executed on attempt #1\n",
            "../outputs/trials_v2/execution\\BIG_text2sql_workflow_execution.html\n",
            "[SUCCESS] Recent execution path saved to: ../outputs/trials_v2/execution\\BIG_text2sql_workflow_execution.html\n",
            "Workflow result: <think>\n",
            "Okay, let's see. The user provided a query about the year The Notorious B.I.G was signed to Bad Boy. They also gave a SQL query and a response.\n",
            "\n",
            "First, I need to parse the SQL query. The query is SELECT Year_signed FROM people_info WHERE Act = 'The Notorious B.I.G'. The response from the SQL shows that the result is [(1993,)], which means the year is 1993. \n",
            "\n",
            "Wait, but the user mentioned that the SQL response is a list of tuples. So each tuple represents a year. Since there are four entries, each is 1993. So the answer should be 1993.\n",
            "\n",
            "I should check if there's any other possible year mentioned, but the response seems straightforward. The metadata shows that the result is all 1993s. So the answer is 1993.\n",
            "</think>\n",
            "\n",
            "The Notorious B.I.G was signed to Bad Boy in the year **1993**.\n"
          ]
        }
      ],
      "source": [
        "await visualize_text2sql_workflow(query, \"BIG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6129239b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drawing all possible flows...\n",
            "../outputs/trials_v2/execution\\best_dir_1972_text2sql_workflow_flow.html\n",
            "[SUCCESS] All possible flows saved to: ../outputs/trials_v2/execution\\best_dir_1972_text2sql_workflow_flow.html\n",
            "Running workflow and drawing execution path...\n",
            "Attempt #1\n",
            "LLM Response: <think>\n",
            "Okay, let's see. The user is asking, \"Who won best director in the 1972 academy awards?\" So I need to find the answer based on the given tables.\n",
            "\n",
            "First, I need to figure out which tables are relevant. The question is about the 1972 Academy Awards and the Best Director. Looking at the tables provided, the 'award_data_1972' table has the awards in 1972. The columns there include Award, Category, Nominee, Result. The example given in that table has entries like 'Academy Awards, 1972' as the Award, Best Director as the category, and William Friedkin as the nominee.\n",
            "\n",
            "So the user is asking for the winner of the Best Director in 1972 Academy Awards. That would correspond to the Nominee column in the 'award_data_1972' table where the Award is 'Academy Awards, 1972' and the Category is 'Best Director'. The Result column in that table would have the result, which the user is asking for. But the question is not about the result, it's about who won. Wait, the example shows that in the 'award_data_1972' table, the Nominee is 'William Friedkin' for that entry. So the answer should be William Friedkin.\n",
            "\n",
            "But how to structure the SQL query to get that information. The user wants a valid SQL query. So I need to select the Nominee from the 'award_data_1972' table where the Award matches 'Academy Awards, 1972' and the Category is 'Best Director'. That should give the answer. Let me check the example again. The example rows in the 'award_data_1972' have the Nominee as the third column. So the SQL query would be selecting that column from that table where the Award and Category match the given values.\n",
            "\n",
            "Wait, but the user's question is phrased as \"Who won best director in the 1972 academy awards?\" So maybe the answer is the Nominee, which is in the 'award_data_1972' table. So the SQL query would be SELECT Nominee FROM award_data_1972 WHERE Award = 'Academy Awards, 1972' AND Category = 'Best Director'; but the example uses the Nominee column. So the correct query is to select that column from that table.\n",
            "\n",
            "I need to make sure that the tables are correctly referenced. The user's tables are:\n",
            "\n",
            "- award_nominations: includes Year, Award, Film, Result\n",
            "- award_data_1972: includes Award, Category, Nominee, Result\n",
            "- Grammy awards: includes Year, Award, Work_Artist, Result\n",
            "- award_data: includes Year, Award, Category, Nominated_work, Result\n",
            "- movie_data: includes Year, Title, Role, Notes\n",
            "\n",
            "The question is about the 1972 Academy Awards and Best Director. The 'award_data_1972' table has the relevant data, so the query should be selecting the Nominee from that table. The SQL query would be as simple as SELECT Nominee FROM award_data_1972 WHERE Award = 'Academy Awards, 1972' AND Category = 'Best Director'; but the example rows in the table show that the Nominee is the third column. So the query is correct.\n",
            "\n",
            "I need to use the columns that exist in the provided schema. The answer requires only the Nominee column. So the final SQL query is as above.\n",
            "</think>\n",
            "\n",
            "SELECT Nominee FROM award_data_1972 WHERE Award = 'Academy Awards, 1972' AND Category = 'Best Director';\n",
            "Cleaned SQL: SELECT Nominee FROM award_data_1972 WHERE Award = 'Academy Awards, 1972' AND Category = 'Best Director'\n",
            "[SUCCESS] Executed on attempt #1\n",
            "../outputs/trials_v2/execution\\best_dir_1972_text2sql_workflow_execution.html\n",
            "[SUCCESS] Recent execution path saved to: ../outputs/trials_v2/execution\\best_dir_1972_text2sql_workflow_execution.html\n",
            "Workflow result: <think>\n",
            "Okay, let's see. The user is asking about who won the Best Director in the 1972 Academy Awards. They provided a SQL query and a response that lists the nominee names. My task is to synthesize the response from the query results.\n",
            "\n",
            "First, I need to look at the SQL query. It selects the Nominee from the award_data_1972 table where the Award is 'Academy Awards, 1972' and the Category is 'Best Director'. The result from the SQL has a list of [('William Friedkin',), ...]. So the answer should be William Friedkin, repeated multiple times as the results.\n",
            "\n",
            "Wait, but the user's question is straightforward. The SQL query returns multiple entries, but the actual answer is the same. So the response should just state that William Friedkin won, maybe with a note that it's repeated. The SQL response shows the list, so the answer is William Friedkin. I need to make sure there are no other entries and that the answer is accurate based on the SQL.\n",
            "</think>\n",
            "\n",
            "The query results indicate that William Friedkin was nominated for the Best Director category in the 1972 Academy Awards. Since the SQL response lists multiple entries of \"William Friedkin,\" it confirms that he is the winner. \n",
            "\n",
            "**Answer:** William Friedkin won the Best Director award in the 1972 Academy Awards.\n"
          ]
        }
      ],
      "source": [
        "await visualize_text2sql_workflow(\"Who won best director in the 1972 academy awards?\", \"Best_Dir_1972\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
