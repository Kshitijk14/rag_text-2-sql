{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "855f9f50-ef38-4069-932a-fb49af02d28e",
      "metadata": {
        "id": "855f9f50-ef38-4069-932a-fb49af02d28e"
      },
      "source": [
        "1. **Query-Time Table Retrieval**: Dynamically retrieve relevant tables in the text-to-SQL prompt.\n",
        "2. **Query-Time Sample Row retrieval**: Embed/Index each row, and dynamically retrieve example rows for each table in the text-to-SQL prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "997717d9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Hp\\\\Documents\\\\GitHub\\\\rag_text-2-sql\\\\notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6ad056f1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Hp\\\\Documents\\\\GitHub\\\\rag_text-2-sql'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"../\")\n",
        "\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6bc8cd21",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from utils.helpers.other_imports import (\n",
        "    io,\n",
        "    time,\n",
        "    re,\n",
        "    requests,\n",
        "    zipfile,\n",
        "    shutil,\n",
        "    gc,\n",
        "    traceback,\n",
        "    json,\n",
        "    pyjson,\n",
        "    pd,\n",
        "    Path,\n",
        "    List,\n",
        "    Dict,\n",
        "    BaseModel,\n",
        "    Field,\n",
        "    px,\n",
        "    chromadb,\n",
        ")\n",
        "\n",
        "from utils.helpers.sql_alchemy_imports import (\n",
        "    create_engine,\n",
        "    text,\n",
        "    inspect,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        ")\n",
        "\n",
        "from utils.helpers.llama_index_imports import (\n",
        "    Settings, \n",
        "    SQLDatabase, \n",
        "    VectorStoreIndex, \n",
        "    ChromaVectorStore,\n",
        "    load_index_from_storage,\n",
        "    set_global_handler,\n",
        "    LLMTextCompletionProgram,\n",
        "    SQLTableNodeMapping,\n",
        "    ObjectIndex,\n",
        "    SQLTableSchema,\n",
        "    SQLRetriever,\n",
        "    DEFAULT_TEXT_TO_SQL_PROMPT,\n",
        "    PromptTemplate,\n",
        "    FunctionTool,\n",
        "    ChatResponse,\n",
        "    TextNode,\n",
        "    StorageContext,\n",
        "    Workflow,\n",
        "    step,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    draw_all_possible_flows,\n",
        "    draw_most_recent_execution,\n",
        ")\n",
        "\n",
        "from utils.config import CONFIG\n",
        "from utils.logger import setup_logger\n",
        "\n",
        "\n",
        "# configurations\n",
        "LOG_PATH = Path(CONFIG[\"LOG_PATH\"])\n",
        "\n",
        "CHINOOK_DBEAVER_DB_PATH = Path(CONFIG[\"CHINOOK_DBEAVER_DB_PATH\"])\n",
        "CHINOOK_TABLE_INDEX_DIR = Path(CONFIG[\"CHINOOK_TABLE_INDEX_DIR\"])\n",
        "SQLITE_DB_DIR = Path(CONFIG[\"SQLITE_DB_DIR\"])\n",
        "CHROMA_DB_DIR = Path(CONFIG[\"CHROMA_DB_DIR\"])\n",
        "\n",
        "WORKFLOW_VISUALIZATION_DIR = Path(CONFIG[\"WORKFLOW_VISUALIZATION_DIR\"])\n",
        "\n",
        "QUERY_1 = CONFIG[\"QUERY_1\"]\n",
        "QUERY_1_INITIAL = CONFIG[\"QUERY_1_INITIAL\"]\n",
        "QUERY_2 = CONFIG[\"QUERY_2\"]\n",
        "QUERY_2_INITIAL = CONFIG[\"QUERY_2_INITIAL\"]\n",
        "\n",
        "TOP_K = CONFIG[\"TOP_K\"]\n",
        "TOP_N = CONFIG[\"TOP_N\"]\n",
        "MAX_RETRIES = CONFIG[\"MAX_RETRIES\"]\n",
        "\n",
        "\n",
        "# setup logging\n",
        "LOG_DIR = os.path.join(os.getcwd(), LOG_PATH)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)  # Create the logs directory if it doesn't exist\n",
        "\n",
        "# comment out line 15 in utils/logger.py -> only for notebooks\n",
        "LOG_FILE = os.path.join(LOG_DIR, \"db_connect_notebook.log\")\n",
        "logger = setup_logger(\"db_connect_notebook_logger\", LOG_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6fe7acbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 17:10:06,376 [INFO] Creating SQLite DB Engine for the new summaries database: db\\Chinook\\sqlite\\table_summaries.db\n",
            "2025-08-20 17:10:06,379 [INFO]  - Ensuring the table exists (id, table_name, table_summary, created_at)\n",
            "2025-08-20 17:10:06,392 [INFO] Creating SQLite DB Engine for the existing Chinook database at C:\\Users\\Hp\\AppData\\Roaming\\DBeaverData\\workspace6\\.metadata\\sample-database-sqlite-1\\Chinook.db\n",
            "2025-08-20 17:10:06,397 [INFO] Generating table summaries...\n",
            "2025-08-20 17:10:06,399 [INFO] Fetching existing summaries from the summaries database...\n",
            "2025-08-20 17:10:06,402 [INFO] Found 0 existing summaries in DB\n",
            "2025-08-20 17:10:06,404 [INFO]  - Processing new table: Album\n",
            "2025-08-20 17:10:10,876 [ERROR] Error with attempt 1 for Album: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
            "2025-08-20 17:10:43,536 [INFO] Normalize LLM output\n",
            "2025-08-20 17:10:43,539 [INFO] Processed table: Album\n",
            "2025-08-20 17:10:43,540 [INFO] Saving table summary for Album immediately to summaries DB\n",
            "2025-08-20 17:10:43,548 [INFO]  - Processing new table: Artist\n",
            "2025-08-20 17:10:55,298 [INFO] Normalize LLM output\n",
            "2025-08-20 17:10:55,299 [INFO] Processed table: Artist\n",
            "2025-08-20 17:10:55,300 [INFO] Saving table summary for Artist immediately to summaries DB\n",
            "2025-08-20 17:10:55,307 [INFO]  - Processing new table: Customer\n",
            "2025-08-20 17:11:23,289 [ERROR] Error with attempt 1 for Customer: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 84 [type=json_invalid, input_value='{\"table_name\": \"customer... customer information\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 17:11:41,171 [INFO] Normalize LLM output\n",
            "2025-08-20 17:11:41,173 [INFO] Processed table: Customer\n",
            "2025-08-20 17:11:41,174 [INFO] Saving table summary for Customer immediately to summaries DB\n",
            "2025-08-20 17:11:41,185 [INFO]  - Processing new table: Employee\n",
            "2025-08-20 17:12:30,377 [ERROR] Error with attempt 1 for Employee: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 100 [type=json_invalid, input_value='{\"table_name\": \"chinook_...ta from Chinook Corp.\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 17:12:48,143 [INFO] Normalize LLM output\n",
            "2025-08-20 17:12:48,144 [INFO] Processed table: Employee\n",
            "2025-08-20 17:12:48,145 [INFO] Saving table summary for Employee immediately to summaries DB\n",
            "2025-08-20 17:12:48,155 [INFO]  - Processing new table: Genre\n",
            "2025-08-20 17:13:01,349 [ERROR] Error with attempt 1 for Genre: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 66 [type=json_invalid, input_value='{\"table_name\": \"Genre\", ...Summary of genre data\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 17:13:11,736 [ERROR] Error with attempt 2 for Genre: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 68 [type=json_invalid, input_value='{\"table_name\": \"Genre\", ...mmary of music genres\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 17:13:23,615 [INFO] Normalize LLM output\n",
            "2025-08-20 17:13:23,616 [INFO] Processed table: Genre\n",
            "2025-08-20 17:13:23,619 [INFO] Saving table summary for Genre immediately to summaries DB\n",
            "2025-08-20 17:13:23,626 [INFO]  - Processing new table: Invoice\n",
            "2025-08-20 17:13:44,319 [ERROR] Error with attempt 1 for Invoice: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 78 [type=json_invalid, input_value='{\"table_name\": \"invoice_...mmary of invoice data\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 17:13:58,341 [ERROR] Error with attempt 2 for Invoice: 1 validation error for TableInfo\n",
            "  Invalid JSON: trailing characters at line 1 column 75 [type=json_invalid, input_value='{\"table_name\": \"invoice_...\"Invoice Data Summary\"}', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n",
            "2025-08-20 17:14:10,118 [INFO] Normalize LLM output\n",
            "2025-08-20 17:14:10,120 [INFO] Processed table: Invoice\n",
            "2025-08-20 17:14:10,120 [INFO] Saving table summary for Invoice immediately to summaries DB\n",
            "2025-08-20 17:14:10,130 [INFO]  - Processing new table: InvoiceLine\n",
            "2025-08-20 17:14:23,186 [INFO] Normalize LLM output\n",
            "2025-08-20 17:14:23,188 [INFO] Processed table: InvoiceLine\n",
            "2025-08-20 17:14:23,189 [INFO] Saving table summary for InvoiceLine immediately to summaries DB\n",
            "2025-08-20 17:14:23,197 [INFO]  - Processing new table: MediaType\n",
            "2025-08-20 17:14:36,479 [INFO] Normalize LLM output\n",
            "2025-08-20 17:14:36,480 [INFO] Processed table: MediaType\n",
            "2025-08-20 17:14:36,480 [INFO] Saving table summary for MediaType immediately to summaries DB\n",
            "2025-08-20 17:14:36,489 [INFO]  - Processing new table: Playlist\n",
            "2025-08-20 17:14:48,808 [INFO] Normalize LLM output\n",
            "2025-08-20 17:14:48,809 [INFO] Processed table: Playlist\n",
            "2025-08-20 17:14:48,810 [INFO] Saving table summary for Playlist immediately to summaries DB\n",
            "2025-08-20 17:14:48,816 [INFO]  - Processing new table: PlaylistTrack\n",
            "2025-08-20 17:14:59,530 [INFO] Normalize LLM output\n",
            "2025-08-20 17:14:59,531 [INFO] Processed table: PlaylistTrack\n",
            "2025-08-20 17:14:59,531 [INFO] Saving table summary for PlaylistTrack immediately to summaries DB\n",
            "2025-08-20 17:14:59,537 [INFO]  - Processing new table: Track\n",
            "2025-08-20 17:15:17,457 [INFO] Normalize LLM output\n",
            "2025-08-20 17:15:17,458 [INFO] Processed table: Track\n",
            "2025-08-20 17:15:17,459 [INFO] Saving table summary for Track immediately to summaries DB\n",
            "2025-08-20 17:15:17,466 [INFO] \n",
            " FINAL TABLE SUMMARIES\n",
            "2025-08-20 17:15:17,467 [INFO] - Album: Summary of album data\n",
            "2025-08-20 17:15:17,468 [INFO] - Artist: Summary of artist data\n",
            "2025-08-20 17:15:17,468 [INFO] - Customer: Summary of customer information\n",
            "2025-08-20 17:15:17,469 [INFO] - Employee: Employee information table\n",
            "2025-08-20 17:15:17,469 [INFO] - Genre: Summary of music genres\n",
            "2025-08-20 17:15:17,470 [INFO] - Invoice: Invoice Information\n",
            "2025-08-20 17:15:17,471 [INFO] - InvoiceLine: Summary of invoice data\n",
            "2025-08-20 17:15:17,471 [INFO] - MediaType: Summary of media type information\n",
            "2025-08-20 17:15:17,472 [INFO] - Playlist: Summary of playlist data\n",
            "2025-08-20 17:15:17,473 [INFO] - PlaylistTrack: Summary of playlist and track data\n",
            "2025-08-20 17:15:17,474 [INFO] - Track: Summary of track data\n",
            "2025-08-20 17:15:17,479 [INFO] \n",
            "Saved 11 summaries to:\n",
            "2025-08-20 17:15:17,480 [INFO]  - SQLite DB: db\\Chinook\\sqlite\\table_summaries.db\n",
            "2025-08-20 17:15:17,483 [INFO]  - JSON backup: db\\Chinook\\sqlite\\table_summaries.json\n"
          ]
        }
      ],
      "source": [
        "from utils.llm.get_prompt_temp import TABLE_INFO_PROMPT\n",
        "from utils.llm.get_llm_func import get_llm_func\n",
        "\n",
        "\n",
        "class TableInfo(BaseModel):\n",
        "    \"\"\"Information regarding a structured table.\"\"\"\n",
        "\n",
        "    table_name: str = Field(\n",
        "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
        "    )\n",
        "    table_summary: str = Field(\n",
        "        ..., description=\"short, concise summary/caption of the table\"\n",
        "    )\n",
        "\n",
        "\n",
        "program = LLMTextCompletionProgram.from_defaults(\n",
        "    output_cls=TableInfo,\n",
        "    prompt_template_str=TABLE_INFO_PROMPT,\n",
        "    llm=get_llm_func(),\n",
        ")\n",
        "\n",
        "\n",
        "def extract_first_json_block(text: str):\n",
        "    logger.info(\"Extracting the first valid JSON object from text, ignoring extra trailing text.\")\n",
        "    \n",
        "    match = re.search(r\"\\{.*\\}\", text, re.S)\n",
        "    if not match:\n",
        "        logger.error(f\"No JSON object found in text: {text}\")\n",
        "        raise ValueError(\"No JSON object found in output\")\n",
        "    \n",
        "    try:\n",
        "        logger.info(f\"Extracted JSON: {match.group()}\")\n",
        "        return pyjson.loads(match.group())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to parse JSON: {e}\\nRaw text: {text}\")\n",
        "        raise ValueError(f\"Failed to parse JSON: {e}\\nRaw text: {text}\")\n",
        "\n",
        "\n",
        "os.makedirs(SQLITE_DB_DIR, exist_ok=True)\n",
        "SUMMARY_DB_PATH = os.path.join(SQLITE_DB_DIR, \"table_summaries.db\")\n",
        "\n",
        "logger.info(f\"Creating SQLite DB Engine for the new summaries database: {SUMMARY_DB_PATH}\")\n",
        "summary_engine = create_engine(f\"sqlite:///{SUMMARY_DB_PATH}\")\n",
        "\n",
        "logger.info(f\" - Ensuring the table exists (id, table_name, table_summary, created_at)\")\n",
        "with summary_engine.begin() as conn:\n",
        "    conn.execute(text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS table_summaries (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        table_name TEXT NOT NULL,\n",
        "        table_summary TEXT NOT NULL,\n",
        "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "    )\n",
        "    \"\"\"))\n",
        "\n",
        "table_infos = []\n",
        "\n",
        "logger.info(f\"Creating SQLite DB Engine for the existing Chinook database at {CHINOOK_DBEAVER_DB_PATH}\")\n",
        "engine = create_engine(f\"sqlite:///{CHINOOK_DBEAVER_DB_PATH}\")\n",
        "inspector = inspect(engine)\n",
        "\n",
        "logger.info(\"Generating table summaries...\")\n",
        "with engine.connect() as conn:\n",
        "    existing_tables = set()\n",
        "    \n",
        "    logger.info(\"Fetching existing summaries from the summaries database...\")\n",
        "    with summary_engine.connect() as summary_conn:\n",
        "        rows = summary_conn.execute(text(\"SELECT table_name FROM table_summaries\")).fetchall()\n",
        "        existing_tables = {row[0] for row in rows}\n",
        "        logger.info(f\"Found {len(existing_tables)} existing summaries in DB\")\n",
        "    \n",
        "    for idx, table in enumerate(inspector.get_table_names()):\n",
        "        if table in existing_tables:\n",
        "            logger.info(f\" - Skipping table '{table}' — summary already exists.\")\n",
        "            continue\n",
        "        \n",
        "        logger.info(f\" - Processing new table: {table}\")\n",
        "        df = pd.read_sql(f\"SELECT * FROM {table} LIMIT 10;\", conn)\n",
        "        df_str = df.to_csv(index=False)\n",
        "\n",
        "        table_info = None\n",
        "        for attempt in range(MAX_RETRIES):\n",
        "            try:\n",
        "                raw_output = program(\n",
        "                    table_str=df_str,\n",
        "                    exclude_table_name_list=str(list(inspector.get_table_names())),\n",
        "                )\n",
        "\n",
        "                logger.info(f\"Normalize LLM output\")\n",
        "                if isinstance(raw_output, str):\n",
        "                    parsed_dict = extract_first_json_block(raw_output)\n",
        "                elif isinstance(raw_output, dict):\n",
        "                    parsed_dict = raw_output\n",
        "                elif isinstance(raw_output, TableInfo):\n",
        "                    parsed_dict = raw_output.model_dump()\n",
        "                else:\n",
        "                    logger.error(f\"Unexpected return type: {type(raw_output)}\")\n",
        "                    raise TypeError(f\"Unexpected return type: {type(raw_output)}\")\n",
        "\n",
        "                table_info = TableInfo(\n",
        "                    table_name=table,  # use actual SQLAlchemy inspector name\n",
        "                    table_summary=parsed_dict[\"table_summary\"],\n",
        "                )\n",
        "\n",
        "                logger.info(f\"Processed table: {table_info.table_name}\")\n",
        "                break  # success → next table\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error with attempt {attempt+1} for {table}: {e}\")\n",
        "                time.sleep(2)\n",
        "\n",
        "        if table_info:\n",
        "            table_infos.append(table_info)\n",
        "            \n",
        "            try:\n",
        "                logger.info(f\"Saving table summary for {table_info.table_name} immediately to summaries DB\")\n",
        "                with summary_engine.begin() as conn2:\n",
        "                    conn2.execute(\n",
        "                        text(\"INSERT INTO table_summaries (table_name, table_summary) VALUES (:name, :summary)\"),\n",
        "                        {\"name\": table_info.table_name, \"summary\": table_info.table_summary},\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to save table summary for {table_info.table_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "logger.info(\"\\n FINAL TABLE SUMMARIES\")\n",
        "for t in table_infos:\n",
        "    logger.info(f\"- {t.table_name}: {t.table_summary}\")\n",
        "\n",
        "logger.debug(\"JSON dump for testing purposes only\")\n",
        "json_path = os.path.join(SQLITE_DB_DIR, \"table_summaries.json\")\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump([t.model_dump() for t in table_infos], f, indent=2, ensure_ascii=False)\n",
        "\n",
        "\n",
        "logger.info(f\"\\nSaved {len(table_infos)} summaries to:\")\n",
        "logger.info(f\" - SQLite DB: {SUMMARY_DB_PATH}\")\n",
        "logger.info(f\" - JSON backup: {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "53544059-de7d-48dd-8e00-89517964852b",
      "metadata": {
        "id": "53544059-de7d-48dd-8e00-89517964852b",
        "outputId": "df224651-1765-4aa7-e841-d58546c20e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 17:15:57,472 [INFO] Table Schemas\n",
            "2025-08-20 17:15:57,478 [INFO] \n",
            "Table: Album\n",
            "2025-08-20 17:15:57,479 [INFO]   AlbumId: INTEGER\n",
            "2025-08-20 17:15:57,480 [INFO]   Title: NVARCHAR(160)\n",
            "2025-08-20 17:15:57,481 [INFO]   ArtistId: INTEGER\n",
            "2025-08-20 17:15:57,483 [INFO] \n",
            "Table: Artist\n",
            "2025-08-20 17:15:57,484 [INFO]   ArtistId: INTEGER\n",
            "2025-08-20 17:15:57,484 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 17:15:57,486 [INFO] \n",
            "Table: Customer\n",
            "2025-08-20 17:15:57,487 [INFO]   CustomerId: INTEGER\n",
            "2025-08-20 17:15:57,487 [INFO]   FirstName: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,488 [INFO]   LastName: NVARCHAR(20)\n",
            "2025-08-20 17:15:57,489 [INFO]   Company: NVARCHAR(80)\n",
            "2025-08-20 17:15:57,491 [INFO]   Address: NVARCHAR(70)\n",
            "2025-08-20 17:15:57,493 [INFO]   City: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,495 [INFO]   State: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,496 [INFO]   Country: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,497 [INFO]   PostalCode: NVARCHAR(10)\n",
            "2025-08-20 17:15:57,497 [INFO]   Phone: NVARCHAR(24)\n",
            "2025-08-20 17:15:57,498 [INFO]   Fax: NVARCHAR(24)\n",
            "2025-08-20 17:15:57,499 [INFO]   Email: NVARCHAR(60)\n",
            "2025-08-20 17:15:57,500 [INFO]   SupportRepId: INTEGER\n",
            "2025-08-20 17:15:57,502 [INFO] \n",
            "Table: Employee\n",
            "2025-08-20 17:15:57,503 [INFO]   EmployeeId: INTEGER\n",
            "2025-08-20 17:15:57,503 [INFO]   LastName: NVARCHAR(20)\n",
            "2025-08-20 17:15:57,504 [INFO]   FirstName: NVARCHAR(20)\n",
            "2025-08-20 17:15:57,505 [INFO]   Title: NVARCHAR(30)\n",
            "2025-08-20 17:15:57,505 [INFO]   ReportsTo: INTEGER\n",
            "2025-08-20 17:15:57,506 [INFO]   BirthDate: DATETIME\n",
            "2025-08-20 17:15:57,507 [INFO]   HireDate: DATETIME\n",
            "2025-08-20 17:15:57,507 [INFO]   Address: NVARCHAR(70)\n",
            "2025-08-20 17:15:57,508 [INFO]   City: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,508 [INFO]   State: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,509 [INFO]   Country: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,510 [INFO]   PostalCode: NVARCHAR(10)\n",
            "2025-08-20 17:15:57,510 [INFO]   Phone: NVARCHAR(24)\n",
            "2025-08-20 17:15:57,511 [INFO]   Fax: NVARCHAR(24)\n",
            "2025-08-20 17:15:57,512 [INFO]   Email: NVARCHAR(60)\n",
            "2025-08-20 17:15:57,513 [INFO] \n",
            "Table: Genre\n",
            "2025-08-20 17:15:57,514 [INFO]   GenreId: INTEGER\n",
            "2025-08-20 17:15:57,514 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 17:15:57,515 [INFO] \n",
            "Table: Invoice\n",
            "2025-08-20 17:15:57,516 [INFO]   InvoiceId: INTEGER\n",
            "2025-08-20 17:15:57,516 [INFO]   CustomerId: INTEGER\n",
            "2025-08-20 17:15:57,517 [INFO]   InvoiceDate: DATETIME\n",
            "2025-08-20 17:15:57,517 [INFO]   BillingAddress: NVARCHAR(70)\n",
            "2025-08-20 17:15:57,518 [INFO]   BillingCity: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,518 [INFO]   BillingState: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,519 [INFO]   BillingCountry: NVARCHAR(40)\n",
            "2025-08-20 17:15:57,519 [INFO]   BillingPostalCode: NVARCHAR(10)\n",
            "2025-08-20 17:15:57,520 [INFO]   Total: NUMERIC(10, 2)\n",
            "2025-08-20 17:15:57,521 [INFO] \n",
            "Table: InvoiceLine\n",
            "2025-08-20 17:15:57,521 [INFO]   InvoiceLineId: INTEGER\n",
            "2025-08-20 17:15:57,522 [INFO]   InvoiceId: INTEGER\n",
            "2025-08-20 17:15:57,523 [INFO]   TrackId: INTEGER\n",
            "2025-08-20 17:15:57,524 [INFO]   UnitPrice: NUMERIC(10, 2)\n",
            "2025-08-20 17:15:57,524 [INFO]   Quantity: INTEGER\n",
            "2025-08-20 17:15:57,528 [INFO] \n",
            "Table: MediaType\n",
            "2025-08-20 17:15:57,529 [INFO]   MediaTypeId: INTEGER\n",
            "2025-08-20 17:15:57,529 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 17:15:57,531 [INFO] \n",
            "Table: Playlist\n",
            "2025-08-20 17:15:57,531 [INFO]   PlaylistId: INTEGER\n",
            "2025-08-20 17:15:57,532 [INFO]   Name: NVARCHAR(120)\n",
            "2025-08-20 17:15:57,533 [INFO] \n",
            "Table: PlaylistTrack\n",
            "2025-08-20 17:15:57,534 [INFO]   PlaylistId: INTEGER\n",
            "2025-08-20 17:15:57,534 [INFO]   TrackId: INTEGER\n",
            "2025-08-20 17:15:57,535 [INFO] \n",
            "Table: Track\n",
            "2025-08-20 17:15:57,536 [INFO]   TrackId: INTEGER\n",
            "2025-08-20 17:15:57,536 [INFO]   Name: NVARCHAR(200)\n",
            "2025-08-20 17:15:57,537 [INFO]   AlbumId: INTEGER\n",
            "2025-08-20 17:15:57,538 [INFO]   MediaTypeId: INTEGER\n",
            "2025-08-20 17:15:57,538 [INFO]   GenreId: INTEGER\n",
            "2025-08-20 17:15:57,539 [INFO]   Composer: NVARCHAR(220)\n",
            "2025-08-20 17:15:57,540 [INFO]   Milliseconds: INTEGER\n",
            "2025-08-20 17:15:57,541 [INFO]   Bytes: INTEGER\n",
            "2025-08-20 17:15:57,541 [INFO]   UnitPrice: NUMERIC(10, 2)\n"
          ]
        }
      ],
      "source": [
        "def get_table_schema(table_name: str):\n",
        "    \"\"\"Fetch column names and types for an existing table.\"\"\"\n",
        "    columns = inspector.get_columns(table_name)\n",
        "    schema = {col[\"name\"]: str(col[\"type\"]) for col in columns}\n",
        "    return schema\n",
        "\n",
        "\n",
        "logger.info(\"Table Schemas\")\n",
        "for table_name in inspector.get_table_names():\n",
        "    schema = get_table_schema(table_name)\n",
        "    logger.info(f\"\\nTable: {table_name}\")\n",
        "    for col, dtype in schema.items():\n",
        "        logger.info(f\"  {col}: {dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
      "metadata": {
        "id": "f12a34b5-1d91-4e85-a6ce-97adb5ddfa06",
        "outputId": "4527a7d1-6b5f-4f89-9a63-e5263f4e4441"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148: SAWarning: Skipped unsupported reflection of expression-based index ix_cumulative_llm_token_count_total\n",
            "  next(self.gen)\n",
            "C:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148: SAWarning: Skipped unsupported reflection of expression-based index ix_latency\n",
            "  next(self.gen)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
            "📖 For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
            "2025-08-20 17:16:12,522 [INFO] Phoenix launched and global handler set.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unknown span: U3Bhbjox\n",
            "\n",
            "GraphQL request:4:3\n",
            "3 | ) {\n",
            "4 |   span: node(id: $id) {\n",
            "  |   ^\n",
            "5 |     __typename\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\graphql\\execution\\execute.py\", line 530, in await_result\n",
            "    return_type, field_nodes, info, path, await result\n",
            "                                          ^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\strawberry\\schema\\schema_converter.py\", line 788, in _async_resolver\n",
            "    return await await_maybe(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "    )\n",
            "    ^\n",
            "  File \"c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\strawberry\\utils\\await_maybe.py\", line 13, in await_maybe\n",
            "    return await value\n",
            "           ^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\phoenix\\server\\api\\queries.py\", line 902, in node\n",
            "    raise NotFound(f\"Unknown span: {id}\")\n",
            "phoenix.server.api.exceptions.NotFound: Unknown span: U3Bhbjox\n"
          ]
        }
      ],
      "source": [
        "px.launch_app()\n",
        "set_global_handler(\"arize_phoenix\")\n",
        "\n",
        "# logger.info(\"🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\")\n",
        "# logger.info(\"📖 For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\")\n",
        "\n",
        "logger.info(\"Phoenix launched and global handler set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266b9e05",
      "metadata": {},
      "source": [
        "1. Object index, retriever, SQLDatabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019",
      "metadata": {
        "id": "8a89bc36-a5ac-46bf-b9ae-801f34992019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 17:17:04,989 [INFO] Wrapping engine into LlamaIndex SQLDatabase\n",
            "2025-08-20 17:17:05,015 [INFO] Creating table node mapping, i.e. mapping from SQL tables -> nodes\n",
            "2025-08-20 17:17:05,016 [INFO] Loading all existing summaries from SQLite DB\n",
            "2025-08-20 17:17:05,018 [INFO] Filtering out only valid tables from loaded summaries that exist in the db\n",
            "2025-08-20 17:17:05,020 [INFO] Adding table: Album with summary: Summary of album data\n",
            "2025-08-20 17:17:05,021 [INFO] Adding table: Artist with summary: Summary of artist data\n",
            "2025-08-20 17:17:05,021 [INFO] Adding table: Customer with summary: Summary of customer information\n",
            "2025-08-20 17:17:05,022 [INFO] Adding table: Employee with summary: Employee information table\n",
            "2025-08-20 17:17:05,023 [INFO] Adding table: Genre with summary: Summary of music genres\n",
            "2025-08-20 17:17:05,023 [INFO] Adding table: Invoice with summary: Invoice Information\n",
            "2025-08-20 17:17:05,024 [INFO] Adding table: InvoiceLine with summary: Summary of invoice data\n",
            "2025-08-20 17:17:05,025 [INFO] Adding table: MediaType with summary: Summary of media type information\n",
            "2025-08-20 17:17:05,025 [INFO] Adding table: Playlist with summary: Summary of playlist data\n",
            "2025-08-20 17:17:05,026 [INFO] Adding table: PlaylistTrack with summary: Summary of playlist and track data\n",
            "2025-08-20 17:17:05,029 [INFO] Adding table: Track with summary: Summary of track data\n",
            "2025-08-20 17:17:05,030 [INFO] Building object index for table retrieval\n",
            "2025-08-20 17:17:09,996 [INFO] Creating SQL retriever for query execution\n"
          ]
        }
      ],
      "source": [
        "from utils.llm.get_llm_func import get_embedding_func\n",
        "from utils.llm.get_prompt_temp import RESPONSE_SYNTHESIS_PROMPT\n",
        "\n",
        "\n",
        "logger.info(\"Wrapping engine into LlamaIndex SQLDatabase\")\n",
        "sql_database = SQLDatabase(engine)\n",
        "\n",
        "logger.info(\"Creating table node mapping, i.e. mapping from SQL tables -> nodes\")\n",
        "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
        "\n",
        "logger.info(\"Loading all existing summaries from SQLite DB\")\n",
        "with summary_engine.connect() as conn:\n",
        "    rows = conn.execute(text(\"SELECT table_name, table_summary FROM table_summaries\")).fetchall()\n",
        "\n",
        "logger.info(\"Filtering out only valid tables from loaded summaries that exist in the db\")\n",
        "table_schema_objs = []\n",
        "\n",
        "with engine.connect() as conn:\n",
        "    inspector = inspect(conn)\n",
        "    existing_tables = inspector.get_table_names()\n",
        "\n",
        "# for t in table_infos:\n",
        "#     if t.table_name in existing_tables and t.table_summary:\n",
        "#         table_schema_objs.append(\n",
        "#             SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
        "#         )\n",
        "#         logger.info(f\"Adding table: {t.table_name} with summary: {t.table_summary}\")\n",
        "#     else:\n",
        "#         logger.warning(f\"Skipping missing/unextracted table: {t.table_name}\")\n",
        "\n",
        "for row in rows:\n",
        "    if row.table_name in existing_tables and row.table_summary:\n",
        "        table_schema_objs.append(\n",
        "            SQLTableSchema(table_name=row.table_name, context_str=row.table_summary)\n",
        "        )\n",
        "        logger.info(f\"Adding table: {row.table_name} with summary: {row.table_summary}\")\n",
        "    else:\n",
        "        logger.warning(f\"Skipping missing/unextracted table: {row.table_name}\")\n",
        "\n",
        "logger.info(\"Building object index for table retrieval\")\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    table_schema_objs,\n",
        "    table_node_mapping,\n",
        "    VectorStoreIndex,\n",
        "    embed_model=get_embedding_func(),\n",
        ")\n",
        "obj_retriever = obj_index.as_retriever(similarity_top_k=TOP_K)\n",
        "\n",
        "logger.info(\"Creating SQL retriever for query execution\")\n",
        "sql_retriever = SQLRetriever(sql_database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b25bf248",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 17:18:08,379 [INFO] Table Context: Table 'Album' has columns: AlbumId (INTEGER), Title (NVARCHAR(160)), ArtistId (INTEGER),  and foreign keys: ['ArtistId'] -> Artist.['ArtistId']. The table description is: Summary of album data\n",
            "\n",
            "Table 'Artist' has columns: ArtistId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of artist data\n",
            "\n",
            "Table 'Customer' has columns: CustomerId (INTEGER), FirstName (NVARCHAR(40)), LastName (NVARCHAR(20)), Company (NVARCHAR(80)), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)), SupportRepId (INTEGER),  and foreign keys: ['SupportRepId'] -> Employee.['EmployeeId']. The table description is: Summary of customer information\n",
            "\n",
            "Table 'Employee' has columns: EmployeeId (INTEGER), LastName (NVARCHAR(20)), FirstName (NVARCHAR(20)), Title (NVARCHAR(30)), ReportsTo (INTEGER), BirthDate (DATETIME), HireDate (DATETIME), Address (NVARCHAR(70)), City (NVARCHAR(40)), State (NVARCHAR(40)), Country (NVARCHAR(40)), PostalCode (NVARCHAR(10)), Phone (NVARCHAR(24)), Fax (NVARCHAR(24)), Email (NVARCHAR(60)),  and foreign keys: ['ReportsTo'] -> Employee.['EmployeeId']. The table description is: Employee information table\n",
            "\n",
            "Table 'Genre' has columns: GenreId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of music genres\n",
            "\n",
            "Table 'Invoice' has columns: InvoiceId (INTEGER), CustomerId (INTEGER), InvoiceDate (DATETIME), BillingAddress (NVARCHAR(70)), BillingCity (NVARCHAR(40)), BillingState (NVARCHAR(40)), BillingCountry (NVARCHAR(40)), BillingPostalCode (NVARCHAR(10)), Total (NUMERIC(10, 2)),  and foreign keys: ['CustomerId'] -> Customer.['CustomerId']. The table description is: Invoice Information\n",
            "\n",
            "Table 'InvoiceLine' has columns: InvoiceLineId (INTEGER), InvoiceId (INTEGER), TrackId (INTEGER), UnitPrice (NUMERIC(10, 2)), Quantity (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['InvoiceId'] -> Invoice.['InvoiceId']. The table description is: Summary of invoice data\n",
            "\n",
            "Table 'MediaType' has columns: MediaTypeId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of media type information\n",
            "\n",
            "Table 'Playlist' has columns: PlaylistId (INTEGER), Name (NVARCHAR(120)), . The table description is: Summary of playlist data\n",
            "\n",
            "Table 'PlaylistTrack' has columns: PlaylistId (INTEGER), TrackId (INTEGER),  and foreign keys: ['TrackId'] -> Track.['TrackId'], ['PlaylistId'] -> Playlist.['PlaylistId']. The table description is: Summary of playlist and track data\n",
            "\n",
            "Table 'Track' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2)),  and foreign keys: ['MediaTypeId'] -> MediaType.['MediaTypeId'], ['GenreId'] -> Genre.['GenreId'], ['AlbumId'] -> Album.['AlbumId']. The table description is: Summary of track data\n"
          ]
        }
      ],
      "source": [
        "# Table Context String\n",
        "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
        "    \"\"\"Get table context string (schema + summary).\"\"\"\n",
        "    context_strs = []\n",
        "    for table_schema_obj in table_schema_objs:\n",
        "        try:\n",
        "            # pull schema directly from DB\n",
        "            table_info = sql_database.get_single_table_info(\n",
        "                table_schema_obj.table_name\n",
        "            )\n",
        "            if table_schema_obj.context_str:\n",
        "                table_opt_context = \" The table description is: \"\n",
        "                table_opt_context += table_schema_obj.context_str\n",
        "                table_info += table_opt_context\n",
        "            context_strs.append(table_info)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Skipping table {table_schema_obj.table_name}: {e}\")\n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "\n",
        "table_parser_component = get_table_context_str(table_schema_objs)\n",
        "logger.info(f\"Table Context: {table_parser_component}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9e8a31c5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 17:18:23,905 [INFO] \n",
            " Text-to-SQL Prompt: Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
            "\n",
            "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
            "\n",
            "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
            "\n",
            "Question: Question here\n",
            "SQLQuery: SQL Query to run\n",
            "SQLResult: Result of the SQLQuery\n",
            "Answer: Final answer here\n",
            "\n",
            "Only use tables listed below.\n",
            "{schema}\n",
            "\n",
            "Question: {query_str}\n",
            "SQLQuery: \n"
          ]
        }
      ],
      "source": [
        "# SQL Output Parser\n",
        "def parse_response_to_sql(response: ChatResponse) -> str:\n",
        "    \"\"\"Parse response into a clean SQL string.\"\"\"\n",
        "    response = response.message.content\n",
        "\n",
        "    sql_query_start = response.find(\"SQLQuery:\")\n",
        "    if sql_query_start != -1:\n",
        "        response = response[sql_query_start:]\n",
        "        if response.startswith(\"SQLQuery:\"):\n",
        "            response = response[len(\"SQLQuery:\") :]\n",
        "\n",
        "    sql_result_start = response.find(\"SQLResult:\")\n",
        "    if sql_result_start != -1:\n",
        "        response = response[:sql_result_start]\n",
        "\n",
        "    return response.strip().strip(\"```\").strip()\n",
        "\n",
        "\n",
        "sql_parser_component = FunctionTool.from_defaults(fn=parse_response_to_sql)\n",
        "\n",
        "\n",
        "# Prompts\n",
        "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
        "    dialect=engine.dialect.name\n",
        ")\n",
        "logger.info(f\"\\n Text-to-SQL Prompt: {text2sql_prompt.template}\")\n",
        "\n",
        "\n",
        "response_synthesis_prompt = PromptTemplate(RESPONSE_SYNTHESIS_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15203320",
      "metadata": {},
      "source": [
        "### Index Each Table\n",
        "\n",
        "We embed/index the rows of each table, resulting in one index per table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b103cbc0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 17:18:33,190 [INFO]  [00] Creating persistent Chroma client at: db\\Chinook\\chromadb\n",
            "2025-08-20 17:18:33,958 [INFO]  [01] Indexing rows in table: Album\n",
            "2025-08-20 17:18:34,007 [INFO]  [02] Fetching all rows from table: Album\n",
            "2025-08-20 17:18:34,019 [INFO]   [02.1.1] No existing index found → building new index for: Album\n",
            "2025-08-20 17:18:34,020 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:18:34,023 [INFO]   [02.1.2] Converting rows to text nodes for table: Album\n",
            "2025-08-20 17:18:34,026 [INFO]   [02.1.3] Building vector index for table: Album\n",
            "2025-08-20 17:18:36,481 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Album\n",
            "2025-08-20 17:18:36,489 [INFO]  [01] Indexing rows in table: Artist\n",
            "2025-08-20 17:18:36,522 [INFO]  [02] Fetching all rows from table: Artist\n",
            "2025-08-20 17:18:36,525 [INFO]   [02.1.1] No existing index found → building new index for: Artist\n",
            "2025-08-20 17:18:36,527 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:18:36,535 [INFO]   [02.1.2] Converting rows to text nodes for table: Artist\n",
            "2025-08-20 17:18:36,538 [INFO]   [02.1.3] Building vector index for table: Artist\n",
            "2025-08-20 17:18:38,023 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Artist\n",
            "2025-08-20 17:18:38,032 [INFO]  [01] Indexing rows in table: Customer\n",
            "2025-08-20 17:18:38,059 [INFO]  [02] Fetching all rows from table: Customer\n",
            "2025-08-20 17:18:38,062 [INFO]   [02.1.1] No existing index found → building new index for: Customer\n",
            "2025-08-20 17:18:38,065 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:18:38,070 [INFO]   [02.1.2] Converting rows to text nodes for table: Customer\n",
            "2025-08-20 17:18:38,072 [INFO]   [02.1.3] Building vector index for table: Customer\n",
            "2025-08-20 17:18:39,348 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Customer\n",
            "2025-08-20 17:18:39,374 [INFO]  [01] Indexing rows in table: Employee\n",
            "2025-08-20 17:18:39,402 [INFO]  [02] Fetching all rows from table: Employee\n",
            "2025-08-20 17:18:39,404 [INFO]   [02.1.1] No existing index found → building new index for: Employee\n",
            "2025-08-20 17:18:39,406 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:18:39,407 [INFO]   [02.1.2] Converting rows to text nodes for table: Employee\n",
            "2025-08-20 17:18:39,411 [INFO]   [02.1.3] Building vector index for table: Employee\n",
            "2025-08-20 17:18:39,702 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Employee\n",
            "2025-08-20 17:18:39,714 [INFO]  [01] Indexing rows in table: Genre\n",
            "2025-08-20 17:18:39,747 [INFO]  [02] Fetching all rows from table: Genre\n",
            "2025-08-20 17:18:39,750 [INFO]   [02.1.1] No existing index found → building new index for: Genre\n",
            "2025-08-20 17:18:39,751 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:18:39,752 [INFO]   [02.1.2] Converting rows to text nodes for table: Genre\n",
            "2025-08-20 17:18:39,754 [INFO]   [02.1.3] Building vector index for table: Genre\n",
            "2025-08-20 17:18:40,077 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Genre\n",
            "2025-08-20 17:18:40,096 [INFO]  [01] Indexing rows in table: Invoice\n",
            "2025-08-20 17:18:40,129 [INFO]  [02] Fetching all rows from table: Invoice\n",
            "2025-08-20 17:18:40,136 [INFO]   [02.1.1] No existing index found → building new index for: Invoice\n",
            "2025-08-20 17:18:40,137 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:18:40,151 [INFO]   [02.1.2] Converting rows to text nodes for table: Invoice\n",
            "2025-08-20 17:18:40,155 [INFO]   [02.1.3] Building vector index for table: Invoice\n",
            "2025-08-20 17:18:51,739 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Invoice\n",
            "2025-08-20 17:18:51,750 [INFO]  [01] Indexing rows in table: InvoiceLine\n",
            "2025-08-20 17:18:51,789 [INFO]  [02] Fetching all rows from table: InvoiceLine\n",
            "2025-08-20 17:18:51,792 [INFO]   [02.1.1] No existing index found → building new index for: InvoiceLine\n",
            "2025-08-20 17:18:51,797 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:18:51,820 [INFO]   [02.1.2] Converting rows to text nodes for table: InvoiceLine\n",
            "2025-08-20 17:18:51,849 [INFO]   [02.1.3] Building vector index for table: InvoiceLine\n",
            "2025-08-20 17:19:18,566 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_InvoiceLine\n",
            "2025-08-20 17:19:18,572 [INFO]  [01] Indexing rows in table: MediaType\n",
            "2025-08-20 17:19:18,624 [INFO]  [02] Fetching all rows from table: MediaType\n",
            "2025-08-20 17:19:18,627 [INFO]   [02.1.1] No existing index found → building new index for: MediaType\n",
            "2025-08-20 17:19:18,629 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:19:18,630 [INFO]   [02.1.2] Converting rows to text nodes for table: MediaType\n",
            "2025-08-20 17:19:18,631 [INFO]   [02.1.3] Building vector index for table: MediaType\n",
            "2025-08-20 17:19:18,790 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_MediaType\n",
            "2025-08-20 17:19:18,805 [INFO]  [01] Indexing rows in table: Playlist\n",
            "2025-08-20 17:19:18,834 [INFO]  [02] Fetching all rows from table: Playlist\n",
            "2025-08-20 17:19:18,839 [INFO]   [02.1.1] No existing index found → building new index for: Playlist\n",
            "2025-08-20 17:19:18,840 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:19:18,841 [INFO]   [02.1.2] Converting rows to text nodes for table: Playlist\n",
            "2025-08-20 17:19:18,842 [INFO]   [02.1.3] Building vector index for table: Playlist\n",
            "2025-08-20 17:19:19,083 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Playlist\n",
            "2025-08-20 17:19:19,106 [INFO]  [01] Indexing rows in table: PlaylistTrack\n",
            "2025-08-20 17:19:19,140 [INFO]  [02] Fetching all rows from table: PlaylistTrack\n",
            "2025-08-20 17:19:19,142 [INFO]   [02.1.1] No existing index found → building new index for: PlaylistTrack\n",
            "2025-08-20 17:19:19,144 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:19:19,173 [INFO]   [02.1.2] Converting rows to text nodes for table: PlaylistTrack\n",
            "2025-08-20 17:19:19,577 [INFO]   [02.1.3] Building vector index for table: PlaylistTrack\n",
            "2025-08-20 17:20:00,259 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_PlaylistTrack\n",
            "2025-08-20 17:20:00,269 [INFO]  [01] Indexing rows in table: Track\n",
            "2025-08-20 17:20:00,316 [INFO]  [02] Fetching all rows from table: Track\n",
            "2025-08-20 17:20:00,320 [INFO]   [02.1.1] No existing index found → building new index for: Track\n",
            "2025-08-20 17:20:00,321 [INFO]  - Converting rows to structured strings with col=value format\n",
            "2025-08-20 17:20:00,350 [INFO]   [02.1.2] Converting rows to text nodes for table: Track\n",
            "2025-08-20 17:20:00,378 [INFO]   [02.1.3] Building vector index for table: Track\n",
            "2025-08-20 17:20:58,298 [INFO]   [02.1.4] Persisting index to: db\\Chinook\\chromadb\\table_Track\n"
          ]
        }
      ],
      "source": [
        "def index_all_tables_with_chroma(sql_database, chroma_db_dir: str = CHROMA_DB_DIR) -> Dict[str, VectorStoreIndex]:\n",
        "    \"\"\"Index all tables in the SQL database using ChromaDB as the backend.\"\"\"\n",
        "    os.makedirs(chroma_db_dir, exist_ok=True)\n",
        "\n",
        "    vector_index_dict = {}\n",
        "    engine = sql_database.engine\n",
        "\n",
        "    logger.info(f\" [00] Creating persistent Chroma client at: {chroma_db_dir}\")\n",
        "    chroma_client = chromadb.PersistentClient(path=chroma_db_dir)\n",
        "\n",
        "    for table_name in sql_database.get_usable_table_names():\n",
        "        logger.info(f\" [01] Indexing rows in table: {table_name}\")\n",
        "\n",
        "        # Each table = separate Chroma collection\n",
        "        collection = chroma_client.get_or_create_collection(name=f\"table_{table_name}\")\n",
        "        vector_store = ChromaVectorStore(chroma_collection=collection)\n",
        "        persist_dir = os.path.join(chroma_db_dir, f\"table_{table_name}\")\n",
        "\n",
        "        logger.info(f\" [02] Fetching all rows from table: {table_name}\")\n",
        "        \n",
        "        if collection.count() == 0:\n",
        "            logger.info(f\"  [02.1.1] No existing index found → building new index for: {table_name}\")\n",
        "            \n",
        "            with engine.connect() as conn:\n",
        "                result = conn.execute(text(f'SELECT * FROM \"{table_name}\"'))\n",
        "                \n",
        "                # row_tuples = [tuple(row) for row in result.fetchall()]\n",
        "                \n",
        "                logger.info(f\" - Converting rows to structured strings with col=value format\")\n",
        "                col_names = result.keys()\n",
        "                row_texts = [\n",
        "                    \" | \".join([f\"{col}={val}\" for col, val in zip(col_names, row)])\n",
        "                    for row in result.fetchall()\n",
        "                ]\n",
        "\n",
        "            logger.info(f\"  [02.1.2] Converting rows to text nodes for table: {table_name}\")\n",
        "            # nodes = [TextNode(text=str(row)) for row in row_tuples]\n",
        "            nodes = [TextNode(text=row_text) for row_text in row_texts]\n",
        "\n",
        "            logger.info(f\"  [02.1.3] Building vector index for table: {table_name}\")\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "\n",
        "            logger.info(f\"  [02.1.4] Persisting index to: {persist_dir}\")\n",
        "            storage_context.persist(persist_dir=persist_dir)\n",
        "\n",
        "        else:\n",
        "            logger.info(f\"  [02.2] Reloading existing Chroma index for table: {table_name}\")\n",
        "            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "            index = load_index_from_storage(storage_context)\n",
        "            \n",
        "        vector_index_dict[table_name] = index\n",
        "\n",
        "    return vector_index_dict\n",
        "\n",
        "# Build vector indexes for all tables using ChromaDB\n",
        "vector_index_dict = index_all_tables_with_chroma(sql_database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "31906c11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-20 17:21:03,237 [INFO] [01] Getting schema for table (use .table_name instead of .name)\n",
            "2025-08-20 17:21:03,240 [INFO] [02] Retrieving example rows for table\n",
            "2025-08-20 17:21:03,344 [INFO] Retrieved 2 relevant nodes for table: Album\n",
            "2025-08-20 17:21:03,345 [ERROR] No vector index found for Album\n",
            "2025-08-20 17:21:03,348 [INFO] [01] Getting schema for table (use .table_name instead of .name)\n",
            "2025-08-20 17:21:03,351 [INFO] [02] Retrieving example rows for table\n",
            "2025-08-20 17:21:03,490 [INFO] Retrieved 2 relevant nodes for table: Artist\n",
            "2025-08-20 17:21:03,491 [ERROR] No vector index found for Artist\n",
            "2025-08-20 17:21:03,493 [INFO] [01] Getting schema for table (use .table_name instead of .name)\n",
            "2025-08-20 17:21:03,494 [INFO] [02] Retrieving example rows for table\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(context_strs)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m table_parser_component = \u001b[43mget_table_context_and_rows_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUERY_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_schema_objs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUpdated table context with rows:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtable_parser_component\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mget_table_context_and_rows_str\u001b[39m\u001b[34m(query_str, table_schema_objs)\u001b[39m\n\u001b[32m      9\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33m[02] Retrieving example rows for table\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m vector_retriever = vector_index_dict[table_info_obj.table_name].as_retriever(similarity_top_k=TOP_N)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m relevant_nodes = \u001b[43mvector_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# this will return the TextNodes we stored as vector indexes \u001b[39;00m\n\u001b[32m     12\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(relevant_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m relevant nodes for table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_info_obj.table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(relevant_nodes) > \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:317\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    320\u001b[39m         new_future = asyncio.ensure_future(result)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\llama_index\\core\\base\\base_retriever.py:210\u001b[39m, in \u001b[36mBaseRetriever.retrieve\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.as_trace(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    207\u001b[39m         CBEventType.RETRIEVE,\n\u001b[32m    208\u001b[39m         payload={EventPayload.QUERY_STR: query_bundle.query_str},\n\u001b[32m    209\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m         nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m         nodes = \u001b[38;5;28mself\u001b[39m._handle_recursive_retrieval(query_bundle, nodes)\n\u001b[32m    212\u001b[39m         retrieve_event.on_end(\n\u001b[32m    213\u001b[39m             payload={EventPayload.NODES: nodes},\n\u001b[32m    214\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:317\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    320\u001b[39m         new_future = asyncio.ensure_future(result)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:104\u001b[39m, in \u001b[36mVectorIndexRetriever._retrieve\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle.embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle.embedding_strs) > \u001b[32m0\u001b[39m:\n\u001b[32m     99\u001b[39m         query_bundle.embedding = (\n\u001b[32m    100\u001b[39m             \u001b[38;5;28mself\u001b[39m._embed_model.get_agg_embedding_from_queries(\n\u001b[32m    101\u001b[39m                 query_bundle.embedding_strs\n\u001b[32m    102\u001b[39m             )\n\u001b[32m    103\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:220\u001b[39m, in \u001b[36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[39m\u001b[34m(self, query_bundle_with_embeddings)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_nodes_with_embeddings\u001b[39m(\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[32m    218\u001b[39m ) -> List[NodeWithScore]:\n\u001b[32m    219\u001b[39m     query = \u001b[38;5;28mself\u001b[39m._build_vector_store_query(query_bundle_with_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     nodes_to_fetch = \u001b[38;5;28mself\u001b[39m._determine_nodes_to_fetch(query_result)\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nodes_to_fetch:\n\u001b[32m    224\u001b[39m         \u001b[38;5;66;03m# Fetch any missing nodes from the docstore and insert them into the query result\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:378\u001b[39m, in \u001b[36mChromaVectorStore.query\u001b[39m\u001b[34m(self, query, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query.query_embedding:\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get(limit=query.similarity_top_k, where=where, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:396\u001b[39m, in \u001b[36mChromaVectorStore._query\u001b[39m\u001b[34m(self, query_embeddings, n_results, where, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._collection.query(\n\u001b[32m    390\u001b[39m         query_embeddings=query_embeddings,\n\u001b[32m    391\u001b[39m         n_results=n_results,\n\u001b[32m    392\u001b[39m         where=where,\n\u001b[32m    393\u001b[39m         **kwargs,\n\u001b[32m    394\u001b[39m     )\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m> Top \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results[\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nodes:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    403\u001b[39m nodes = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:221\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    186\u001b[39m \n\u001b[32m    187\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m \n\u001b[32m    207\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m query_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_query_request(\n\u001b[32m    210\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    211\u001b[39m     query_texts=query_texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m    218\u001b[39m     include=include,\n\u001b[32m    219\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere_document\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    234\u001b[39m     response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    235\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hp\\Documents\\GitHub\\rag_text-2-sql\\.venv\\Lib\\site-packages\\chromadb\\api\\rust.py:505\u001b[39m, in \u001b[36mRustBindingsAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, ids, n_results, where, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    489\u001b[39m filtered_ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    491\u001b[39m     CollectionQueryEvent(\n\u001b[32m    492\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    502\u001b[39m     )\n\u001b[32m    503\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[32m    518\u001b[39m     ids=rust_response.ids,\n\u001b[32m    519\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    525\u001b[39m     distances=rust_response.distances,\n\u001b[32m    526\u001b[39m )\n",
            "\u001b[31mInternalError\u001b[39m: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk"
          ]
        }
      ],
      "source": [
        "def get_table_context_and_rows_str(query_str: str, table_schema_objs: List[TableInfo]):\n",
        "    \"\"\"Get table context string for your TableInfo objects.\"\"\"\n",
        "    context_strs = []\n",
        "\n",
        "    for table_info_obj in table_schema_objs:\n",
        "        logger.info(\"[01] Getting schema for table (use .table_name instead of .name)\")\n",
        "        table_info = sql_database.get_single_table_info(table_info_obj.table_name)\n",
        "\n",
        "        logger.info(\"[02] Retrieving example rows for table\")\n",
        "        vector_retriever = vector_index_dict[table_info_obj.table_name].as_retriever(similarity_top_k=TOP_N)\n",
        "        relevant_nodes = vector_retriever.retrieve(query_str) # this will return the TextNodes we stored as vector indexes \n",
        "        logger.info(f\"Retrieved {len(relevant_nodes)} relevant nodes for table: {table_info_obj.table_name}\")\n",
        "\n",
        "        if len(relevant_nodes) > 0:\n",
        "            table_row_context = (\"\\nHere are some relevant example rows (column=value):\\n\")\n",
        "            for node in relevant_nodes:\n",
        "                table_row_context += str(node.get_content()) + \"\\n\"\n",
        "            table_info += table_row_context\n",
        "\n",
        "        context_strs.append(table_info)\n",
        "        \n",
        "        logger.error(f\"No vector index found for {table_info_obj.table_name}\")\n",
        "        continue\n",
        "\n",
        "    return \"\\n\\n\".join(context_strs)\n",
        "\n",
        "table_parser_component = get_table_context_and_rows_str(QUERY_1, table_schema_objs)\n",
        "logger.info(f\"Updated table context with rows:\\n{table_parser_component}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab71cf00",
      "metadata": {},
      "source": [
        "### Define Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22323164",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.t2SQL_workflow.custom_events import (\n",
        "    TableRetrievedEvent,\n",
        "    SchemaProcessedEvent,\n",
        "    SQLPromptReadyEvent,\n",
        "    SQLGeneratedEvent,\n",
        "    SQLParsedEvent,\n",
        "    SQLResultsEvent,\n",
        "    ResponsePromptReadyEvent,\n",
        ")\n",
        "from utils.t2SQL_workflow.custom_fallbacks import (\n",
        "    extract_sql_from_response,\n",
        "    analyze_sql_error,\n",
        "    create_t2s_prompt,\n",
        ")\n",
        "\n",
        "\n",
        "class Text2SQLWorkflowRowRetrieval(Workflow):\n",
        "    @step\n",
        "    async def input_step(self, ev: StartEvent) -> TableRetrievedEvent:\n",
        "        logger.info(f\"[Step 01] Process initial query and retrieve relevant tables\")\n",
        "        query = ev.query\n",
        "\n",
        "        logger.info(f\" - Use object retriever built from your table summaries\")\n",
        "        tables = obj_retriever.retrieve(query)  # candidate schemas\n",
        "        logger.info(f\" - Retrieved {len(tables)} candidate tables for query: {query}\")\n",
        "        \n",
        "        return TableRetrievedEvent(\n",
        "            tables=tables, \n",
        "            query_str=query\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def table_output_parser_step(self, ev: TableRetrievedEvent) -> SchemaProcessedEvent:\n",
        "        logger.info(f\"[Step 02] Parsing schemas and retrieving relevant rows for query: {ev.query_str}\")\n",
        "\n",
        "        logger.info(f\" - Enriching context function with vector row retrieval for tables: {ev.tables}\")\n",
        "        schema_str = get_table_context_and_rows_str(ev.query_str, ev.tables)\n",
        "        \n",
        "        return SchemaProcessedEvent(\n",
        "            table_schema=schema_str, \n",
        "            query_str=ev.query_str\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_prompt_step(self, ev: SchemaProcessedEvent | SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        logger.info(f\"[Step 03] Creating SQL prompt for query: {ev.query_str}\")\n",
        "        if isinstance(ev, SchemaProcessedEvent):\n",
        "            table_schema = ev.table_schema\n",
        "            query_str = ev.query_str\n",
        "            retry_count = 0\n",
        "            error_message = \"\"\n",
        "        else:\n",
        "            table_schema = getattr(ev, 'table_schema', '')\n",
        "            query_str = ev.query_str\n",
        "            retry_count = getattr(ev, 'retry_count', 0) + 1\n",
        "            error_message = getattr(ev, 'error_message', '')\n",
        "\n",
        "        prompt = create_t2s_prompt(table_schema, query_str, retry_count, error_message)\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=prompt,\n",
        "            query_str=query_str,\n",
        "            table_schema=table_schema,\n",
        "            retry_count=retry_count,\n",
        "            error_message=error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def text2sql_llm_step(self, ev: SQLPromptReadyEvent) -> SQLGeneratedEvent:\n",
        "        logger.info(f\"[Step 04] Running LLM to generate SQL for query: {ev.query_str}\")\n",
        "        sql_response = await Settings.llm.acomplete(ev.t2s_prompt)\n",
        "        \n",
        "        return SQLGeneratedEvent(\n",
        "            sql_query=str(sql_response).strip(),\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_output_parser_step(self, ev: SQLGeneratedEvent) -> SQLParsedEvent:\n",
        "        logger.info(f\"[Step 05] Parsing LLM response to extract clean SQL for query: {ev.query_str}\")\n",
        "        try:\n",
        "            clean_sql = parse_response_to_sql(ev.sql_query)  # primary parser\n",
        "        except Exception:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query, logger)  # fallback\n",
        "        \n",
        "        if not clean_sql:\n",
        "            clean_sql = extract_sql_from_response(ev.sql_query, logger)\n",
        "\n",
        "        logger.info(f\"Attempt #{ev.retry_count + 1}\")\n",
        "        logger.info(f\"LLM Response: {ev.sql_query}\")\n",
        "        logger.info(f\"Cleaned SQL: {clean_sql}\")\n",
        "\n",
        "        return SQLParsedEvent(\n",
        "            sql_query=clean_sql,\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=ev.table_schema,\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=ev.error_message\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def sql_retriever_step(self, ev: SQLParsedEvent) -> SQLResultsEvent:\n",
        "        logger.info(f\"[Step 06] Executing SQL for query: {ev.query_str}\")\n",
        "        try:\n",
        "            results = sql_retriever.retrieve(ev.sql_query)\n",
        "            logger.info(f\"[SUCCESS] Executed on attempt #{ev.retry_count + 1}\")\n",
        "\n",
        "            return SQLResultsEvent(\n",
        "                context_str=str(results),\n",
        "                sql_query=ev.sql_query,\n",
        "                query_str=ev.query_str,\n",
        "                success=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            logger.error(f\"Execution failed (Attempt #{ev.retry_count + 1}): {error_msg}\")\n",
        "\n",
        "            if ev.retry_count < MAX_RETRIES:\n",
        "                retry_event = SQLResultsEvent(\n",
        "                    context_str=\"\",\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False\n",
        "                )\n",
        "                retry_event.retry_count = ev.retry_count + 1\n",
        "                retry_event.error_message = analyze_sql_error(error_msg, ev.sql_query, ev.table_schema, logger)\n",
        "                retry_event.table_schema = ev.table_schema\n",
        "                \n",
        "                return retry_event\n",
        "            else:\n",
        "                return SQLResultsEvent(\n",
        "                    context_str=(f\"Failed after {MAX_RETRIES+1} attempts. Final error: {error_msg}\"),\n",
        "                    sql_query=ev.sql_query,\n",
        "                    query_str=ev.query_str,\n",
        "                    success=False\n",
        "                )\n",
        "\n",
        "    @step\n",
        "    async def retry_handler_step(self, ev: SQLResultsEvent) -> SQLPromptReadyEvent:\n",
        "        logger.info(f\"[Step 07] Handling retry for query: {ev.query_str}\")\n",
        "        if ev.success:\n",
        "            return None\n",
        "        \n",
        "        return SQLPromptReadyEvent(\n",
        "            t2s_prompt=\"\",  # regenerated later\n",
        "            query_str=ev.query_str,\n",
        "            table_schema=getattr(ev, 'table_schema', ''),\n",
        "            retry_count=ev.retry_count,\n",
        "            error_message=getattr(ev, 'error_message', 'Unknown error')\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_prompt_step(self, ev: SQLResultsEvent) -> ResponsePromptReadyEvent:\n",
        "        logger.info(f\"[Step 08] Preparing synthesis prompt for query: {ev.query_str}\")\n",
        "        if not ev.success:\n",
        "            return None\n",
        "        prompt = response_synthesis_prompt.format(\n",
        "            query_str=ev.query_str,\n",
        "            context_str=ev.context_str,\n",
        "            sql_query=ev.sql_query\n",
        "        )\n",
        "        \n",
        "        return ResponsePromptReadyEvent(rs_prompt=prompt)\n",
        "\n",
        "    @step\n",
        "    async def response_synthesis_llm_step(self, ev: ResponsePromptReadyEvent) -> StopEvent:\n",
        "        logger.info(f\"[Step 09] Generating final answer for query: {ev.query_str}\")\n",
        "        answer = await Settings.llm.acomplete(ev.rs_prompt)\n",
        "        \n",
        "        return StopEvent(result=str(answer))\n",
        "\n",
        "\n",
        "# Runner\n",
        "async def run_text2sql_workflow_row(query: str):\n",
        "    workflow = Text2SQLWorkflowRowRetrieval(timeout=480)\n",
        "    result = await workflow.run(query=query)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "702f159a",
      "metadata": {},
      "source": [
        "Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0129d573",
      "metadata": {},
      "outputs": [],
      "source": [
        "async def visualize_text2sql_workflow_row(sample_query: str, execution_name: str, output_dir: str = WORKFLOW_VISUALIZATION_DIR):\n",
        "    \"\"\"\n",
        "    Function to visualize the Text2SQL workflow in your version:\n",
        "    - Draws all possible flows\n",
        "    - Runs your row-retrieval Text2SQL workflow\n",
        "    - Draws execution path of the actual run\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    logger.info(\"[01] Drawing all possible flows...\")\n",
        "    all_flows_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_flow.html\")\n",
        "    draw_all_possible_flows(\n",
        "        Text2SQLWorkflowRowRetrieval,\n",
        "        filename=all_flows_path\n",
        "    )\n",
        "    logger.info(f\"[SUCCESS] All possible flows saved to: {all_flows_path}\")\n",
        "\n",
        "    logger.info(\"[02] Running workflow and drawing execution path...\")\n",
        "    try:\n",
        "        logger.info(\" - wrapper function instead of manual instantiation\")\n",
        "        result = await run_text2sql_workflow_row(sample_query)\n",
        "\n",
        "        logger.info(\" - Recreating workflow object for execution path drawing\")\n",
        "        workflow = Text2SQLWorkflowRowRetrieval(timeout=240)\n",
        "\n",
        "        execution_path = os.path.join(output_dir, f\"{execution_name}_text2sql_workflow_execution.html\")\n",
        "        draw_most_recent_execution(\n",
        "            workflow,\n",
        "            filename=execution_path\n",
        "        )\n",
        "        logger.info(f\"[SUCCESS] Recent execution path saved to: {execution_path}\")\n",
        "        logger.info(f\"Workflow result: {result.result}\")  \n",
        "        logger.debug(\"this `.result` holds final answer\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during workflow execution: {e}\")\n",
        "        logger.info(\"Note: Ensure retrievers + LLM configs are initialized correctly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb9637d",
      "metadata": {},
      "source": [
        "### Run Some Queries\n",
        "\n",
        "We can now ask about relevant entries even if it doesn't exactly match the entry in the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3daf393",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = await run_text2sql_workflow_row(QUERY_1)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "963acccf",
      "metadata": {},
      "outputs": [],
      "source": [
        "await visualize_text2sql_workflow_row(QUERY_1, QUERY_1_INITIAL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-text-2-sql",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
